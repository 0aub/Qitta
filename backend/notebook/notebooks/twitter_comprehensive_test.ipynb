{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üê¶ Twitter Scraper - Comprehensive Testing & Validation\n",
    "\n",
    "**Complete end-to-end testing suite for Twitter scraper with detailed analysis**\n",
    "\n",
    "This notebook provides:\n",
    "- ‚úÖ Complete extraction testing across multiple accounts and levels\n",
    "- üìä Detailed data validation and quality analysis\n",
    "- üîç Performance metrics and success rate tracking\n",
    "- üéØ Variable counting and comprehensive result analysis\n",
    "- üöÄ All results in a single output for easy tracing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Auto-detected browser endpoint: http://browser:8004\n",
      "üê¶ TWITTER SCRAPER COMPREHENSIVE TEST SUITE\n",
      "================================================================================\n",
      "üìç API Endpoint: http://browser:8004\n",
      "üìÅ Storage Path: /storage/scraped_data\n",
      "üéØ Test Accounts: ['naval', 'elonmusk', 'paulg', 'sama', 'vitalikbuterin']\n",
      "üîß Test Configurations: ['basic', 'enhanced', 'comprehensive', 'date_filtered']\n",
      "‚úÖ API connectivity: Connected to browser service\n",
      "\n",
      "================================================================================\n",
      "üß™ TEST PHASE 1: BASIC EXTRACTION ACROSS ACCOUNTS\n",
      "================================================================================\n",
      "\n",
      "üöÄ SUBMITTING: Basic Extraction - @naval\n",
      "üìù Payload: {\n",
      "  \"username\": \"naval\",\n",
      "  \"scrape_posts\": true,\n",
      "  \"max_posts\": 10,\n",
      "  \"scrape_level\": 1\n",
      "}\n",
      "üÜî Job ID: 2e4a666a2d2b4d58ba0d499ea725af7e\n",
      "‚è≥ Waiting for job 2e4a666a2d2b4d58ba0d499ea725af7e...\n",
      "‚è±Ô∏è  running 18s (18s)\n",
      "‚úÖ FINISHED in 21.0s\n",
      "\n",
      "================================================================================\n",
      "üîç ANALYZING: Basic Extraction - @naval\n",
      "================================================================================\n",
      "‚úÖ STATUS: Task completed successfully\n",
      "üéØ TARGET: @naval\n",
      "üìä EXTRACTION METHOD: level_1_basic\n",
      "üìà SCRAPE LEVEL: 1\n",
      "üìà SUCCESS RATE: 10.0%\n",
      "‚úÖ TOTAL EXTRACTED: 1 items\n",
      "\n",
      "üìã COMPREHENSIVE USER DATA ANALYSIS:\n",
      "   üë§ Profile: Name='Naval' | Bio=14 chars\n",
      "   üìä Stats: 2.8M followers | 0 following\n",
      "\n",
      "üìä EXTRACTED DATA BREAKDOWN:\n",
      "   üìù Posts: 8 items\n",
      "      üìù Sample: How to Get Rich (without getting lucky):...\n",
      "\n",
      "üìà TOTAL DATA ITEMS: 8 across all categories\n",
      "\n",
      "üìä DATA QUALITY SCORE: 70%\n",
      "‚úÖ GOOD data quality\n",
      "\n",
      "üöÄ SUBMITTING: Basic Extraction - @paulg\n",
      "üìù Payload: {\n",
      "  \"username\": \"paulg\",\n",
      "  \"scrape_posts\": true,\n",
      "  \"max_posts\": 10,\n",
      "  \"scrape_level\": 1\n",
      "}\n",
      "üÜî Job ID: ff06aff1406f4e81b7aad85048f0ed02\n",
      "‚è≥ Waiting for job ff06aff1406f4e81b7aad85048f0ed02...\n",
      "‚è±Ô∏è  running 27s (27s)\n",
      "‚úÖ FINISHED in 30.0s\n",
      "\n",
      "================================================================================\n",
      "üîç ANALYZING: Basic Extraction - @paulg\n",
      "================================================================================\n",
      "‚úÖ STATUS: Task completed successfully\n",
      "üéØ TARGET: @paulg\n",
      "üìä EXTRACTION METHOD: level_1_basic\n",
      "üìà SCRAPE LEVEL: 1\n",
      "üìà SUCCESS RATE: 10.0%\n",
      "‚úÖ TOTAL EXTRACTED: 1 items\n",
      "\n",
      "üìã COMPREHENSIVE USER DATA ANALYSIS:\n",
      "   üë§ Profile: Name='Paul Graham' | Bio=500 chars\n",
      "   üìä Stats: 2M followers | 779 following\n",
      "\n",
      "üìä EXTRACTED DATA BREAKDOWN:\n",
      "   üìù Posts: 8 items\n",
      "      üìù Sample: Homelessness should be reframed into 3 groups....\n",
      "\n",
      "üìà TOTAL DATA ITEMS: 8 across all categories\n",
      "\n",
      "üìä DATA QUALITY SCORE: 70%\n",
      "‚úÖ GOOD data quality\n",
      "\n",
      "üöÄ SUBMITTING: Basic Extraction - @sama\n",
      "üìù Payload: {\n",
      "  \"username\": \"sama\",\n",
      "  \"scrape_posts\": true,\n",
      "  \"max_posts\": 10,\n",
      "  \"scrape_level\": 1\n",
      "}\n",
      "üÜî Job ID: 45df80ff474a473bad1854dc5e4e9ec0\n",
      "‚è≥ Waiting for job 45df80ff474a473bad1854dc5e4e9ec0...\n",
      "‚è±Ô∏è  running 30s (30s)\n",
      "‚úÖ FINISHED in 33.0s\n",
      "\n",
      "================================================================================\n",
      "üîç ANALYZING: Basic Extraction - @sama\n",
      "================================================================================\n",
      "‚úÖ STATUS: Task completed successfully\n",
      "üéØ TARGET: @sama\n",
      "üìä EXTRACTION METHOD: level_1_basic\n",
      "üìà SCRAPE LEVEL: 1\n",
      "üìà SUCCESS RATE: 10.0%\n",
      "‚úÖ TOTAL EXTRACTED: 1 items\n",
      "\n",
      "üìã COMPREHENSIVE USER DATA ANALYSIS:\n",
      "   üë§ Profile: Name='Sam Altman' | Bio=18 chars\n",
      "   üìä Stats: 3.9M followers | 969 following\n",
      "\n",
      "üìä EXTRACTED DATA BREAKDOWN:\n",
      "   üìù Posts: 8 items\n",
      "      üìù Sample: You can now use gpt-5-codex to investigate and find critical...\n",
      "\n",
      "üìà TOTAL DATA ITEMS: 8 across all categories\n",
      "\n",
      "üìä DATA QUALITY SCORE: 70%\n",
      "‚úÖ GOOD data quality\n",
      "\n",
      "================================================================================\n",
      "üß™ TEST PHASE 2: SCRAPE LEVEL COMPARISON\n",
      "================================================================================\n",
      "\n",
      "üöÄ SUBMITTING: Level 1 Extraction - @vitalikbuterin\n",
      "üìù Payload: {\n",
      "  \"username\": \"vitalikbuterin\",\n",
      "  \"scrape_posts\": true,\n",
      "  \"max_posts\": 8,\n",
      "  \"scrape_level\": 1,\n",
      "  \"level\": 1\n",
      "}\n",
      "üÜî Job ID: fe5693ec88a040c8bfb64686b6f0d5f6\n",
      "‚è≥ Waiting for job fe5693ec88a040c8bfb64686b6f0d5f6...\n",
      "‚è±Ô∏è  running 36s (36s)\n",
      "‚úÖ FINISHED in 39.0s\n",
      "\n",
      "================================================================================\n",
      "üîç ANALYZING: Level 1 Extraction - @vitalikbuterin\n",
      "================================================================================\n",
      "‚úÖ STATUS: Task completed successfully\n",
      "üéØ TARGET: @vitalikbuterin\n",
      "üìä EXTRACTION METHOD: level_1_basic\n",
      "üìà SCRAPE LEVEL: 1\n",
      "üìà SUCCESS RATE: 12.5%\n",
      "‚úÖ TOTAL EXTRACTED: 1 items\n",
      "\n",
      "üìã COMPREHENSIVE USER DATA ANALYSIS:\n",
      "   üë§ Profile: Name='vitalik.eth' | Bio=65 chars\n",
      "   üìä Stats: 5.8M followers | 503 following\n",
      "\n",
      "üìä EXTRACTED DATA BREAKDOWN:\n",
      "   üìù Posts: 8 items\n",
      "      üìù Sample: 0/ The Ethereum Foundation is committed to supporting the ‚ÄòC...\n",
      "\n",
      "üìà TOTAL DATA ITEMS: 8 across all categories\n",
      "\n",
      "üìä DATA QUALITY SCORE: 70%\n",
      "‚úÖ GOOD data quality\n",
      "\n",
      "üöÄ SUBMITTING: Level 2 Extraction - @vitalikbuterin\n",
      "üìù Payload: {\n",
      "  \"username\": \"vitalikbuterin\",\n",
      "  \"scrape_posts\": true,\n",
      "  \"max_posts\": 8,\n",
      "  \"scrape_level\": 2,\n",
      "  \"level\": 2\n",
      "}\n",
      "üÜî Job ID: c31c85e5a5324c5283e6130537ab8fad\n",
      "‚è≥ Waiting for job c31c85e5a5324c5283e6130537ab8fad...\n",
      "‚è±Ô∏è  running 36s (36s)\n",
      "‚úÖ FINISHED in 39.0s\n",
      "\n",
      "================================================================================\n",
      "üîç ANALYZING: Level 2 Extraction - @vitalikbuterin\n",
      "================================================================================\n",
      "‚úÖ STATUS: Task completed successfully\n",
      "üéØ TARGET: @vitalikbuterin\n",
      "üìä EXTRACTION METHOD: level_2_full_profile\n",
      "üìà SCRAPE LEVEL: 2\n",
      "üìà SUCCESS RATE: 12.5%\n",
      "‚úÖ TOTAL EXTRACTED: 1 items\n",
      "\n",
      "üìã COMPREHENSIVE USER DATA ANALYSIS:\n",
      "   üë§ Profile: Name='vitalik.eth' | Bio=65 chars\n",
      "   üìä Stats: 5.8M followers | 503 following\n",
      "\n",
      "üìä EXTRACTED DATA BREAKDOWN:\n",
      "   üìù Posts: 8 items\n",
      "      üìù Sample: 0/ The Ethereum Foundation is committed to supporting the ‚ÄòC...\n",
      "\n",
      "üìà TOTAL DATA ITEMS: 8 across all categories\n",
      "\n",
      "üìä DATA QUALITY SCORE: 70%\n",
      "‚úÖ GOOD data quality\n",
      "\n",
      "üöÄ SUBMITTING: Level 3 Extraction - @vitalikbuterin\n",
      "üìù Payload: {\n",
      "  \"username\": \"vitalikbuterin\",\n",
      "  \"scrape_posts\": true,\n",
      "  \"max_posts\": 8,\n",
      "  \"scrape_level\": 3,\n",
      "  \"level\": 3\n",
      "}\n",
      "üÜî Job ID: 0ae804edf00044d9a44386cc6133c76f\n",
      "‚è≥ Waiting for job 0ae804edf00044d9a44386cc6133c76f...\n",
      "‚è±Ô∏è  running 45s (45s)\n",
      "‚úÖ FINISHED in 48.0s\n",
      "\n",
      "================================================================================\n",
      "üîç ANALYZING: Level 3 Extraction - @vitalikbuterin\n",
      "================================================================================\n",
      "‚úÖ STATUS: Task completed successfully\n",
      "üéØ TARGET: @vitalikbuterin\n",
      "üìä EXTRACTION METHOD: level_3_with_media\n",
      "üìà SCRAPE LEVEL: 3\n",
      "üìà SUCCESS RATE: 12.5%\n",
      "‚úÖ TOTAL EXTRACTED: 1 items\n",
      "\n",
      "üìã COMPREHENSIVE USER DATA ANALYSIS:\n",
      "   üë§ Profile: Name='vitalik.eth' | Bio=65 chars\n",
      "   üìä Stats: 5.8M followers | 503 following\n",
      "\n",
      "üìä EXTRACTED DATA BREAKDOWN:\n",
      "   üìù Posts: 8 items\n",
      "      üìù Sample: 0/ The Ethereum Foundation is committed to supporting the ‚ÄòC...\n",
      "   üñºÔ∏è Media: 25 items\n",
      "      üìù Sample: I choose balance. First-level balance....\n",
      "\n",
      "üìà TOTAL DATA ITEMS: 33 across all categories\n",
      "\n",
      "üìä DATA QUALITY SCORE: 75%\n",
      "‚úÖ GOOD data quality\n",
      "\n",
      "üöÄ SUBMITTING: Level 4 Extraction - @vitalikbuterin\n",
      "üìù Payload: {\n",
      "  \"username\": \"vitalikbuterin\",\n",
      "  \"scrape_posts\": true,\n",
      "  \"max_posts\": 8,\n",
      "  \"scrape_level\": 4,\n",
      "  \"level\": 4\n",
      "}\n",
      "üÜî Job ID: 3091121867e046f9b089742645d10332\n",
      "‚è≥ Waiting for job 3091121867e046f9b089742645d10332...\n",
      "‚è±Ô∏è  running 2m 54s (174s)\n",
      "‚úÖ FINISHED in 177.2s\n",
      "\n",
      "================================================================================\n",
      "üîç ANALYZING: Level 4 Extraction - @vitalikbuterin\n",
      "================================================================================\n",
      "‚úÖ STATUS: Task completed successfully\n",
      "üéØ TARGET: @vitalikbuterin\n",
      "üìä EXTRACTION METHOD: level_4_comprehensive\n",
      "üìà SCRAPE LEVEL: 4\n",
      "üìà SUCCESS RATE: 12.5%\n",
      "‚úÖ TOTAL EXTRACTED: 1 items\n",
      "\n",
      "üìã COMPREHENSIVE USER DATA ANALYSIS:\n",
      "   üë§ Profile: Name='vitalik.eth' | Bio=65 chars\n",
      "   üìä Stats: 5.8M followers | 503 following\n",
      "\n",
      "üìä EXTRACTED DATA BREAKDOWN:\n",
      "   üìù Posts: 50 items\n",
      "      üìù Sample: 0/ The Ethereum Foundation is committed to supporting the ‚ÄòC...\n",
      "\n",
      "üìà TOTAL DATA ITEMS: 50 across all categories\n",
      "\n",
      "üìä DATA QUALITY SCORE: 70%\n",
      "‚úÖ GOOD data quality\n",
      "\n",
      "================================================================================\n",
      "üß™ TEST PHASE 3: COMPREHENSIVE EXTRACTION\n",
      "================================================================================\n",
      "\n",
      "üöÄ SUBMITTING: Comprehensive Extraction - @naval\n",
      "üìù Payload: {\n",
      "  \"username\": \"naval\",\n",
      "  \"scrape_posts\": true,\n",
      "  \"max_posts\": 20,\n",
      "  \"scrape_likes\": true,\n",
      "  \"max_likes\": 10,\n",
      "  \"scrape_mentions\": true,\n",
      "  \"max_mentions\": 5,\n",
      "  \"scrape_media\": true,\n",
      "  \"max_media\": 5,\n",
      "  \"scrape_level\": 4\n",
      "}\n",
      "üÜî Job ID: a920a9082aef43cab05a14ed6893a6eb\n",
      "‚è≥ Waiting for job a920a9082aef43cab05a14ed6893a6eb...\n",
      "‚è±Ô∏è  running 4m 42s (282s)\n",
      "‚úÖ FINISHED in 285.3s\n",
      "\n",
      "================================================================================\n",
      "üîç ANALYZING: Comprehensive Extraction - @naval\n",
      "================================================================================\n",
      "‚úÖ STATUS: Task completed successfully\n",
      "üéØ TARGET: @naval\n",
      "üìä EXTRACTION METHOD: level_4_comprehensive\n",
      "üìà SCRAPE LEVEL: 4\n",
      "üìà SUCCESS RATE: 5.0%\n",
      "‚úÖ TOTAL EXTRACTED: 1 items\n",
      "\n",
      "üìã COMPREHENSIVE USER DATA ANALYSIS:\n",
      "   üë§ Profile: Name='Naval' | Bio=14 chars\n",
      "   üìä Stats: 2.8M followers | 0 following\n",
      "\n",
      "üìä EXTRACTED DATA BREAKDOWN:\n",
      "   üìù Posts: 50 items\n",
      "      üìù Sample: How to Get Rich (without getting lucky):...\n",
      "   ‚ù§Ô∏è Likes: 1 items\n",
      "      üìù Sample: {'type': 'access_restricted', 'message': 'Likes for @naval a...\n",
      "   @Ô∏è‚É£ Mentions: 5 items\n",
      "      üìù Sample: I would love to meet their mother. Wow!...\n",
      "   üñºÔ∏è Media: 5 items\n",
      "      üìù Sample: {'type': 'image', 'url': 'https://pbs.twimg.com/media/G0CCur...\n",
      "\n",
      "üìà TOTAL DATA ITEMS: 61 across all categories\n",
      "\n",
      "üìä DATA QUALITY SCORE: 80%\n",
      "üéâ EXCELLENT data quality\n",
      "\n",
      "================================================================================\n",
      "üß™ TEST PHASE 4: DATE FILTERING PERFORMANCE\n",
      "================================================================================\n",
      "\n",
      "üöÄ SUBMITTING: Date Filter (last_day) - @sama\n",
      "üìù Payload: {\n",
      "  \"username\": \"sama\",\n",
      "  \"scrape_posts\": true,\n",
      "  \"max_posts\": 15,\n",
      "  \"enable_date_filtering\": true,\n",
      "  \"date_range\": \"last_day\",\n",
      "  \"scrape_level\": 4\n",
      "}\n",
      "üÜî Job ID: f6ed52b947b147e794780cee6dbcc212\n",
      "‚è≥ Waiting for job f6ed52b947b147e794780cee6dbcc212...\n",
      "‚è±Ô∏è  running 1m 33s (93s)\n",
      "‚úÖ FINISHED in 96.1s\n",
      "\n",
      "================================================================================\n",
      "üîç ANALYZING: Date Filter (last_day) - @sama\n",
      "================================================================================\n",
      "‚úÖ STATUS: Task completed successfully\n",
      "üéØ TARGET: @sama\n",
      "üìä EXTRACTION METHOD: level_4_comprehensive\n",
      "üìà SCRAPE LEVEL: 4\n",
      "üìà SUCCESS RATE: 6.7%\n",
      "‚úÖ TOTAL EXTRACTED: 1 items\n",
      "\n",
      "üìã COMPREHENSIVE USER DATA ANALYSIS:\n",
      "   üë§ Profile: Name='Sam Altman' | Bio=18 chars\n",
      "   üìä Stats: 3.9M followers | 969 following\n",
      "\n",
      "üìä EXTRACTED DATA BREAKDOWN:\n",
      "   üìù Posts: 38 items\n",
      "      üìù Sample: You can now use gpt-5-codex to investigate and find critical...\n",
      "\n",
      "üìà TOTAL DATA ITEMS: 38 across all categories\n",
      "\n",
      "üìä DATA QUALITY SCORE: 70%\n",
      "‚úÖ GOOD data quality\n",
      "\n",
      "üöÄ SUBMITTING: Date Filter (last_week) - @sama\n",
      "üìù Payload: {\n",
      "  \"username\": \"sama\",\n",
      "  \"scrape_posts\": true,\n",
      "  \"max_posts\": 15,\n",
      "  \"enable_date_filtering\": true,\n",
      "  \"date_range\": \"last_week\",\n",
      "  \"scrape_level\": 4\n",
      "}\n",
      "üÜî Job ID: 07a8e453bc064a93a0e823b4dd236db5\n",
      "‚è≥ Waiting for job 07a8e453bc064a93a0e823b4dd236db5...\n",
      "‚è±Ô∏è  running 3m 15s (195s)\n",
      "‚úÖ FINISHED in 198.2s\n",
      "\n",
      "================================================================================\n",
      "üîç ANALYZING: Date Filter (last_week) - @sama\n",
      "================================================================================\n",
      "‚úÖ STATUS: Task completed successfully\n",
      "üéØ TARGET: @sama\n",
      "üìä EXTRACTION METHOD: level_4_comprehensive\n",
      "üìà SCRAPE LEVEL: 4\n",
      "üìà SUCCESS RATE: 6.7%\n",
      "‚úÖ TOTAL EXTRACTED: 1 items\n",
      "\n",
      "üìã COMPREHENSIVE USER DATA ANALYSIS:\n",
      "   üë§ Profile: Name='Sam Altman' | Bio=18 chars\n",
      "   üìä Stats: 3.9M followers | 969 following\n",
      "\n",
      "üìä EXTRACTED DATA BREAKDOWN:\n",
      "   üìù Posts: 50 items\n",
      "      üìù Sample: You can now use gpt-5-codex to investigate and find critical...\n",
      "\n",
      "üìà TOTAL DATA ITEMS: 50 across all categories\n",
      "\n",
      "üìä DATA QUALITY SCORE: 70%\n",
      "‚úÖ GOOD data quality\n",
      "\n",
      "================================================================================\n",
      "üìä COMPREHENSIVE TEST RESULTS & ANALYSIS\n",
      "================================================================================\n",
      "üìà OVERALL STATISTICS:\n",
      "   üéØ Total Tests: 10\n",
      "   ‚úÖ Successful: 10 (100.0%)\n",
      "   ‚ùå Failed: 0\n",
      "   üìä Tests with Data: 10\n",
      "   üèÜ Average Quality Score: 71.5%\n",
      "\n",
      "üìã DETAILED TEST RESULTS:\n",
      "   ‚úÖ Basic Extraction - @naval: (8 items) Q:70%\n",
      "   ‚úÖ Basic Extraction - @paulg: (8 items) Q:70%\n",
      "   ‚úÖ Basic Extraction - @sama: (8 items) Q:70%\n",
      "   ‚úÖ Level 1 Extraction - @vitalikbuterin: (8 items) Q:70%\n",
      "   ‚úÖ Level 2 Extraction - @vitalikbuterin: (8 items) Q:70%\n",
      "   ‚úÖ Level 3 Extraction - @vitalikbuterin: (33 items) Q:75%\n",
      "   ‚úÖ Level 4 Extraction - @vitalikbuterin: (50 items) Q:70%\n",
      "   ‚úÖ Comprehensive Extraction - @naval: (61 items) Q:80%\n",
      "   ‚úÖ Date Filter (last_day) - @sama: (38 items) Q:70%\n",
      "   ‚úÖ Date Filter (last_week) - @sama: (50 items) Q:70%\n",
      "\n",
      "üìä DATA EXTRACTION ANALYSIS:\n",
      "   üìà Tests with Data: 10/10 (100.0%)\n",
      "   üìä Total Items Extracted: 272\n",
      "   üìà Average Items per Successful Test: 27.2\n",
      "\n",
      "üìä DATA TYPES EXTRACTED:\n",
      "   ‚ù§Ô∏è Likes: 1 total items\n",
      "   üñºÔ∏è Media: 30 total items\n",
      "   @Ô∏è‚É£ Mentions: 5 total items\n",
      "   üìù Posts: 236 total items\n",
      "   üìä Profile: 10 total items\n",
      "\n",
      "üéØ OVERALL ASSESSMENT:\n",
      "üéâ EXCELLENT - Twitter scraper is working perfectly!\n",
      "\n",
      "üìÅ RECENT JOB DATA:\n",
      "   üìÇ ff06aff1406f4e81b7aad85048f0ed02: 2 files\n",
      "   üìÇ fe5693ec88a040c8bfb64686b6f0d5f6: 2 files\n",
      "   üìÇ f6ed52b947b147e794780cee6dbcc212: 2 files\n",
      "   üìÇ c31c85e5a5324c5283e6130537ab8fad: 2 files\n",
      "   üìÇ a920a9082aef43cab05a14ed6893a6eb: 2 files\n",
      "\n",
      "üîó Raw data files: /storage/scraped_data/twitter/\n",
      "üéâ COMPREHENSIVE TESTING COMPLETED!\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "üê¶ COMPREHENSIVE TWITTER SCRAPER TEST SUITE\n",
    "==========================================\n",
    "\n",
    "Complete testing and validation suite for Twitter scraper functionality.\n",
    "Provides detailed analysis, validation, and performance metrics in a single output.\n",
    "\"\"\"\n",
    "\n",
    "import os, time, pathlib, pprint, requests, json\n",
    "from datetime import datetime, timedelta\n",
    "from typing import Dict, List, Any, Optional\n",
    "\n",
    "# Configuration - Auto-detect correct endpoint\n",
    "def find_browser_endpoint():\n",
    "    \"\"\"Auto-detect the correct browser endpoint.\"\"\"\n",
    "    import urllib.request\n",
    "    \n",
    "    # Try different possible endpoints\n",
    "    endpoints = [\n",
    "        \"http://browser:8001\",\n",
    "        \"http://browser:8004\", \n",
    "        \"http://localhost:8001\",\n",
    "        \"http://localhost:8004\"\n",
    "    ]\n",
    "    \n",
    "    for endpoint in endpoints:\n",
    "        try:\n",
    "            with urllib.request.urlopen(f\"{endpoint}/healthz\", timeout=2) as response:\n",
    "                if response.status == 200:\n",
    "                    print(f\"üîç Auto-detected browser endpoint: {endpoint}\")\n",
    "                    return endpoint\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    # Default fallback\n",
    "    return \"http://localhost:8001\"\n",
    "\n",
    "EP = find_browser_endpoint()  # Auto-detect correct endpoint\n",
    "SCRAPED = pathlib.Path(\"/storage/scraped_data\")\n",
    "\n",
    "# Test accounts with different characteristics\n",
    "TEST_ACCOUNTS = {\n",
    "    \"naval\": \"High-quality tweets, philosophy\",\n",
    "    \"elonmusk\": \"High activity, mixed content\",\n",
    "    \"paulg\": \"Startup advice, essays\",\n",
    "    \"sama\": \"AI/tech commentary\",\n",
    "    \"vitalikbuterin\": \"Crypto/blockchain content\"\n",
    "}\n",
    "\n",
    "# Test configurations\n",
    "TEST_CONFIGS = {\n",
    "    \"basic\": {\n",
    "        \"scrape_posts\": True,\n",
    "        \"max_posts\": 10,\n",
    "        \"scrape_level\": 1\n",
    "    },\n",
    "    \"enhanced\": {\n",
    "        \"scrape_posts\": True,\n",
    "        \"max_posts\": 15,\n",
    "        \"scrape_likes\": True,\n",
    "        \"max_likes\": 5,\n",
    "        \"scrape_level\": 2\n",
    "    },\n",
    "    \"comprehensive\": {\n",
    "        \"scrape_posts\": True,\n",
    "        \"max_posts\": 20,\n",
    "        \"scrape_likes\": True,\n",
    "        \"max_likes\": 10,\n",
    "        \"scrape_mentions\": True,\n",
    "        \"max_mentions\": 5,\n",
    "        \"scrape_media\": True,\n",
    "        \"max_media\": 5,\n",
    "        \"scrape_level\": 4\n",
    "    },\n",
    "    \"date_filtered\": {\n",
    "        \"scrape_posts\": True,\n",
    "        \"max_posts\": 25,\n",
    "        \"enable_date_filtering\": True,\n",
    "        \"date_range\": \"last_week\",\n",
    "        \"scrape_level\": 4\n",
    "    }\n",
    "}\n",
    "\n",
    "class TwitterTestSuite:\n",
    "    \"\"\"Comprehensive Twitter scraper test suite.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.results = {}\n",
    "        self.performance_metrics = {}\n",
    "        self.validation_results = {}\n",
    "        self.total_tests = 0\n",
    "        self.successful_tests = 0\n",
    "        self.failed_tests = 0\n",
    "        self.data_quality_scores = []\n",
    "        \n",
    "    def wait_for_job(self, job_id: str, every: int = 3) -> Dict[str, Any]:\n",
    "        \"\"\"Wait for job completion and return result.\"\"\"\n",
    "        print(f\"‚è≥ Waiting for job {job_id}...\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        while True:\n",
    "            try:\n",
    "                rec = requests.get(f\"{EP}/jobs/{job_id}\", timeout=10).json()\n",
    "                status = rec[\"status\"]\n",
    "                \n",
    "                if status not in {\"finished\", \"error\"}:\n",
    "                    elapsed = time.time() - start_time\n",
    "                    print(f\"\\r‚è±Ô∏è  {rec.get('status_with_elapsed', status)} ({elapsed:.0f}s)\", end=\"\")\n",
    "                else:\n",
    "                    elapsed = time.time() - start_time\n",
    "                    print(f\"\\n‚úÖ {status.upper()} in {elapsed:.1f}s\")\n",
    "                    return rec\n",
    "                    \n",
    "                time.sleep(every)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"\\n‚ùå Error checking job status: {e}\")\n",
    "                return {\"status\": \"error\", \"error\": str(e)}\n",
    "\n",
    "    def submit_job(self, task: str, payload: Dict[str, Any], test_name: str) -> Dict[str, Any]:\n",
    "        \"\"\"Submit job and wait for completion.\"\"\"\n",
    "        print(f\"\\nüöÄ SUBMITTING: {test_name}\")\n",
    "        print(f\"üìù Payload: {json.dumps(payload, indent=2)}\")\n",
    "        \n",
    "        try:\n",
    "            r = requests.post(f\"{EP}/jobs/{task}\", json=payload, timeout=30)\n",
    "            r.raise_for_status()\n",
    "            jid = r.json()[\"job_id\"]\n",
    "            print(f\"üÜî Job ID: {jid}\")\n",
    "            \n",
    "            result = self.wait_for_job(jid)\n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Job submission failed: {e}\")\n",
    "            return {\"status\": \"error\", \"error\": str(e)}\n",
    "\n",
    "    def analyze_extraction_result(self, result: Dict[str, Any], test_name: str) -> Dict[str, Any]:\n",
    "        \"\"\"Comprehensive analysis of extraction results.\"\"\"\n",
    "        print(f\"\\n\" + \"=\"*80)\n",
    "        print(f\"üîç ANALYZING: {test_name}\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        analysis = {\n",
    "            \"test_name\": test_name,\n",
    "            \"status\": result.get(\"status\", \"unknown\"),\n",
    "            \"job_id\": result.get(\"job_id\", \"N/A\"),\n",
    "            \"execution_time\": 0,\n",
    "            \"data_extracted\": False,\n",
    "            \"total_items\": 0,\n",
    "            \"data_types\": {},\n",
    "            \"quality_score\": 0,\n",
    "            \"issues\": [],\n",
    "            \"success\": False\n",
    "        }\n",
    "        \n",
    "        if result[\"status\"] == \"error\":\n",
    "            error_msg = result.get('error', 'Unknown error')\n",
    "            print(f\"‚ùå FAILED: {error_msg}\")\n",
    "            analysis[\"issues\"].append(f\"Job failed: {error_msg}\")\n",
    "            return analysis\n",
    "        \n",
    "        if \"result\" not in result:\n",
    "            print(f\"‚ùå No result data found\")\n",
    "            analysis[\"issues\"].append(\"No result data in response\")\n",
    "            return analysis\n",
    "        \n",
    "        res = result[\"result\"]\n",
    "        metadata = res.get(\"search_metadata\", {})\n",
    "        data = res.get(\"data\", [])\n",
    "        \n",
    "        # Basic extraction info\n",
    "        print(f\"‚úÖ STATUS: Task completed successfully\")\n",
    "        print(f\"üéØ TARGET: @{metadata.get('target_username', 'N/A')}\")\n",
    "        print(f\"üìä EXTRACTION METHOD: {metadata.get('extraction_method', 'N/A')}\")\n",
    "        print(f\"üìà SCRAPE LEVEL: {metadata.get('scrape_level', 'N/A')}\")\n",
    "        print(f\"üìà SUCCESS RATE: {metadata.get('success_rate', 0):.1%}\")\n",
    "        \n",
    "        # Update analysis\n",
    "        analysis[\"success\"] = True\n",
    "        analysis[\"total_items\"] = len(data)\n",
    "        analysis[\"data_extracted\"] = len(data) > 0\n",
    "        \n",
    "        if not data:\n",
    "            print(f\"‚ö†Ô∏è NO DATA EXTRACTED - 0 items returned\")\n",
    "            analysis[\"issues\"].append(\"No data extracted\")\n",
    "            return analysis\n",
    "        \n",
    "        print(f\"‚úÖ TOTAL EXTRACTED: {len(data)} items\")\n",
    "        \n",
    "        # Analyze data structure\n",
    "        first_item = data[0] if data else {}\n",
    "        \n",
    "        # Check if comprehensive user data (profile + posts structure)\n",
    "        if isinstance(first_item, dict) and 'profile' in first_item:\n",
    "            self._analyze_comprehensive_data(first_item, analysis)\n",
    "        else:\n",
    "            self._analyze_direct_posts(data, analysis)\n",
    "        \n",
    "        # Calculate quality score\n",
    "        quality_score = self._calculate_quality_score(data, analysis)\n",
    "        analysis[\"quality_score\"] = quality_score\n",
    "        \n",
    "        print(f\"\\nüìä DATA QUALITY SCORE: {quality_score:.0f}%\")\n",
    "        if quality_score >= 80:\n",
    "            print(f\"üéâ EXCELLENT data quality\")\n",
    "        elif quality_score >= 60:\n",
    "            print(f\"‚úÖ GOOD data quality\")\n",
    "        elif quality_score >= 40:\n",
    "            print(f\"‚ö†Ô∏è FAIR data quality\")\n",
    "        else:\n",
    "            print(f\"‚ùå POOR data quality\")\n",
    "        \n",
    "        return analysis\n",
    "    \n",
    "    def _analyze_comprehensive_data(self, data: Dict[str, Any], analysis: Dict[str, Any]):\n",
    "        \"\"\"Analyze comprehensive user data structure.\"\"\"\n",
    "        print(f\"\\nüìã COMPREHENSIVE USER DATA ANALYSIS:\")\n",
    "        \n",
    "        # Profile analysis\n",
    "        profile = data.get('profile', {})\n",
    "        if profile:\n",
    "            print(f\"   üë§ Profile: Name='{profile.get('display_name', 'N/A')}' | Bio={len(profile.get('bio', ''))} chars\")\n",
    "            print(f\"   üìä Stats: {profile.get('followers_count', 'N/A')} followers | {profile.get('following_count', 'N/A')} following\")\n",
    "            analysis[\"data_types\"][\"profile\"] = 1\n",
    "        \n",
    "        # Data types analysis\n",
    "        data_types = ['posts', 'likes', 'mentions', 'media', 'followers', 'following']\n",
    "        total_items = 0\n",
    "        \n",
    "        print(f\"\\nüìä EXTRACTED DATA BREAKDOWN:\")\n",
    "        for data_type in data_types:\n",
    "            items = data.get(data_type, [])\n",
    "            if items and isinstance(items, list):\n",
    "                count = len(items)\n",
    "                total_items += count\n",
    "                analysis[\"data_types\"][data_type] = count\n",
    "                \n",
    "                emoji = self._get_emoji(data_type)\n",
    "                print(f\"   {emoji} {data_type.title()}: {count} items\")\n",
    "                \n",
    "                # Show sample if available\n",
    "                if count > 0 and isinstance(items[0], dict):\n",
    "                    sample = items[0]\n",
    "                    sample_text = sample.get('text', sample.get('content', str(sample)))[:60]\n",
    "                    print(f\"      üìù Sample: {sample_text}{'...' if len(str(sample)) > 60 else ''}\")\n",
    "        \n",
    "        analysis[\"total_items\"] = total_items\n",
    "        print(f\"\\nüìà TOTAL DATA ITEMS: {total_items} across all categories\")\n",
    "    \n",
    "    def _analyze_direct_posts(self, data: List[Dict], analysis: Dict[str, Any]):\n",
    "        \"\"\"Analyze direct posts/tweets data.\"\"\"\n",
    "        print(f\"\\nüìù DIRECT POSTS ANALYSIS:\")\n",
    "        print(f\"   üìä Total Posts: {len(data)}\")\n",
    "        \n",
    "        analysis[\"data_types\"][\"posts\"] = len(data)\n",
    "        \n",
    "        # Count different data attributes\n",
    "        posts_with_text = sum(1 for p in data if isinstance(p, dict) and p.get('text'))\n",
    "        posts_with_dates = sum(1 for p in data if isinstance(p, dict) and p.get('date'))\n",
    "        posts_with_metrics = sum(1 for p in data if isinstance(p, dict) and any(k in p for k in ['likes', 'retweets', 'replies']))\n",
    "        \n",
    "        print(f\"   üìù With text: {posts_with_text}/{len(data)} ({posts_with_text/len(data)*100:.0f}%)\")\n",
    "        print(f\"   üìÖ With dates: {posts_with_dates}/{len(data)} ({posts_with_dates/len(data)*100:.0f}%)\")\n",
    "        print(f\"   üìä With metrics: {posts_with_metrics}/{len(data)} ({posts_with_metrics/len(data)*100:.0f}%)\")\n",
    "        \n",
    "        # Show samples\n",
    "        sample_count = min(3, len(data))\n",
    "        for i in range(sample_count):\n",
    "            post = data[i]\n",
    "            if isinstance(post, dict):\n",
    "                text = post.get('text', 'No text')[:100]\n",
    "                date = post.get('date', 'No date')\n",
    "                likes = post.get('likes', 'N/A')\n",
    "                print(f\"   üê¶ Post {i+1}: {text}{'...' if len(post.get('text', '')) > 100 else ''}\")\n",
    "                print(f\"      üìÖ {date} | ‚ù§Ô∏è {likes}\")\n",
    "    \n",
    "    def _calculate_quality_score(self, data: List[Dict], analysis: Dict[str, Any]) -> float:\n",
    "        \"\"\"Calculate data quality score based on completeness and structure.\"\"\"\n",
    "        if not data:\n",
    "            return 0.0\n",
    "        \n",
    "        score = 0\n",
    "        max_score = 100\n",
    "        \n",
    "        # Basic data presence (40 points)\n",
    "        if len(data) > 0:\n",
    "            score += 20\n",
    "        if len(data) >= 5:\n",
    "            score += 20\n",
    "        \n",
    "        # Data structure quality (60 points)\n",
    "        if isinstance(data[0], dict):\n",
    "            # Check for key attributes\n",
    "            first_item = data[0]\n",
    "            \n",
    "            if 'profile' in first_item:\n",
    "                # Comprehensive data structure\n",
    "                profile = first_item['profile']\n",
    "                if profile.get('display_name'): score += 10\n",
    "                if profile.get('username'): score += 10\n",
    "                if profile.get('bio'): score += 5\n",
    "                if profile.get('followers_count') is not None: score += 10\n",
    "                \n",
    "                # Data types presence\n",
    "                if first_item.get('posts'): score += 15\n",
    "                if first_item.get('likes'): score += 5\n",
    "                if first_item.get('media'): score += 5\n",
    "            else:\n",
    "                # Direct posts structure\n",
    "                posts_with_text = sum(1 for p in data if isinstance(p, dict) and p.get('text'))\n",
    "                if posts_with_text > 0: score += 25\n",
    "                if posts_with_text / len(data) > 0.8: score += 15\n",
    "                \n",
    "                posts_with_dates = sum(1 for p in data if isinstance(p, dict) and p.get('date'))\n",
    "                if posts_with_dates > 0: score += 10\n",
    "                \n",
    "                posts_with_metrics = sum(1 for p in data if isinstance(p, dict) and any(k in p for k in ['likes', 'retweets']))\n",
    "                if posts_with_metrics > 0: score += 10\n",
    "        \n",
    "        return min(score, max_score)\n",
    "    \n",
    "    def _get_emoji(self, data_type: str) -> str:\n",
    "        \"\"\"Get emoji for data type.\"\"\"\n",
    "        emojis = {\n",
    "            'posts': 'üìù', 'likes': '‚ù§Ô∏è', 'mentions': '@Ô∏è‚É£',\n",
    "            'media': 'üñºÔ∏è', 'followers': 'üë•', 'following': '‚û°Ô∏è'\n",
    "        }\n",
    "        return emojis.get(data_type, 'üìä')\n",
    "    \n",
    "    def run_comprehensive_tests(self):\n",
    "        \"\"\"Run complete test suite.\"\"\"\n",
    "        print(\"üê¶ TWITTER SCRAPER COMPREHENSIVE TEST SUITE\")\n",
    "        print(\"=\"*80)\n",
    "        print(f\"üìç API Endpoint: {EP}\")\n",
    "        print(f\"üìÅ Storage Path: {SCRAPED}\")\n",
    "        print(f\"üéØ Test Accounts: {list(TEST_ACCOUNTS.keys())}\")\n",
    "        print(f\"üîß Test Configurations: {list(TEST_CONFIGS.keys())}\")\n",
    "        \n",
    "        # Check API connectivity\n",
    "        try:\n",
    "            test_response = requests.get(f\"{EP}/healthz\", timeout=5)\n",
    "            if test_response.status_code == 200:\n",
    "                print(f\"‚úÖ API connectivity: Connected to browser service\")\n",
    "            else:\n",
    "                print(f\"‚ö†Ô∏è API connectivity: Unexpected response {test_response.status_code}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå API connectivity: Failed - {e}\")\n",
    "            return\n",
    "        \n",
    "        # Test 1: Basic extraction across multiple accounts\n",
    "        print(f\"\\n\" + \"=\"*80)\n",
    "        print(f\"üß™ TEST PHASE 1: BASIC EXTRACTION ACROSS ACCOUNTS\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        accounts_to_test = [\"naval\", \"paulg\", \"sama\"]\n",
    "        for account in accounts_to_test:\n",
    "            self.total_tests += 1\n",
    "            \n",
    "            payload = {\n",
    "                \"username\": account,\n",
    "                **TEST_CONFIGS[\"basic\"]\n",
    "            }\n",
    "            \n",
    "            test_name = f\"Basic Extraction - @{account}\"\n",
    "            result = self.submit_job(\"twitter\", payload, test_name)\n",
    "            \n",
    "            analysis = self.analyze_extraction_result(result, test_name)\n",
    "            self.results[test_name] = analysis\n",
    "            \n",
    "            if analysis[\"success\"]:\n",
    "                self.successful_tests += 1\n",
    "                if analysis[\"data_extracted\"]:\n",
    "                    self.data_quality_scores.append(analysis[\"quality_score\"])\n",
    "            else:\n",
    "                self.failed_tests += 1\n",
    "            \n",
    "            time.sleep(2)  # Brief pause between tests\n",
    "        \n",
    "        # Test 2: Level comparison on single account\n",
    "        print(f\"\\n\" + \"=\"*80)\n",
    "        print(f\"üß™ TEST PHASE 2: SCRAPE LEVEL COMPARISON\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        test_account = \"vitalikbuterin\"\n",
    "        for level in [1, 2, 3, 4]:\n",
    "            self.total_tests += 1\n",
    "            \n",
    "            payload = {\n",
    "                \"username\": test_account,\n",
    "                \"scrape_posts\": True,\n",
    "                \"max_posts\": 8,\n",
    "                \"scrape_level\": level,\n",
    "                \"level\": level\n",
    "            }\n",
    "            \n",
    "            test_name = f\"Level {level} Extraction - @{test_account}\"\n",
    "            result = self.submit_job(\"twitter\", payload, test_name)\n",
    "            \n",
    "            analysis = self.analyze_extraction_result(result, test_name)\n",
    "            self.results[test_name] = analysis\n",
    "            \n",
    "            if analysis[\"success\"]:\n",
    "                self.successful_tests += 1\n",
    "                if analysis[\"data_extracted\"]:\n",
    "                    self.data_quality_scores.append(analysis[\"quality_score\"])\n",
    "            else:\n",
    "                self.failed_tests += 1\n",
    "        \n",
    "        # Test 3: Comprehensive extraction with all features\n",
    "        print(f\"\\n\" + \"=\"*80)\n",
    "        print(f\"üß™ TEST PHASE 3: COMPREHENSIVE EXTRACTION\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        self.total_tests += 1\n",
    "        \n",
    "        payload = {\n",
    "            \"username\": \"naval\",\n",
    "            **TEST_CONFIGS[\"comprehensive\"]\n",
    "        }\n",
    "        \n",
    "        test_name = \"Comprehensive Extraction - @naval\"\n",
    "        result = self.submit_job(\"twitter\", payload, test_name)\n",
    "        \n",
    "        analysis = self.analyze_extraction_result(result, test_name)\n",
    "        self.results[test_name] = analysis\n",
    "        \n",
    "        if analysis[\"success\"]:\n",
    "            self.successful_tests += 1\n",
    "            if analysis[\"data_extracted\"]:\n",
    "                self.data_quality_scores.append(analysis[\"quality_score\"])\n",
    "        else:\n",
    "            self.failed_tests += 1\n",
    "        \n",
    "        # Test 4: Date filtering performance\n",
    "        print(f\"\\n\" + \"=\"*80)\n",
    "        print(f\"üß™ TEST PHASE 4: DATE FILTERING PERFORMANCE\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        date_ranges = [\"last_day\", \"last_week\"]\n",
    "        for date_range in date_ranges:\n",
    "            self.total_tests += 1\n",
    "            \n",
    "            payload = {\n",
    "                \"username\": \"sama\",\n",
    "                \"scrape_posts\": True,\n",
    "                \"max_posts\": 15,\n",
    "                \"enable_date_filtering\": True,\n",
    "                \"date_range\": date_range,\n",
    "                \"scrape_level\": 4\n",
    "            }\n",
    "            \n",
    "            test_name = f\"Date Filter ({date_range}) - @sama\"\n",
    "            result = self.submit_job(\"twitter\", payload, test_name)\n",
    "            \n",
    "            analysis = self.analyze_extraction_result(result, test_name)\n",
    "            self.results[test_name] = analysis\n",
    "            \n",
    "            if analysis[\"success\"]:\n",
    "                self.successful_tests += 1\n",
    "                if analysis[\"data_extracted\"]:\n",
    "                    self.data_quality_scores.append(analysis[\"quality_score\"])\n",
    "            else:\n",
    "                self.failed_tests += 1\n",
    "        \n",
    "        # Generate final report\n",
    "        self.generate_final_report()\n",
    "    \n",
    "    def generate_final_report(self):\n",
    "        \"\"\"Generate comprehensive final report.\"\"\"\n",
    "        print(f\"\\n\" + \"=\"*80)\n",
    "        print(f\"üìä COMPREHENSIVE TEST RESULTS & ANALYSIS\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        # Overall statistics\n",
    "        success_rate = (self.successful_tests / self.total_tests * 100) if self.total_tests > 0 else 0\n",
    "        avg_quality = sum(self.data_quality_scores) / len(self.data_quality_scores) if self.data_quality_scores else 0\n",
    "        \n",
    "        print(f\"üìà OVERALL STATISTICS:\")\n",
    "        print(f\"   üéØ Total Tests: {self.total_tests}\")\n",
    "        print(f\"   ‚úÖ Successful: {self.successful_tests} ({success_rate:.1f}%)\")\n",
    "        print(f\"   ‚ùå Failed: {self.failed_tests}\")\n",
    "        print(f\"   üìä Tests with Data: {len(self.data_quality_scores)}\")\n",
    "        print(f\"   üèÜ Average Quality Score: {avg_quality:.1f}%\")\n",
    "        \n",
    "        # Detailed results breakdown\n",
    "        print(f\"\\nüìã DETAILED TEST RESULTS:\")\n",
    "        for test_name, analysis in self.results.items():\n",
    "            status = \"‚úÖ\" if analysis[\"success\"] else \"‚ùå\"\n",
    "            data_status = f\"({analysis['total_items']} items)\" if analysis[\"data_extracted\"] else \"(no data)\"\n",
    "            quality = f\"Q:{analysis['quality_score']:.0f}%\" if analysis[\"quality_score\"] > 0 else \"Q:0%\"\n",
    "            \n",
    "            print(f\"   {status} {test_name}: {data_status} {quality}\")\n",
    "            \n",
    "            # Show issues if any\n",
    "            if analysis[\"issues\"]:\n",
    "                for issue in analysis[\"issues\"]:\n",
    "                    print(f\"      ‚ö†Ô∏è {issue}\")\n",
    "        \n",
    "        # Data extraction analysis\n",
    "        tests_with_data = sum(1 for a in self.results.values() if a[\"data_extracted\"])\n",
    "        total_items_extracted = sum(a[\"total_items\"] for a in self.results.values())\n",
    "        \n",
    "        print(f\"\\nüìä DATA EXTRACTION ANALYSIS:\")\n",
    "        print(f\"   üìà Tests with Data: {tests_with_data}/{self.total_tests} ({tests_with_data/self.total_tests*100:.1f}%)\")\n",
    "        print(f\"   üìä Total Items Extracted: {total_items_extracted}\")\n",
    "        print(f\"   üìà Average Items per Successful Test: {total_items_extracted/tests_with_data:.1f}\" if tests_with_data > 0 else \"   üìà Average Items: 0\")\n",
    "        \n",
    "        # Data types breakdown\n",
    "        data_type_counts = {}\n",
    "        for analysis in self.results.values():\n",
    "            for data_type, count in analysis.get(\"data_types\", {}).items():\n",
    "                data_type_counts[data_type] = data_type_counts.get(data_type, 0) + count\n",
    "        \n",
    "        if data_type_counts:\n",
    "            print(f\"\\nüìä DATA TYPES EXTRACTED:\")\n",
    "            for data_type, count in sorted(data_type_counts.items()):\n",
    "                emoji = self._get_emoji(data_type)\n",
    "                print(f\"   {emoji} {data_type.title()}: {count} total items\")\n",
    "        \n",
    "        # Performance assessment\n",
    "        print(f\"\\nüéØ OVERALL ASSESSMENT:\")\n",
    "        if success_rate >= 90 and avg_quality >= 70:\n",
    "            print(f\"üéâ EXCELLENT - Twitter scraper is working perfectly!\")\n",
    "        elif success_rate >= 75 and avg_quality >= 50:\n",
    "            print(f\"‚úÖ GOOD - Twitter scraper is working well with minor issues\")\n",
    "        elif success_rate >= 50:\n",
    "            print(f\"‚ö†Ô∏è FAIR - Twitter scraper has issues that need attention\")\n",
    "        else:\n",
    "            print(f\"‚ùå POOR - Twitter scraper needs significant fixes\")\n",
    "        \n",
    "        # Storage information\n",
    "        twitter_dir = SCRAPED / \"twitter\"\n",
    "        if twitter_dir.exists():\n",
    "            recent_jobs = sorted([d.name for d in twitter_dir.iterdir() if d.is_dir()], reverse=True)[:5]\n",
    "            print(f\"\\nüìÅ RECENT JOB DATA:\")\n",
    "            for job_id in recent_jobs:\n",
    "                job_path = twitter_dir / job_id\n",
    "                files = list(job_path.glob(\"*\")) if job_path.exists() else []\n",
    "                print(f\"   üìÇ {job_id}: {len(files)} files\")\n",
    "        \n",
    "        print(f\"\\nüîó Raw data files: {SCRAPED}/twitter/\")\n",
    "        print(f\"üéâ COMPREHENSIVE TESTING COMPLETED!\")\n",
    "\n",
    "# Initialize and run comprehensive test suite\n",
    "if __name__ == \"__main__\":\n",
    "    test_suite = TwitterTestSuite()\n",
    "    test_suite.run_comprehensive_tests()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
