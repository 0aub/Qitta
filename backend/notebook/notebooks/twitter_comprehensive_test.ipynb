{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🐦 Twitter Scraper - Comprehensive Testing & Validation\n",
    "\n",
    "**Complete end-to-end testing suite for Twitter scraper with detailed analysis**\n",
    "\n",
    "This notebook provides:\n",
    "- ✅ Complete extraction testing across multiple accounts and levels\n",
    "- 📊 Detailed data validation and quality analysis\n",
    "- 🔍 Performance metrics and success rate tracking\n",
    "- 🎯 Variable counting and comprehensive result analysis\n",
    "- 🚀 All results in a single output for easy tracing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Auto-detected browser endpoint: http://browser:8004\n",
      "🐦 TWITTER SCRAPER COMPREHENSIVE TEST SUITE\n",
      "================================================================================\n",
      "📍 API Endpoint: http://browser:8004\n",
      "📁 Storage Path: /storage/scraped_data\n",
      "🎯 Test Accounts: ['naval', 'elonmusk', 'paulg', 'sama', 'vitalikbuterin']\n",
      "🔧 Test Configurations: ['basic', 'enhanced', 'comprehensive', 'date_filtered']\n",
      "✅ API connectivity: Connected to browser service\n",
      "\n",
      "================================================================================\n",
      "🧪 TEST PHASE 1: BASIC EXTRACTION ACROSS ACCOUNTS\n",
      "================================================================================\n",
      "\n",
      "🚀 SUBMITTING: Basic Extraction - @naval\n",
      "📝 Payload: {\n",
      "  \"username\": \"naval\",\n",
      "  \"scrape_posts\": true,\n",
      "  \"max_posts\": 10,\n",
      "  \"scrape_level\": 1\n",
      "}\n",
      "🆔 Job ID: 2e4a666a2d2b4d58ba0d499ea725af7e\n",
      "⏳ Waiting for job 2e4a666a2d2b4d58ba0d499ea725af7e...\n",
      "⏱️  running 18s (18s)\n",
      "✅ FINISHED in 21.0s\n",
      "\n",
      "================================================================================\n",
      "🔍 ANALYZING: Basic Extraction - @naval\n",
      "================================================================================\n",
      "✅ STATUS: Task completed successfully\n",
      "🎯 TARGET: @naval\n",
      "📊 EXTRACTION METHOD: level_1_basic\n",
      "📈 SCRAPE LEVEL: 1\n",
      "📈 SUCCESS RATE: 10.0%\n",
      "✅ TOTAL EXTRACTED: 1 items\n",
      "\n",
      "📋 COMPREHENSIVE USER DATA ANALYSIS:\n",
      "   👤 Profile: Name='Naval' | Bio=14 chars\n",
      "   📊 Stats: 2.8M followers | 0 following\n",
      "\n",
      "📊 EXTRACTED DATA BREAKDOWN:\n",
      "   📝 Posts: 8 items\n",
      "      📝 Sample: How to Get Rich (without getting lucky):...\n",
      "\n",
      "📈 TOTAL DATA ITEMS: 8 across all categories\n",
      "\n",
      "📊 DATA QUALITY SCORE: 70%\n",
      "✅ GOOD data quality\n",
      "\n",
      "🚀 SUBMITTING: Basic Extraction - @paulg\n",
      "📝 Payload: {\n",
      "  \"username\": \"paulg\",\n",
      "  \"scrape_posts\": true,\n",
      "  \"max_posts\": 10,\n",
      "  \"scrape_level\": 1\n",
      "}\n",
      "🆔 Job ID: ff06aff1406f4e81b7aad85048f0ed02\n",
      "⏳ Waiting for job ff06aff1406f4e81b7aad85048f0ed02...\n",
      "⏱️  running 27s (27s)\n",
      "✅ FINISHED in 30.0s\n",
      "\n",
      "================================================================================\n",
      "🔍 ANALYZING: Basic Extraction - @paulg\n",
      "================================================================================\n",
      "✅ STATUS: Task completed successfully\n",
      "🎯 TARGET: @paulg\n",
      "📊 EXTRACTION METHOD: level_1_basic\n",
      "📈 SCRAPE LEVEL: 1\n",
      "📈 SUCCESS RATE: 10.0%\n",
      "✅ TOTAL EXTRACTED: 1 items\n",
      "\n",
      "📋 COMPREHENSIVE USER DATA ANALYSIS:\n",
      "   👤 Profile: Name='Paul Graham' | Bio=500 chars\n",
      "   📊 Stats: 2M followers | 779 following\n",
      "\n",
      "📊 EXTRACTED DATA BREAKDOWN:\n",
      "   📝 Posts: 8 items\n",
      "      📝 Sample: Homelessness should be reframed into 3 groups....\n",
      "\n",
      "📈 TOTAL DATA ITEMS: 8 across all categories\n",
      "\n",
      "📊 DATA QUALITY SCORE: 70%\n",
      "✅ GOOD data quality\n",
      "\n",
      "🚀 SUBMITTING: Basic Extraction - @sama\n",
      "📝 Payload: {\n",
      "  \"username\": \"sama\",\n",
      "  \"scrape_posts\": true,\n",
      "  \"max_posts\": 10,\n",
      "  \"scrape_level\": 1\n",
      "}\n",
      "🆔 Job ID: 45df80ff474a473bad1854dc5e4e9ec0\n",
      "⏳ Waiting for job 45df80ff474a473bad1854dc5e4e9ec0...\n",
      "⏱️  running 30s (30s)\n",
      "✅ FINISHED in 33.0s\n",
      "\n",
      "================================================================================\n",
      "🔍 ANALYZING: Basic Extraction - @sama\n",
      "================================================================================\n",
      "✅ STATUS: Task completed successfully\n",
      "🎯 TARGET: @sama\n",
      "📊 EXTRACTION METHOD: level_1_basic\n",
      "📈 SCRAPE LEVEL: 1\n",
      "📈 SUCCESS RATE: 10.0%\n",
      "✅ TOTAL EXTRACTED: 1 items\n",
      "\n",
      "📋 COMPREHENSIVE USER DATA ANALYSIS:\n",
      "   👤 Profile: Name='Sam Altman' | Bio=18 chars\n",
      "   📊 Stats: 3.9M followers | 969 following\n",
      "\n",
      "📊 EXTRACTED DATA BREAKDOWN:\n",
      "   📝 Posts: 8 items\n",
      "      📝 Sample: You can now use gpt-5-codex to investigate and find critical...\n",
      "\n",
      "📈 TOTAL DATA ITEMS: 8 across all categories\n",
      "\n",
      "📊 DATA QUALITY SCORE: 70%\n",
      "✅ GOOD data quality\n",
      "\n",
      "================================================================================\n",
      "🧪 TEST PHASE 2: SCRAPE LEVEL COMPARISON\n",
      "================================================================================\n",
      "\n",
      "🚀 SUBMITTING: Level 1 Extraction - @vitalikbuterin\n",
      "📝 Payload: {\n",
      "  \"username\": \"vitalikbuterin\",\n",
      "  \"scrape_posts\": true,\n",
      "  \"max_posts\": 8,\n",
      "  \"scrape_level\": 1,\n",
      "  \"level\": 1\n",
      "}\n",
      "🆔 Job ID: fe5693ec88a040c8bfb64686b6f0d5f6\n",
      "⏳ Waiting for job fe5693ec88a040c8bfb64686b6f0d5f6...\n",
      "⏱️  running 36s (36s)\n",
      "✅ FINISHED in 39.0s\n",
      "\n",
      "================================================================================\n",
      "🔍 ANALYZING: Level 1 Extraction - @vitalikbuterin\n",
      "================================================================================\n",
      "✅ STATUS: Task completed successfully\n",
      "🎯 TARGET: @vitalikbuterin\n",
      "📊 EXTRACTION METHOD: level_1_basic\n",
      "📈 SCRAPE LEVEL: 1\n",
      "📈 SUCCESS RATE: 12.5%\n",
      "✅ TOTAL EXTRACTED: 1 items\n",
      "\n",
      "📋 COMPREHENSIVE USER DATA ANALYSIS:\n",
      "   👤 Profile: Name='vitalik.eth' | Bio=65 chars\n",
      "   📊 Stats: 5.8M followers | 503 following\n",
      "\n",
      "📊 EXTRACTED DATA BREAKDOWN:\n",
      "   📝 Posts: 8 items\n",
      "      📝 Sample: 0/ The Ethereum Foundation is committed to supporting the ‘C...\n",
      "\n",
      "📈 TOTAL DATA ITEMS: 8 across all categories\n",
      "\n",
      "📊 DATA QUALITY SCORE: 70%\n",
      "✅ GOOD data quality\n",
      "\n",
      "🚀 SUBMITTING: Level 2 Extraction - @vitalikbuterin\n",
      "📝 Payload: {\n",
      "  \"username\": \"vitalikbuterin\",\n",
      "  \"scrape_posts\": true,\n",
      "  \"max_posts\": 8,\n",
      "  \"scrape_level\": 2,\n",
      "  \"level\": 2\n",
      "}\n",
      "🆔 Job ID: c31c85e5a5324c5283e6130537ab8fad\n",
      "⏳ Waiting for job c31c85e5a5324c5283e6130537ab8fad...\n",
      "⏱️  running 36s (36s)\n",
      "✅ FINISHED in 39.0s\n",
      "\n",
      "================================================================================\n",
      "🔍 ANALYZING: Level 2 Extraction - @vitalikbuterin\n",
      "================================================================================\n",
      "✅ STATUS: Task completed successfully\n",
      "🎯 TARGET: @vitalikbuterin\n",
      "📊 EXTRACTION METHOD: level_2_full_profile\n",
      "📈 SCRAPE LEVEL: 2\n",
      "📈 SUCCESS RATE: 12.5%\n",
      "✅ TOTAL EXTRACTED: 1 items\n",
      "\n",
      "📋 COMPREHENSIVE USER DATA ANALYSIS:\n",
      "   👤 Profile: Name='vitalik.eth' | Bio=65 chars\n",
      "   📊 Stats: 5.8M followers | 503 following\n",
      "\n",
      "📊 EXTRACTED DATA BREAKDOWN:\n",
      "   📝 Posts: 8 items\n",
      "      📝 Sample: 0/ The Ethereum Foundation is committed to supporting the ‘C...\n",
      "\n",
      "📈 TOTAL DATA ITEMS: 8 across all categories\n",
      "\n",
      "📊 DATA QUALITY SCORE: 70%\n",
      "✅ GOOD data quality\n",
      "\n",
      "🚀 SUBMITTING: Level 3 Extraction - @vitalikbuterin\n",
      "📝 Payload: {\n",
      "  \"username\": \"vitalikbuterin\",\n",
      "  \"scrape_posts\": true,\n",
      "  \"max_posts\": 8,\n",
      "  \"scrape_level\": 3,\n",
      "  \"level\": 3\n",
      "}\n",
      "🆔 Job ID: 0ae804edf00044d9a44386cc6133c76f\n",
      "⏳ Waiting for job 0ae804edf00044d9a44386cc6133c76f...\n",
      "⏱️  running 45s (45s)\n",
      "✅ FINISHED in 48.0s\n",
      "\n",
      "================================================================================\n",
      "🔍 ANALYZING: Level 3 Extraction - @vitalikbuterin\n",
      "================================================================================\n",
      "✅ STATUS: Task completed successfully\n",
      "🎯 TARGET: @vitalikbuterin\n",
      "📊 EXTRACTION METHOD: level_3_with_media\n",
      "📈 SCRAPE LEVEL: 3\n",
      "📈 SUCCESS RATE: 12.5%\n",
      "✅ TOTAL EXTRACTED: 1 items\n",
      "\n",
      "📋 COMPREHENSIVE USER DATA ANALYSIS:\n",
      "   👤 Profile: Name='vitalik.eth' | Bio=65 chars\n",
      "   📊 Stats: 5.8M followers | 503 following\n",
      "\n",
      "📊 EXTRACTED DATA BREAKDOWN:\n",
      "   📝 Posts: 8 items\n",
      "      📝 Sample: 0/ The Ethereum Foundation is committed to supporting the ‘C...\n",
      "   🖼️ Media: 25 items\n",
      "      📝 Sample: I choose balance. First-level balance....\n",
      "\n",
      "📈 TOTAL DATA ITEMS: 33 across all categories\n",
      "\n",
      "📊 DATA QUALITY SCORE: 75%\n",
      "✅ GOOD data quality\n",
      "\n",
      "🚀 SUBMITTING: Level 4 Extraction - @vitalikbuterin\n",
      "📝 Payload: {\n",
      "  \"username\": \"vitalikbuterin\",\n",
      "  \"scrape_posts\": true,\n",
      "  \"max_posts\": 8,\n",
      "  \"scrape_level\": 4,\n",
      "  \"level\": 4\n",
      "}\n",
      "🆔 Job ID: 3091121867e046f9b089742645d10332\n",
      "⏳ Waiting for job 3091121867e046f9b089742645d10332...\n",
      "⏱️  running 2m 54s (174s)\n",
      "✅ FINISHED in 177.2s\n",
      "\n",
      "================================================================================\n",
      "🔍 ANALYZING: Level 4 Extraction - @vitalikbuterin\n",
      "================================================================================\n",
      "✅ STATUS: Task completed successfully\n",
      "🎯 TARGET: @vitalikbuterin\n",
      "📊 EXTRACTION METHOD: level_4_comprehensive\n",
      "📈 SCRAPE LEVEL: 4\n",
      "📈 SUCCESS RATE: 12.5%\n",
      "✅ TOTAL EXTRACTED: 1 items\n",
      "\n",
      "📋 COMPREHENSIVE USER DATA ANALYSIS:\n",
      "   👤 Profile: Name='vitalik.eth' | Bio=65 chars\n",
      "   📊 Stats: 5.8M followers | 503 following\n",
      "\n",
      "📊 EXTRACTED DATA BREAKDOWN:\n",
      "   📝 Posts: 50 items\n",
      "      📝 Sample: 0/ The Ethereum Foundation is committed to supporting the ‘C...\n",
      "\n",
      "📈 TOTAL DATA ITEMS: 50 across all categories\n",
      "\n",
      "📊 DATA QUALITY SCORE: 70%\n",
      "✅ GOOD data quality\n",
      "\n",
      "================================================================================\n",
      "🧪 TEST PHASE 3: COMPREHENSIVE EXTRACTION\n",
      "================================================================================\n",
      "\n",
      "🚀 SUBMITTING: Comprehensive Extraction - @naval\n",
      "📝 Payload: {\n",
      "  \"username\": \"naval\",\n",
      "  \"scrape_posts\": true,\n",
      "  \"max_posts\": 20,\n",
      "  \"scrape_likes\": true,\n",
      "  \"max_likes\": 10,\n",
      "  \"scrape_mentions\": true,\n",
      "  \"max_mentions\": 5,\n",
      "  \"scrape_media\": true,\n",
      "  \"max_media\": 5,\n",
      "  \"scrape_level\": 4\n",
      "}\n",
      "🆔 Job ID: a920a9082aef43cab05a14ed6893a6eb\n",
      "⏳ Waiting for job a920a9082aef43cab05a14ed6893a6eb...\n",
      "⏱️  running 4m 42s (282s)\n",
      "✅ FINISHED in 285.3s\n",
      "\n",
      "================================================================================\n",
      "🔍 ANALYZING: Comprehensive Extraction - @naval\n",
      "================================================================================\n",
      "✅ STATUS: Task completed successfully\n",
      "🎯 TARGET: @naval\n",
      "📊 EXTRACTION METHOD: level_4_comprehensive\n",
      "📈 SCRAPE LEVEL: 4\n",
      "📈 SUCCESS RATE: 5.0%\n",
      "✅ TOTAL EXTRACTED: 1 items\n",
      "\n",
      "📋 COMPREHENSIVE USER DATA ANALYSIS:\n",
      "   👤 Profile: Name='Naval' | Bio=14 chars\n",
      "   📊 Stats: 2.8M followers | 0 following\n",
      "\n",
      "📊 EXTRACTED DATA BREAKDOWN:\n",
      "   📝 Posts: 50 items\n",
      "      📝 Sample: How to Get Rich (without getting lucky):...\n",
      "   ❤️ Likes: 1 items\n",
      "      📝 Sample: {'type': 'access_restricted', 'message': 'Likes for @naval a...\n",
      "   @️⃣ Mentions: 5 items\n",
      "      📝 Sample: I would love to meet their mother. Wow!...\n",
      "   🖼️ Media: 5 items\n",
      "      📝 Sample: {'type': 'image', 'url': 'https://pbs.twimg.com/media/G0CCur...\n",
      "\n",
      "📈 TOTAL DATA ITEMS: 61 across all categories\n",
      "\n",
      "📊 DATA QUALITY SCORE: 80%\n",
      "🎉 EXCELLENT data quality\n",
      "\n",
      "================================================================================\n",
      "🧪 TEST PHASE 4: DATE FILTERING PERFORMANCE\n",
      "================================================================================\n",
      "\n",
      "🚀 SUBMITTING: Date Filter (last_day) - @sama\n",
      "📝 Payload: {\n",
      "  \"username\": \"sama\",\n",
      "  \"scrape_posts\": true,\n",
      "  \"max_posts\": 15,\n",
      "  \"enable_date_filtering\": true,\n",
      "  \"date_range\": \"last_day\",\n",
      "  \"scrape_level\": 4\n",
      "}\n",
      "🆔 Job ID: f6ed52b947b147e794780cee6dbcc212\n",
      "⏳ Waiting for job f6ed52b947b147e794780cee6dbcc212...\n",
      "⏱️  running 1m 33s (93s)\n",
      "✅ FINISHED in 96.1s\n",
      "\n",
      "================================================================================\n",
      "🔍 ANALYZING: Date Filter (last_day) - @sama\n",
      "================================================================================\n",
      "✅ STATUS: Task completed successfully\n",
      "🎯 TARGET: @sama\n",
      "📊 EXTRACTION METHOD: level_4_comprehensive\n",
      "📈 SCRAPE LEVEL: 4\n",
      "📈 SUCCESS RATE: 6.7%\n",
      "✅ TOTAL EXTRACTED: 1 items\n",
      "\n",
      "📋 COMPREHENSIVE USER DATA ANALYSIS:\n",
      "   👤 Profile: Name='Sam Altman' | Bio=18 chars\n",
      "   📊 Stats: 3.9M followers | 969 following\n",
      "\n",
      "📊 EXTRACTED DATA BREAKDOWN:\n",
      "   📝 Posts: 38 items\n",
      "      📝 Sample: You can now use gpt-5-codex to investigate and find critical...\n",
      "\n",
      "📈 TOTAL DATA ITEMS: 38 across all categories\n",
      "\n",
      "📊 DATA QUALITY SCORE: 70%\n",
      "✅ GOOD data quality\n",
      "\n",
      "🚀 SUBMITTING: Date Filter (last_week) - @sama\n",
      "📝 Payload: {\n",
      "  \"username\": \"sama\",\n",
      "  \"scrape_posts\": true,\n",
      "  \"max_posts\": 15,\n",
      "  \"enable_date_filtering\": true,\n",
      "  \"date_range\": \"last_week\",\n",
      "  \"scrape_level\": 4\n",
      "}\n",
      "🆔 Job ID: 07a8e453bc064a93a0e823b4dd236db5\n",
      "⏳ Waiting for job 07a8e453bc064a93a0e823b4dd236db5...\n",
      "⏱️  running 3m 15s (195s)\n",
      "✅ FINISHED in 198.2s\n",
      "\n",
      "================================================================================\n",
      "🔍 ANALYZING: Date Filter (last_week) - @sama\n",
      "================================================================================\n",
      "✅ STATUS: Task completed successfully\n",
      "🎯 TARGET: @sama\n",
      "📊 EXTRACTION METHOD: level_4_comprehensive\n",
      "📈 SCRAPE LEVEL: 4\n",
      "📈 SUCCESS RATE: 6.7%\n",
      "✅ TOTAL EXTRACTED: 1 items\n",
      "\n",
      "📋 COMPREHENSIVE USER DATA ANALYSIS:\n",
      "   👤 Profile: Name='Sam Altman' | Bio=18 chars\n",
      "   📊 Stats: 3.9M followers | 969 following\n",
      "\n",
      "📊 EXTRACTED DATA BREAKDOWN:\n",
      "   📝 Posts: 50 items\n",
      "      📝 Sample: You can now use gpt-5-codex to investigate and find critical...\n",
      "\n",
      "📈 TOTAL DATA ITEMS: 50 across all categories\n",
      "\n",
      "📊 DATA QUALITY SCORE: 70%\n",
      "✅ GOOD data quality\n",
      "\n",
      "================================================================================\n",
      "📊 COMPREHENSIVE TEST RESULTS & ANALYSIS\n",
      "================================================================================\n",
      "📈 OVERALL STATISTICS:\n",
      "   🎯 Total Tests: 10\n",
      "   ✅ Successful: 10 (100.0%)\n",
      "   ❌ Failed: 0\n",
      "   📊 Tests with Data: 10\n",
      "   🏆 Average Quality Score: 71.5%\n",
      "\n",
      "📋 DETAILED TEST RESULTS:\n",
      "   ✅ Basic Extraction - @naval: (8 items) Q:70%\n",
      "   ✅ Basic Extraction - @paulg: (8 items) Q:70%\n",
      "   ✅ Basic Extraction - @sama: (8 items) Q:70%\n",
      "   ✅ Level 1 Extraction - @vitalikbuterin: (8 items) Q:70%\n",
      "   ✅ Level 2 Extraction - @vitalikbuterin: (8 items) Q:70%\n",
      "   ✅ Level 3 Extraction - @vitalikbuterin: (33 items) Q:75%\n",
      "   ✅ Level 4 Extraction - @vitalikbuterin: (50 items) Q:70%\n",
      "   ✅ Comprehensive Extraction - @naval: (61 items) Q:80%\n",
      "   ✅ Date Filter (last_day) - @sama: (38 items) Q:70%\n",
      "   ✅ Date Filter (last_week) - @sama: (50 items) Q:70%\n",
      "\n",
      "📊 DATA EXTRACTION ANALYSIS:\n",
      "   📈 Tests with Data: 10/10 (100.0%)\n",
      "   📊 Total Items Extracted: 272\n",
      "   📈 Average Items per Successful Test: 27.2\n",
      "\n",
      "📊 DATA TYPES EXTRACTED:\n",
      "   ❤️ Likes: 1 total items\n",
      "   🖼️ Media: 30 total items\n",
      "   @️⃣ Mentions: 5 total items\n",
      "   📝 Posts: 236 total items\n",
      "   📊 Profile: 10 total items\n",
      "\n",
      "🎯 OVERALL ASSESSMENT:\n",
      "🎉 EXCELLENT - Twitter scraper is working perfectly!\n",
      "\n",
      "📁 RECENT JOB DATA:\n",
      "   📂 ff06aff1406f4e81b7aad85048f0ed02: 2 files\n",
      "   📂 fe5693ec88a040c8bfb64686b6f0d5f6: 2 files\n",
      "   📂 f6ed52b947b147e794780cee6dbcc212: 2 files\n",
      "   📂 c31c85e5a5324c5283e6130537ab8fad: 2 files\n",
      "   📂 a920a9082aef43cab05a14ed6893a6eb: 2 files\n",
      "\n",
      "🔗 Raw data files: /storage/scraped_data/twitter/\n",
      "🎉 COMPREHENSIVE TESTING COMPLETED!\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "🐦 COMPREHENSIVE TWITTER SCRAPER TEST SUITE\n",
    "==========================================\n",
    "\n",
    "Complete testing and validation suite for Twitter scraper functionality.\n",
    "Provides detailed analysis, validation, and performance metrics in a single output.\n",
    "\"\"\"\n",
    "\n",
    "import os, time, pathlib, pprint, requests, json\n",
    "from datetime import datetime, timedelta\n",
    "from typing import Dict, List, Any, Optional\n",
    "\n",
    "# Configuration - Auto-detect correct endpoint\n",
    "def find_browser_endpoint():\n",
    "    \"\"\"Auto-detect the correct browser endpoint.\"\"\"\n",
    "    import urllib.request\n",
    "    \n",
    "    # Try different possible endpoints\n",
    "    endpoints = [\n",
    "        \"http://browser:8001\",\n",
    "        \"http://browser:8004\", \n",
    "        \"http://localhost:8001\",\n",
    "        \"http://localhost:8004\"\n",
    "    ]\n",
    "    \n",
    "    for endpoint in endpoints:\n",
    "        try:\n",
    "            with urllib.request.urlopen(f\"{endpoint}/healthz\", timeout=2) as response:\n",
    "                if response.status == 200:\n",
    "                    print(f\"🔍 Auto-detected browser endpoint: {endpoint}\")\n",
    "                    return endpoint\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    # Default fallback\n",
    "    return \"http://localhost:8001\"\n",
    "\n",
    "EP = find_browser_endpoint()  # Auto-detect correct endpoint\n",
    "SCRAPED = pathlib.Path(\"/storage/scraped_data\")\n",
    "\n",
    "# Test accounts with different characteristics\n",
    "TEST_ACCOUNTS = {\n",
    "    \"naval\": \"High-quality tweets, philosophy\",\n",
    "    \"elonmusk\": \"High activity, mixed content\",\n",
    "    \"paulg\": \"Startup advice, essays\",\n",
    "    \"sama\": \"AI/tech commentary\",\n",
    "    \"vitalikbuterin\": \"Crypto/blockchain content\"\n",
    "}\n",
    "\n",
    "# Test configurations\n",
    "TEST_CONFIGS = {\n",
    "    \"basic\": {\n",
    "        \"scrape_posts\": True,\n",
    "        \"max_posts\": 10,\n",
    "        \"scrape_level\": 1\n",
    "    },\n",
    "    \"enhanced\": {\n",
    "        \"scrape_posts\": True,\n",
    "        \"max_posts\": 15,\n",
    "        \"scrape_likes\": True,\n",
    "        \"max_likes\": 5,\n",
    "        \"scrape_level\": 2\n",
    "    },\n",
    "    \"comprehensive\": {\n",
    "        \"scrape_posts\": True,\n",
    "        \"max_posts\": 20,\n",
    "        \"scrape_likes\": True,\n",
    "        \"max_likes\": 10,\n",
    "        \"scrape_mentions\": True,\n",
    "        \"max_mentions\": 5,\n",
    "        \"scrape_media\": True,\n",
    "        \"max_media\": 5,\n",
    "        \"scrape_level\": 4\n",
    "    },\n",
    "    \"date_filtered\": {\n",
    "        \"scrape_posts\": True,\n",
    "        \"max_posts\": 25,\n",
    "        \"enable_date_filtering\": True,\n",
    "        \"date_range\": \"last_week\",\n",
    "        \"scrape_level\": 4\n",
    "    }\n",
    "}\n",
    "\n",
    "class TwitterTestSuite:\n",
    "    \"\"\"Comprehensive Twitter scraper test suite.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.results = {}\n",
    "        self.performance_metrics = {}\n",
    "        self.validation_results = {}\n",
    "        self.total_tests = 0\n",
    "        self.successful_tests = 0\n",
    "        self.failed_tests = 0\n",
    "        self.data_quality_scores = []\n",
    "        \n",
    "    def wait_for_job(self, job_id: str, every: int = 3) -> Dict[str, Any]:\n",
    "        \"\"\"Wait for job completion and return result.\"\"\"\n",
    "        print(f\"⏳ Waiting for job {job_id}...\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        while True:\n",
    "            try:\n",
    "                rec = requests.get(f\"{EP}/jobs/{job_id}\", timeout=10).json()\n",
    "                status = rec[\"status\"]\n",
    "                \n",
    "                if status not in {\"finished\", \"error\"}:\n",
    "                    elapsed = time.time() - start_time\n",
    "                    print(f\"\\r⏱️  {rec.get('status_with_elapsed', status)} ({elapsed:.0f}s)\", end=\"\")\n",
    "                else:\n",
    "                    elapsed = time.time() - start_time\n",
    "                    print(f\"\\n✅ {status.upper()} in {elapsed:.1f}s\")\n",
    "                    return rec\n",
    "                    \n",
    "                time.sleep(every)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"\\n❌ Error checking job status: {e}\")\n",
    "                return {\"status\": \"error\", \"error\": str(e)}\n",
    "\n",
    "    def submit_job(self, task: str, payload: Dict[str, Any], test_name: str) -> Dict[str, Any]:\n",
    "        \"\"\"Submit job and wait for completion.\"\"\"\n",
    "        print(f\"\\n🚀 SUBMITTING: {test_name}\")\n",
    "        print(f\"📝 Payload: {json.dumps(payload, indent=2)}\")\n",
    "        \n",
    "        try:\n",
    "            r = requests.post(f\"{EP}/jobs/{task}\", json=payload, timeout=30)\n",
    "            r.raise_for_status()\n",
    "            jid = r.json()[\"job_id\"]\n",
    "            print(f\"🆔 Job ID: {jid}\")\n",
    "            \n",
    "            result = self.wait_for_job(jid)\n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Job submission failed: {e}\")\n",
    "            return {\"status\": \"error\", \"error\": str(e)}\n",
    "\n",
    "    def analyze_extraction_result(self, result: Dict[str, Any], test_name: str) -> Dict[str, Any]:\n",
    "        \"\"\"Comprehensive analysis of extraction results.\"\"\"\n",
    "        print(f\"\\n\" + \"=\"*80)\n",
    "        print(f\"🔍 ANALYZING: {test_name}\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        analysis = {\n",
    "            \"test_name\": test_name,\n",
    "            \"status\": result.get(\"status\", \"unknown\"),\n",
    "            \"job_id\": result.get(\"job_id\", \"N/A\"),\n",
    "            \"execution_time\": 0,\n",
    "            \"data_extracted\": False,\n",
    "            \"total_items\": 0,\n",
    "            \"data_types\": {},\n",
    "            \"quality_score\": 0,\n",
    "            \"issues\": [],\n",
    "            \"success\": False\n",
    "        }\n",
    "        \n",
    "        if result[\"status\"] == \"error\":\n",
    "            error_msg = result.get('error', 'Unknown error')\n",
    "            print(f\"❌ FAILED: {error_msg}\")\n",
    "            analysis[\"issues\"].append(f\"Job failed: {error_msg}\")\n",
    "            return analysis\n",
    "        \n",
    "        if \"result\" not in result:\n",
    "            print(f\"❌ No result data found\")\n",
    "            analysis[\"issues\"].append(\"No result data in response\")\n",
    "            return analysis\n",
    "        \n",
    "        res = result[\"result\"]\n",
    "        metadata = res.get(\"search_metadata\", {})\n",
    "        data = res.get(\"data\", [])\n",
    "        \n",
    "        # Basic extraction info\n",
    "        print(f\"✅ STATUS: Task completed successfully\")\n",
    "        print(f\"🎯 TARGET: @{metadata.get('target_username', 'N/A')}\")\n",
    "        print(f\"📊 EXTRACTION METHOD: {metadata.get('extraction_method', 'N/A')}\")\n",
    "        print(f\"📈 SCRAPE LEVEL: {metadata.get('scrape_level', 'N/A')}\")\n",
    "        print(f\"📈 SUCCESS RATE: {metadata.get('success_rate', 0):.1%}\")\n",
    "        \n",
    "        # Update analysis\n",
    "        analysis[\"success\"] = True\n",
    "        analysis[\"total_items\"] = len(data)\n",
    "        analysis[\"data_extracted\"] = len(data) > 0\n",
    "        \n",
    "        if not data:\n",
    "            print(f\"⚠️ NO DATA EXTRACTED - 0 items returned\")\n",
    "            analysis[\"issues\"].append(\"No data extracted\")\n",
    "            return analysis\n",
    "        \n",
    "        print(f\"✅ TOTAL EXTRACTED: {len(data)} items\")\n",
    "        \n",
    "        # Analyze data structure\n",
    "        first_item = data[0] if data else {}\n",
    "        \n",
    "        # Check if comprehensive user data (profile + posts structure)\n",
    "        if isinstance(first_item, dict) and 'profile' in first_item:\n",
    "            self._analyze_comprehensive_data(first_item, analysis)\n",
    "        else:\n",
    "            self._analyze_direct_posts(data, analysis)\n",
    "        \n",
    "        # Calculate quality score\n",
    "        quality_score = self._calculate_quality_score(data, analysis)\n",
    "        analysis[\"quality_score\"] = quality_score\n",
    "        \n",
    "        print(f\"\\n📊 DATA QUALITY SCORE: {quality_score:.0f}%\")\n",
    "        if quality_score >= 80:\n",
    "            print(f\"🎉 EXCELLENT data quality\")\n",
    "        elif quality_score >= 60:\n",
    "            print(f\"✅ GOOD data quality\")\n",
    "        elif quality_score >= 40:\n",
    "            print(f\"⚠️ FAIR data quality\")\n",
    "        else:\n",
    "            print(f\"❌ POOR data quality\")\n",
    "        \n",
    "        return analysis\n",
    "    \n",
    "    def _analyze_comprehensive_data(self, data: Dict[str, Any], analysis: Dict[str, Any]):\n",
    "        \"\"\"Analyze comprehensive user data structure.\"\"\"\n",
    "        print(f\"\\n📋 COMPREHENSIVE USER DATA ANALYSIS:\")\n",
    "        \n",
    "        # Profile analysis\n",
    "        profile = data.get('profile', {})\n",
    "        if profile:\n",
    "            print(f\"   👤 Profile: Name='{profile.get('display_name', 'N/A')}' | Bio={len(profile.get('bio', ''))} chars\")\n",
    "            print(f\"   📊 Stats: {profile.get('followers_count', 'N/A')} followers | {profile.get('following_count', 'N/A')} following\")\n",
    "            analysis[\"data_types\"][\"profile\"] = 1\n",
    "        \n",
    "        # Data types analysis\n",
    "        data_types = ['posts', 'likes', 'mentions', 'media', 'followers', 'following']\n",
    "        total_items = 0\n",
    "        \n",
    "        print(f\"\\n📊 EXTRACTED DATA BREAKDOWN:\")\n",
    "        for data_type in data_types:\n",
    "            items = data.get(data_type, [])\n",
    "            if items and isinstance(items, list):\n",
    "                count = len(items)\n",
    "                total_items += count\n",
    "                analysis[\"data_types\"][data_type] = count\n",
    "                \n",
    "                emoji = self._get_emoji(data_type)\n",
    "                print(f\"   {emoji} {data_type.title()}: {count} items\")\n",
    "                \n",
    "                # Show sample if available\n",
    "                if count > 0 and isinstance(items[0], dict):\n",
    "                    sample = items[0]\n",
    "                    sample_text = sample.get('text', sample.get('content', str(sample)))[:60]\n",
    "                    print(f\"      📝 Sample: {sample_text}{'...' if len(str(sample)) > 60 else ''}\")\n",
    "        \n",
    "        analysis[\"total_items\"] = total_items\n",
    "        print(f\"\\n📈 TOTAL DATA ITEMS: {total_items} across all categories\")\n",
    "    \n",
    "    def _analyze_direct_posts(self, data: List[Dict], analysis: Dict[str, Any]):\n",
    "        \"\"\"Analyze direct posts/tweets data.\"\"\"\n",
    "        print(f\"\\n📝 DIRECT POSTS ANALYSIS:\")\n",
    "        print(f\"   📊 Total Posts: {len(data)}\")\n",
    "        \n",
    "        analysis[\"data_types\"][\"posts\"] = len(data)\n",
    "        \n",
    "        # Count different data attributes\n",
    "        posts_with_text = sum(1 for p in data if isinstance(p, dict) and p.get('text'))\n",
    "        posts_with_dates = sum(1 for p in data if isinstance(p, dict) and p.get('date'))\n",
    "        posts_with_metrics = sum(1 for p in data if isinstance(p, dict) and any(k in p for k in ['likes', 'retweets', 'replies']))\n",
    "        \n",
    "        print(f\"   📝 With text: {posts_with_text}/{len(data)} ({posts_with_text/len(data)*100:.0f}%)\")\n",
    "        print(f\"   📅 With dates: {posts_with_dates}/{len(data)} ({posts_with_dates/len(data)*100:.0f}%)\")\n",
    "        print(f\"   📊 With metrics: {posts_with_metrics}/{len(data)} ({posts_with_metrics/len(data)*100:.0f}%)\")\n",
    "        \n",
    "        # Show samples\n",
    "        sample_count = min(3, len(data))\n",
    "        for i in range(sample_count):\n",
    "            post = data[i]\n",
    "            if isinstance(post, dict):\n",
    "                text = post.get('text', 'No text')[:100]\n",
    "                date = post.get('date', 'No date')\n",
    "                likes = post.get('likes', 'N/A')\n",
    "                print(f\"   🐦 Post {i+1}: {text}{'...' if len(post.get('text', '')) > 100 else ''}\")\n",
    "                print(f\"      📅 {date} | ❤️ {likes}\")\n",
    "    \n",
    "    def _calculate_quality_score(self, data: List[Dict], analysis: Dict[str, Any]) -> float:\n",
    "        \"\"\"Calculate data quality score based on completeness and structure.\"\"\"\n",
    "        if not data:\n",
    "            return 0.0\n",
    "        \n",
    "        score = 0\n",
    "        max_score = 100\n",
    "        \n",
    "        # Basic data presence (40 points)\n",
    "        if len(data) > 0:\n",
    "            score += 20\n",
    "        if len(data) >= 5:\n",
    "            score += 20\n",
    "        \n",
    "        # Data structure quality (60 points)\n",
    "        if isinstance(data[0], dict):\n",
    "            # Check for key attributes\n",
    "            first_item = data[0]\n",
    "            \n",
    "            if 'profile' in first_item:\n",
    "                # Comprehensive data structure\n",
    "                profile = first_item['profile']\n",
    "                if profile.get('display_name'): score += 10\n",
    "                if profile.get('username'): score += 10\n",
    "                if profile.get('bio'): score += 5\n",
    "                if profile.get('followers_count') is not None: score += 10\n",
    "                \n",
    "                # Data types presence\n",
    "                if first_item.get('posts'): score += 15\n",
    "                if first_item.get('likes'): score += 5\n",
    "                if first_item.get('media'): score += 5\n",
    "            else:\n",
    "                # Direct posts structure\n",
    "                posts_with_text = sum(1 for p in data if isinstance(p, dict) and p.get('text'))\n",
    "                if posts_with_text > 0: score += 25\n",
    "                if posts_with_text / len(data) > 0.8: score += 15\n",
    "                \n",
    "                posts_with_dates = sum(1 for p in data if isinstance(p, dict) and p.get('date'))\n",
    "                if posts_with_dates > 0: score += 10\n",
    "                \n",
    "                posts_with_metrics = sum(1 for p in data if isinstance(p, dict) and any(k in p for k in ['likes', 'retweets']))\n",
    "                if posts_with_metrics > 0: score += 10\n",
    "        \n",
    "        return min(score, max_score)\n",
    "    \n",
    "    def _get_emoji(self, data_type: str) -> str:\n",
    "        \"\"\"Get emoji for data type.\"\"\"\n",
    "        emojis = {\n",
    "            'posts': '📝', 'likes': '❤️', 'mentions': '@️⃣',\n",
    "            'media': '🖼️', 'followers': '👥', 'following': '➡️'\n",
    "        }\n",
    "        return emojis.get(data_type, '📊')\n",
    "    \n",
    "    def run_comprehensive_tests(self):\n",
    "        \"\"\"Run complete test suite.\"\"\"\n",
    "        print(\"🐦 TWITTER SCRAPER COMPREHENSIVE TEST SUITE\")\n",
    "        print(\"=\"*80)\n",
    "        print(f\"📍 API Endpoint: {EP}\")\n",
    "        print(f\"📁 Storage Path: {SCRAPED}\")\n",
    "        print(f\"🎯 Test Accounts: {list(TEST_ACCOUNTS.keys())}\")\n",
    "        print(f\"🔧 Test Configurations: {list(TEST_CONFIGS.keys())}\")\n",
    "        \n",
    "        # Check API connectivity\n",
    "        try:\n",
    "            test_response = requests.get(f\"{EP}/healthz\", timeout=5)\n",
    "            if test_response.status_code == 200:\n",
    "                print(f\"✅ API connectivity: Connected to browser service\")\n",
    "            else:\n",
    "                print(f\"⚠️ API connectivity: Unexpected response {test_response.status_code}\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ API connectivity: Failed - {e}\")\n",
    "            return\n",
    "        \n",
    "        # Test 1: Basic extraction across multiple accounts\n",
    "        print(f\"\\n\" + \"=\"*80)\n",
    "        print(f\"🧪 TEST PHASE 1: BASIC EXTRACTION ACROSS ACCOUNTS\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        accounts_to_test = [\"naval\", \"paulg\", \"sama\"]\n",
    "        for account in accounts_to_test:\n",
    "            self.total_tests += 1\n",
    "            \n",
    "            payload = {\n",
    "                \"username\": account,\n",
    "                **TEST_CONFIGS[\"basic\"]\n",
    "            }\n",
    "            \n",
    "            test_name = f\"Basic Extraction - @{account}\"\n",
    "            result = self.submit_job(\"twitter\", payload, test_name)\n",
    "            \n",
    "            analysis = self.analyze_extraction_result(result, test_name)\n",
    "            self.results[test_name] = analysis\n",
    "            \n",
    "            if analysis[\"success\"]:\n",
    "                self.successful_tests += 1\n",
    "                if analysis[\"data_extracted\"]:\n",
    "                    self.data_quality_scores.append(analysis[\"quality_score\"])\n",
    "            else:\n",
    "                self.failed_tests += 1\n",
    "            \n",
    "            time.sleep(2)  # Brief pause between tests\n",
    "        \n",
    "        # Test 2: Level comparison on single account\n",
    "        print(f\"\\n\" + \"=\"*80)\n",
    "        print(f\"🧪 TEST PHASE 2: SCRAPE LEVEL COMPARISON\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        test_account = \"vitalikbuterin\"\n",
    "        for level in [1, 2, 3, 4]:\n",
    "            self.total_tests += 1\n",
    "            \n",
    "            payload = {\n",
    "                \"username\": test_account,\n",
    "                \"scrape_posts\": True,\n",
    "                \"max_posts\": 8,\n",
    "                \"scrape_level\": level,\n",
    "                \"level\": level\n",
    "            }\n",
    "            \n",
    "            test_name = f\"Level {level} Extraction - @{test_account}\"\n",
    "            result = self.submit_job(\"twitter\", payload, test_name)\n",
    "            \n",
    "            analysis = self.analyze_extraction_result(result, test_name)\n",
    "            self.results[test_name] = analysis\n",
    "            \n",
    "            if analysis[\"success\"]:\n",
    "                self.successful_tests += 1\n",
    "                if analysis[\"data_extracted\"]:\n",
    "                    self.data_quality_scores.append(analysis[\"quality_score\"])\n",
    "            else:\n",
    "                self.failed_tests += 1\n",
    "        \n",
    "        # Test 3: Comprehensive extraction with all features\n",
    "        print(f\"\\n\" + \"=\"*80)\n",
    "        print(f\"🧪 TEST PHASE 3: COMPREHENSIVE EXTRACTION\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        self.total_tests += 1\n",
    "        \n",
    "        payload = {\n",
    "            \"username\": \"naval\",\n",
    "            **TEST_CONFIGS[\"comprehensive\"]\n",
    "        }\n",
    "        \n",
    "        test_name = \"Comprehensive Extraction - @naval\"\n",
    "        result = self.submit_job(\"twitter\", payload, test_name)\n",
    "        \n",
    "        analysis = self.analyze_extraction_result(result, test_name)\n",
    "        self.results[test_name] = analysis\n",
    "        \n",
    "        if analysis[\"success\"]:\n",
    "            self.successful_tests += 1\n",
    "            if analysis[\"data_extracted\"]:\n",
    "                self.data_quality_scores.append(analysis[\"quality_score\"])\n",
    "        else:\n",
    "            self.failed_tests += 1\n",
    "        \n",
    "        # Test 4: Date filtering performance\n",
    "        print(f\"\\n\" + \"=\"*80)\n",
    "        print(f\"🧪 TEST PHASE 4: DATE FILTERING PERFORMANCE\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        date_ranges = [\"last_day\", \"last_week\"]\n",
    "        for date_range in date_ranges:\n",
    "            self.total_tests += 1\n",
    "            \n",
    "            payload = {\n",
    "                \"username\": \"sama\",\n",
    "                \"scrape_posts\": True,\n",
    "                \"max_posts\": 15,\n",
    "                \"enable_date_filtering\": True,\n",
    "                \"date_range\": date_range,\n",
    "                \"scrape_level\": 4\n",
    "            }\n",
    "            \n",
    "            test_name = f\"Date Filter ({date_range}) - @sama\"\n",
    "            result = self.submit_job(\"twitter\", payload, test_name)\n",
    "            \n",
    "            analysis = self.analyze_extraction_result(result, test_name)\n",
    "            self.results[test_name] = analysis\n",
    "            \n",
    "            if analysis[\"success\"]:\n",
    "                self.successful_tests += 1\n",
    "                if analysis[\"data_extracted\"]:\n",
    "                    self.data_quality_scores.append(analysis[\"quality_score\"])\n",
    "            else:\n",
    "                self.failed_tests += 1\n",
    "        \n",
    "        # Generate final report\n",
    "        self.generate_final_report()\n",
    "    \n",
    "    def generate_final_report(self):\n",
    "        \"\"\"Generate comprehensive final report.\"\"\"\n",
    "        print(f\"\\n\" + \"=\"*80)\n",
    "        print(f\"📊 COMPREHENSIVE TEST RESULTS & ANALYSIS\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        # Overall statistics\n",
    "        success_rate = (self.successful_tests / self.total_tests * 100) if self.total_tests > 0 else 0\n",
    "        avg_quality = sum(self.data_quality_scores) / len(self.data_quality_scores) if self.data_quality_scores else 0\n",
    "        \n",
    "        print(f\"📈 OVERALL STATISTICS:\")\n",
    "        print(f\"   🎯 Total Tests: {self.total_tests}\")\n",
    "        print(f\"   ✅ Successful: {self.successful_tests} ({success_rate:.1f}%)\")\n",
    "        print(f\"   ❌ Failed: {self.failed_tests}\")\n",
    "        print(f\"   📊 Tests with Data: {len(self.data_quality_scores)}\")\n",
    "        print(f\"   🏆 Average Quality Score: {avg_quality:.1f}%\")\n",
    "        \n",
    "        # Detailed results breakdown\n",
    "        print(f\"\\n📋 DETAILED TEST RESULTS:\")\n",
    "        for test_name, analysis in self.results.items():\n",
    "            status = \"✅\" if analysis[\"success\"] else \"❌\"\n",
    "            data_status = f\"({analysis['total_items']} items)\" if analysis[\"data_extracted\"] else \"(no data)\"\n",
    "            quality = f\"Q:{analysis['quality_score']:.0f}%\" if analysis[\"quality_score\"] > 0 else \"Q:0%\"\n",
    "            \n",
    "            print(f\"   {status} {test_name}: {data_status} {quality}\")\n",
    "            \n",
    "            # Show issues if any\n",
    "            if analysis[\"issues\"]:\n",
    "                for issue in analysis[\"issues\"]:\n",
    "                    print(f\"      ⚠️ {issue}\")\n",
    "        \n",
    "        # Data extraction analysis\n",
    "        tests_with_data = sum(1 for a in self.results.values() if a[\"data_extracted\"])\n",
    "        total_items_extracted = sum(a[\"total_items\"] for a in self.results.values())\n",
    "        \n",
    "        print(f\"\\n📊 DATA EXTRACTION ANALYSIS:\")\n",
    "        print(f\"   📈 Tests with Data: {tests_with_data}/{self.total_tests} ({tests_with_data/self.total_tests*100:.1f}%)\")\n",
    "        print(f\"   📊 Total Items Extracted: {total_items_extracted}\")\n",
    "        print(f\"   📈 Average Items per Successful Test: {total_items_extracted/tests_with_data:.1f}\" if tests_with_data > 0 else \"   📈 Average Items: 0\")\n",
    "        \n",
    "        # Data types breakdown\n",
    "        data_type_counts = {}\n",
    "        for analysis in self.results.values():\n",
    "            for data_type, count in analysis.get(\"data_types\", {}).items():\n",
    "                data_type_counts[data_type] = data_type_counts.get(data_type, 0) + count\n",
    "        \n",
    "        if data_type_counts:\n",
    "            print(f\"\\n📊 DATA TYPES EXTRACTED:\")\n",
    "            for data_type, count in sorted(data_type_counts.items()):\n",
    "                emoji = self._get_emoji(data_type)\n",
    "                print(f\"   {emoji} {data_type.title()}: {count} total items\")\n",
    "        \n",
    "        # Performance assessment\n",
    "        print(f\"\\n🎯 OVERALL ASSESSMENT:\")\n",
    "        if success_rate >= 90 and avg_quality >= 70:\n",
    "            print(f\"🎉 EXCELLENT - Twitter scraper is working perfectly!\")\n",
    "        elif success_rate >= 75 and avg_quality >= 50:\n",
    "            print(f\"✅ GOOD - Twitter scraper is working well with minor issues\")\n",
    "        elif success_rate >= 50:\n",
    "            print(f\"⚠️ FAIR - Twitter scraper has issues that need attention\")\n",
    "        else:\n",
    "            print(f\"❌ POOR - Twitter scraper needs significant fixes\")\n",
    "        \n",
    "        # Storage information\n",
    "        twitter_dir = SCRAPED / \"twitter\"\n",
    "        if twitter_dir.exists():\n",
    "            recent_jobs = sorted([d.name for d in twitter_dir.iterdir() if d.is_dir()], reverse=True)[:5]\n",
    "            print(f\"\\n📁 RECENT JOB DATA:\")\n",
    "            for job_id in recent_jobs:\n",
    "                job_path = twitter_dir / job_id\n",
    "                files = list(job_path.glob(\"*\")) if job_path.exists() else []\n",
    "                print(f\"   📂 {job_id}: {len(files)} files\")\n",
    "        \n",
    "        print(f\"\\n🔗 Raw data files: {SCRAPED}/twitter/\")\n",
    "        print(f\"🎉 COMPREHENSIVE TESTING COMPLETED!\")\n",
    "\n",
    "# Initialize and run comprehensive test suite\n",
    "if __name__ == \"__main__\":\n",
    "    test_suite = TwitterTestSuite()\n",
    "    test_suite.run_comprehensive_tests()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
