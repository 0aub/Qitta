{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "üö® ULTRA-SCALE 5K+ DATA EXTRACTION TESTING\n",
    "\n",
    "Patient, methodical testing approach for 5000+ data extraction validation.\n",
    "Implements intelligent batching, real-time progress tracking, and comprehensive validation.\n",
    "\n",
    "This test will take time but provides thorough validation of large-scale capabilities.\n",
    "\"\"\"\n",
    "\n",
    "import os, time, pathlib, pprint, requests, json\n",
    "from datetime import datetime, timedelta\n",
    "from typing import Dict, List, Any, Optional\n",
    "\n",
    "# Configuration - Auto-detect correct endpoint\n",
    "def find_browser_endpoint():\n",
    "    \"\"\"Auto-detect the correct browser endpoint.\"\"\"\n",
    "    import urllib.request\n",
    "    \n",
    "    # Try different possible endpoints\n",
    "    endpoints = [\n",
    "        \"http://browser:8004\", \n",
    "    ]\n",
    "    \n",
    "    for endpoint in endpoints:\n",
    "        try:\n",
    "            with urllib.request.urlopen(f\"{endpoint}/healthz\", timeout=2) as response:\n",
    "                if response.status == 200:\n",
    "                    print(f\"üîç Auto-detected browser endpoint: {endpoint}\")\n",
    "                    return endpoint\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    # Default fallback\n",
    "    return \"http://browser:8004\"\n",
    "\n",
    "EP = find_browser_endpoint()  # Auto-detect correct endpoint\n",
    "\n",
    "class UltraScale5KTester:\n",
    "    \"\"\"\n",
    "    üö® ULTRA-SCALE 5K+ DATA EXTRACTION TESTER\n",
    "    \n",
    "    Patient testing approach for validating 5000+ data extraction capabilities.\n",
    "    Implements progressive testing, intelligent batching, and real-time progress tracking.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, target_username=\"naval\"):\n",
    "        self.target_username = target_username\n",
    "        self.endpoint = EP\n",
    "        self.results = {\n",
    "            \"target_username\": target_username,\n",
    "            \"start_time\": None,\n",
    "            \"batches\": [],\n",
    "            \"total_extracted\": {},\n",
    "            \"quality_metrics\": {},\n",
    "            \"performance_metrics\": {},\n",
    "            \"issues\": [],\n",
    "            \"success\": False\n",
    "        }\n",
    "        \n",
    "    def wait_for_job(self, job_id: str, every: int = 5) -> Dict[str, Any]:\n",
    "        \"\"\"Wait for job completion with detailed progress tracking.\"\"\"\n",
    "        print(f\"‚è≥ Monitoring job {job_id}...\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        while True:\n",
    "            try:\n",
    "                rec = requests.get(f\"{self.endpoint}/jobs/{job_id}\", timeout=10).json()\n",
    "                status = rec[\"status\"]\n",
    "                \n",
    "                if status not in {\"finished\", \"error\"}:\n",
    "                    elapsed = time.time() - start_time\n",
    "                    print(f\"\\r‚è±Ô∏è  {rec.get('status_with_elapsed', status)} (Elapsed: {elapsed:.0f}s)\", end=\"\")\n",
    "                else:\n",
    "                    elapsed = time.time() - start_time\n",
    "                    print(f\"\\n‚úÖ {status.upper()} in {elapsed:.1f}s\")\n",
    "                    return rec\n",
    "                    \n",
    "                time.sleep(every)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"\\n‚ùå Error checking job status: {e}\")\n",
    "                return {\"status\": \"error\", \"error\": str(e)}\n",
    "\n",
    "    def submit_job(self, payload: Dict[str, Any], test_name: str) -> Dict[str, Any]:\n",
    "        \"\"\"Submit job and wait for completion with progress tracking.\"\"\"\n",
    "        print(f\"\\nüöÄ SUBMITTING: {test_name}\")\n",
    "        print(f\"üìù Target: {payload.get('max_posts', 'N/A')} posts from @{payload.get('username', 'N/A')}\")\n",
    "        \n",
    "        try:\n",
    "            r = requests.post(f\"{self.endpoint}/jobs/twitter\", json=payload, timeout=30)\n",
    "            r.raise_for_status()\n",
    "            jid = r.json()[\"job_id\"]\n",
    "            print(f\"üÜî Job ID: {jid}\")\n",
    "            \n",
    "            result = self.wait_for_job(jid)\n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Job submission failed: {e}\")\n",
    "            return {\"status\": \"error\", \"error\": str(e)}\n",
    "\n",
    "    def analyze_extraction_result(self, result: Dict[str, Any], test_name: str) -> Dict[str, Any]:\n",
    "        \"\"\"Analyze extraction results with detailed metrics.\"\"\"\n",
    "        print(f\"\\nüìä ANALYZING: {test_name}\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        analysis = {\n",
    "            \"test_name\": test_name,\n",
    "            \"status\": result.get(\"status\", \"unknown\"),\n",
    "            \"success\": False,\n",
    "            \"extracted\": 0,\n",
    "            \"target\": 0,\n",
    "            \"rate\": 0,\n",
    "            \"duration\": 0,\n",
    "            \"speed\": 0,\n",
    "            \"quality_score\": 0,\n",
    "            \"issues\": []\n",
    "        }\n",
    "        \n",
    "        if result[\"status\"] == \"error\":\n",
    "            error_msg = result.get('error', 'Unknown error')\n",
    "            print(f\"‚ùå FAILED: {error_msg}\")\n",
    "            analysis[\"issues\"].append(f\"Job failed: {error_msg}\")\n",
    "            return analysis\n",
    "        \n",
    "        if \"result\" not in result:\n",
    "            print(f\"‚ùå No result data found\")\n",
    "            analysis[\"issues\"].append(\"No result data in response\")\n",
    "            return analysis\n",
    "        \n",
    "        res = result[\"result\"]\n",
    "        data = res.get(\"data\", [])\n",
    "        \n",
    "        if not data:\n",
    "            print(f\"‚ö†Ô∏è NO DATA EXTRACTED - 0 items returned\")\n",
    "            analysis[\"issues\"].append(\"No data extracted\")\n",
    "            return analysis\n",
    "        \n",
    "        # Extract posts count\n",
    "        first_item = data[0] if data else {}\n",
    "        posts_extracted = 0\n",
    "        \n",
    "        if isinstance(first_item, dict) and 'posts' in first_item:\n",
    "            posts_extracted = len(first_item['posts'])\n",
    "        elif isinstance(data, list):\n",
    "            posts_extracted = len(data)\n",
    "        \n",
    "        analysis.update({\n",
    "            \"success\": posts_extracted > 0,\n",
    "            \"extracted\": posts_extracted\n",
    "        })\n",
    "        \n",
    "        print(f\"‚úÖ EXTRACTED: {posts_extracted:,} posts\")\n",
    "        \n",
    "        return analysis\n",
    "\n",
    "    def run_progressive_scale_test(self) -> bool:\n",
    "        \"\"\"\n",
    "        Phase 1: Progressive Scale Testing (10 -> 100 -> 1000 posts)\n",
    "        Validates system capacity before attempting 5K extraction.\n",
    "        \"\"\"\n",
    "        print(f\"\\nüî¨ PHASE 1: PROGRESSIVE SCALE VALIDATION\")\n",
    "        print(\"=\"*70)\n",
    "        print(f\"üéØ Purpose: Validate system capacity before 5K extraction\")\n",
    "        print(f\"üìä Strategy: Progressive testing from 10 to 1000 posts\")\n",
    "        \n",
    "        progressive_tests = [\n",
    "            {\"name\": \"Micro Scale\", \"posts\": 10, \"timeout\": 120},\n",
    "            {\"name\": \"Small Scale\", \"posts\": 50, \"timeout\": 180},\n",
    "            {\"name\": \"Medium Scale\", \"posts\": 150, \"timeout\": 300},\n",
    "            {\"name\": \"Large Scale\", \"posts\": 500, \"timeout\": 600},\n",
    "            {\"name\": \"Ultra Scale\", \"posts\": 1000, \"timeout\": 900}\n",
    "        ]\n",
    "\n",
    "        baseline_proven = False\n",
    "        max_proven_capacity = 0\n",
    "\n",
    "        for i, test in enumerate(progressive_tests, 1):\n",
    "            print(f\"\\nüìä Test {i}/{len(progressive_tests)}: {test['name']} ({test['posts']} posts)\")\n",
    "            print(f\"‚è±Ô∏è Timeout: {test['timeout']} seconds\")\n",
    "\n",
    "            phase_start = time.time()\n",
    "\n",
    "            payload = {\n",
    "                \"username\": self.target_username,\n",
    "                \"scrape_posts\": True,\n",
    "                \"max_posts\": test['posts'],\n",
    "                \"scrape_level\": 4\n",
    "            }\n",
    "\n",
    "            result = self.submit_job(payload, f\"Progressive Test {i}\")\n",
    "            analysis = self.analyze_extraction_result(result, test['name'])\n",
    "            \n",
    "            phase_duration = time.time() - phase_start\n",
    "            analysis[\"duration\"] = phase_duration\n",
    "            analysis[\"target\"] = test['posts']\n",
    "            \n",
    "            if analysis[\"success\"]:\n",
    "                extraction_rate = (analysis[\"extracted\"] / test['posts']) * 100\n",
    "                speed = analysis[\"extracted\"] / phase_duration if phase_duration > 0 else 0\n",
    "                \n",
    "                analysis[\"rate\"] = extraction_rate\n",
    "                analysis[\"speed\"] = speed\n",
    "\n",
    "                print(f\"‚úÖ SUCCESS: {analysis['extracted']}/{test['posts']} posts ({extraction_rate:.1f}%)\")\n",
    "                print(f\"üìà Speed: {speed:.2f} posts/second\")\n",
    "                print(f\"‚è±Ô∏è Duration: {phase_duration:.1f}s\")\n",
    "\n",
    "                if extraction_rate >= 70:  # 70% success threshold\n",
    "                    baseline_proven = True\n",
    "                    max_proven_capacity = max(max_proven_capacity, test['posts'])\n",
    "                    print(f\"üéØ BASELINE PROVEN: {test['posts']} posts capacity confirmed\")\n",
    "                else:\n",
    "                    print(f\"‚ö†Ô∏è LOW EXTRACTION: Only {extraction_rate:.1f}% success rate\")\n",
    "\n",
    "            else:\n",
    "                print(f\"‚ùå FAILED: {' | '.join(analysis['issues'])}\")\n",
    "                print(f\"‚è±Ô∏è Duration: {phase_duration:.1f}s\")\n",
    "\n",
    "                # If we fail at lower scales, stop progressive testing\n",
    "                if test['posts'] <= 150:\n",
    "                    print(f\"üö® CRITICAL: Failed at {test['posts']} posts - infrastructure issues detected\")\n",
    "                    self.results[\"issues\"].append(f\"Failed at {test['posts']} posts - cannot proceed to 5K\")\n",
    "                    return False\n",
    "\n",
    "            self.results[\"batches\"].append(analysis)\n",
    "\n",
    "            # Brief cooldown between tests\n",
    "            if i < len(progressive_tests):\n",
    "                print(f\"‚è≥ Cooling down 10 seconds...\")\n",
    "                time.sleep(10)\n",
    "\n",
    "        if baseline_proven and max_proven_capacity >= 150:\n",
    "            print(f\"\\nüéâ PHASE 1 SUCCESS: Proven capacity up to {max_proven_capacity:,} posts\")\n",
    "            self.results[\"performance_metrics\"][\"proven_capacity\"] = max_proven_capacity\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"\\nüö® PHASE 1 FAILED: Maximum proven capacity only {max_proven_capacity} posts\")\n",
    "            self.results[\"issues\"].append(f\"Insufficient baseline capacity: {max_proven_capacity} posts\")\n",
    "            return False\n",
    "\n",
    "    def run_intelligent_5k_batching(self, proven_capacity: int) -> int:\n",
    "        \"\"\"\n",
    "        Phase 2: Intelligent Batching for 5K+ Extraction\n",
    "        Uses proven safe limits to extract 5000+ posts via intelligent batching.\n",
    "        \"\"\"\n",
    "        print(f\"\\nüöÄ PHASE 2: INTELLIGENT 5K+ BATCHING\")\n",
    "        print(\"=\"*70)\n",
    "        print(f\"üìä Proven Capacity: {proven_capacity} posts per job\")\n",
    "\n",
    "        # Calculate optimal batch strategy\n",
    "        safe_batch_size = min(proven_capacity, 150)  # Use proven safe limit\n",
    "        target_5k = 5000\n",
    "        num_batches = (target_5k + safe_batch_size - 1) // safe_batch_size  # Ceiling division\n",
    "\n",
    "        print(f\"üéØ Strategy: {num_batches} batches of {safe_batch_size} posts each\")\n",
    "        print(f\"üìà Expected total: {num_batches * safe_batch_size} posts\")\n",
    "        print(f\"‚è±Ô∏è Estimated time: {num_batches * 180} seconds ({(num_batches * 180)/60:.1f} minutes)\")\n",
    "        print(f\"üö® PATIENCE REQUIRED: This will take time - progress shown below\")\n",
    "\n",
    "        total_5k_extracted = 0\n",
    "        batch_failures = 0\n",
    "        test_batch_limit = min(num_batches, 15)  # Test with first 15 batches\n",
    "\n",
    "        print(f\"\\nüì¶ STARTING BATCH EXTRACTION (Testing {test_batch_limit} batches)\")\n",
    "        print(\"=\"*50)\n",
    "\n",
    "        for batch_num in range(1, test_batch_limit + 1):\n",
    "            print(f\"\\nüì¶ BATCH {batch_num}/{test_batch_limit}\")\n",
    "            print(f\"üéØ Target: {safe_batch_size} posts\")\n",
    "\n",
    "            batch_start = time.time()\n",
    "\n",
    "            payload = {\n",
    "                \"username\": self.target_username,\n",
    "                \"scrape_posts\": True,\n",
    "                \"max_posts\": safe_batch_size,\n",
    "                \"scrape_level\": 4,\n",
    "                \"batch_info\": {\n",
    "                    \"batch_number\": batch_num,\n",
    "                    \"total_batches\": test_batch_limit,\n",
    "                    \"posts_offset\": (batch_num - 1) * safe_batch_size\n",
    "                }\n",
    "            }\n",
    "\n",
    "            result = self.submit_job(payload, f\"5K Batch {batch_num}\")\n",
    "            analysis = self.analyze_extraction_result(result, f\"Batch {batch_num}\")\n",
    "            \n",
    "            batch_duration = time.time() - batch_start\n",
    "            elapsed_total = time.time() - self.results[\"start_time\"].timestamp()\n",
    "            \n",
    "            analysis[\"duration\"] = batch_duration\n",
    "            analysis[\"target\"] = safe_batch_size\n",
    "\n",
    "            if analysis[\"success\"]:\n",
    "                batch_extracted = analysis[\"extracted\"]\n",
    "                total_5k_extracted += batch_extracted\n",
    "                extraction_rate = (batch_extracted / safe_batch_size) * 100\n",
    "                speed = batch_extracted / batch_duration if batch_duration > 0 else 0\n",
    "                \n",
    "                analysis[\"rate\"] = extraction_rate\n",
    "                analysis[\"speed\"] = speed\n",
    "\n",
    "                print(f\"‚úÖ BATCH SUCCESS: {batch_extracted}/{safe_batch_size} posts ({extraction_rate:.1f}%)\")\n",
    "                print(f\"üìà Batch Speed: {speed:.2f} posts/second\")\n",
    "                print(f\"üìä CUMULATIVE TOTAL: {total_5k_extracted:,} posts\")\n",
    "                print(f\"‚è±Ô∏è Elapsed: {elapsed_total:.0f}s | This Batch: {batch_duration:.1f}s\")\n",
    "\n",
    "                # Progress tracking\n",
    "                progress = (batch_num / test_batch_limit) * 100\n",
    "                if batch_num > 1:\n",
    "                    avg_batch_time = elapsed_total / batch_num\n",
    "                    eta_remaining = avg_batch_time * (test_batch_limit - batch_num)\n",
    "                    print(f\"üìà Progress: {progress:.1f}% | ETA: {eta_remaining:.0f}s remaining\")\n",
    "                \n",
    "                # Milestone tracking\n",
    "                if total_5k_extracted >= 1000:\n",
    "                    print(f\"üéØ MILESTONE: 1K posts achieved!\")\n",
    "                if total_5k_extracted >= 2500:\n",
    "                    print(f\"üéØ MILESTONE: 2.5K posts achieved!\")\n",
    "\n",
    "            else:\n",
    "                batch_failures += 1\n",
    "                print(f\"‚ùå BATCH FAILED: {' | '.join(analysis['issues'])}\")\n",
    "                print(f\"‚è±Ô∏è Duration: {batch_duration:.1f}s\")\n",
    "                print(f\"üìä CUMULATIVE TOTAL: {total_5k_extracted:,} posts (no change)\")\n",
    "\n",
    "                # Stop if too many consecutive failures\n",
    "                if batch_failures >= 3:\n",
    "                    print(f\"üö® STOPPING: {batch_failures} consecutive batch failures\")\n",
    "                    self.results[\"issues\"].append(f\"Stopped after {batch_failures} batch failures\")\n",
    "                    break\n",
    "\n",
    "            self.results[\"batches\"].append(analysis)\n",
    "\n",
    "            # Brief pause between batches to avoid overwhelming the system\n",
    "            if batch_num < test_batch_limit:\n",
    "                print(f\"‚è≥ Inter-batch cooldown 15 seconds...\")\n",
    "                time.sleep(15)\n",
    "\n",
    "        return total_5k_extracted\n",
    "\n",
    "    def run_multi_data_type_test(self, posts_extracted: int) -> Dict[str, int]:\n",
    "        \"\"\"\n",
    "        Phase 3: Multi-Data Type Testing (followers, following, media, likes, mentions)\n",
    "        Tests extraction of different data types if posts extraction was successful.\n",
    "        \"\"\"\n",
    "        if posts_extracted < 1000:\n",
    "            print(f\"\\nüö® PHASE 3 SKIPPED: Insufficient posts extracted ({posts_extracted})\")\n",
    "            self.results[\"issues\"].append(\"Multi-data type testing skipped due to low post extraction\")\n",
    "            return {}\n",
    "\n",
    "        print(f\"\\nüîç PHASE 3: MULTI-DATA TYPE VALIDATION\")\n",
    "        print(\"=\"*70)\n",
    "        print(f\"üéØ Purpose: Validate extraction of all data types requested\")\n",
    "        print(f\"üìä Scope: followers, following, media, likes, mentions\")\n",
    "\n",
    "        data_types_test = {\n",
    "            \"followers\": {\"max_followers\": 1000},\n",
    "            \"following\": {\"max_following\": 1000}, \n",
    "            \"media\": {\"max_media\": 500},\n",
    "            \"likes\": {\"max_likes\": 500},\n",
    "            \"mentions\": {\"max_mentions\": 500}\n",
    "        }\n",
    "\n",
    "        extraction_results = {}\n",
    "\n",
    "        for data_type, params in data_types_test.items():\n",
    "            print(f\"\\nüìä Testing {data_type.upper()} extraction...\")\n",
    "\n",
    "            payload = {\n",
    "                \"username\": self.target_username,\n",
    "                f\"scrape_{data_type}\": True,\n",
    "                **params,\n",
    "                \"scrape_level\": 4\n",
    "            }\n",
    "\n",
    "            type_start = time.time()\n",
    "            result = self.submit_job(payload, f\"5K {data_type.title()}\")\n",
    "            analysis = self.analyze_extraction_result(result, f\"{data_type.title()} Test\")\n",
    "            type_duration = time.time() - type_start\n",
    "            \n",
    "            analysis[\"duration\"] = type_duration\n",
    "            analysis[\"target\"] = list(params.values())[0]\n",
    "\n",
    "            if analysis[\"success\"]:\n",
    "                extracted_count = analysis[\"extracted\"]\n",
    "                target_count = list(params.values())[0]\n",
    "                rate = (extracted_count / target_count) * 100 if target_count > 0 else 0\n",
    "                \n",
    "                analysis[\"rate\"] = rate\n",
    "\n",
    "                print(f\"‚úÖ {data_type.upper()}: {extracted_count:,}/{target_count:,} ({rate:.1f}%)\")\n",
    "                extraction_results[data_type] = extracted_count\n",
    "\n",
    "            else:\n",
    "                print(f\"‚ùå {data_type.upper()} FAILED: {' | '.join(analysis['issues'])}\")\n",
    "                extraction_results[data_type] = 0\n",
    "\n",
    "            self.results[\"batches\"].append(analysis)\n",
    "            time.sleep(10)  # Brief pause between data types\n",
    "\n",
    "        return extraction_results\n",
    "\n",
    "    def generate_comprehensive_report(self, posts_extracted: int, multi_data_results: Dict[str, int]):\n",
    "        \"\"\"Generate comprehensive ultra-scale testing report.\"\"\"\n",
    "        print(f\"\\n\" + \"=\"*80)\n",
    "        print(f\"üìä ULTRA-SCALE 5K+ TESTING COMPREHENSIVE REPORT\")\n",
    "        print(\"=\"*80)\n",
    "\n",
    "        # Executive Summary\n",
    "        total_followers = multi_data_results.get(\"followers\", 0)\n",
    "        total_following = multi_data_results.get(\"following\", 0)\n",
    "        total_media = multi_data_results.get(\"media\", 0)\n",
    "        total_likes = multi_data_results.get(\"likes\", 0)\n",
    "        total_mentions = multi_data_results.get(\"mentions\", 0)\n",
    "\n",
    "        overall_items = posts_extracted + total_followers + total_following + total_media + total_likes + total_mentions\n",
    "        total_duration = time.time() - self.results[\"start_time\"].timestamp()\n",
    "\n",
    "        print(f\"üéØ EXECUTIVE SUMMARY:\")\n",
    "        print(f\"   Target User: @{self.target_username}\")\n",
    "        print(f\"   Test Duration: {total_duration:.1f} seconds ({total_duration/60:.1f} minutes)\")\n",
    "        print(f\"   Total Data Extracted: {overall_items:,} items\")\n",
    "\n",
    "        # Data Breakdown\n",
    "        print(f\"\\nüìä ULTRA-SCALE DATA EXTRACTION RESULTS:\")\n",
    "        print(f\"   üìù Posts: {posts_extracted:,}\")\n",
    "        print(f\"   üë• Followers: {total_followers:,}\")\n",
    "        print(f\"   ‚û°Ô∏è Following: {total_following:,}\")\n",
    "        print(f\"   üñºÔ∏è Media: {total_media:,}\")\n",
    "        print(f\"   ‚ù§Ô∏è Likes: {total_likes:,}\")\n",
    "        print(f\"   @Ô∏è‚É£ Mentions: {total_mentions:,}\")\n",
    "\n",
    "        # Success Assessment\n",
    "        success_criteria = {\n",
    "            \"5K+ Posts\": posts_extracted >= 5000,\n",
    "            \"2.5K+ Posts\": posts_extracted >= 2500,\n",
    "            \"1K+ Posts\": posts_extracted >= 1000,\n",
    "            \"Multi-Data Types\": sum(multi_data_results.values()) > 0,\n",
    "            \"All Data Types\": len([v for v in multi_data_results.values() if v > 0]) >= 3\n",
    "        }\n",
    "\n",
    "        print(f\"\\nüéØ SUCCESS CRITERIA ASSESSMENT:\")\n",
    "        for criteria, met in success_criteria.items():\n",
    "            icon = \"‚úÖ\" if met else \"‚ùå\"\n",
    "            print(f\"   {icon} {criteria}\")\n",
    "\n",
    "        # Overall Success Determination\n",
    "        if success_criteria[\"5K+ Posts\"]:\n",
    "            overall_success = \"EXCELLENT - 5K+ ACHIEVED\"\n",
    "            capability_score = \"PRODUCTION READY\"\n",
    "        elif success_criteria[\"2.5K+ Posts\"]:\n",
    "            overall_success = \"GOOD - 2.5K+ ACHIEVED\"\n",
    "            capability_score = \"PRODUCTION CAPABLE\"\n",
    "        elif success_criteria[\"1K+ Posts\"]:\n",
    "            overall_success = \"FAIR - 1K+ ACHIEVED\"\n",
    "            capability_score = \"LIMITED PRODUCTION\"\n",
    "        else:\n",
    "            overall_success = \"POOR - <1K POSTS\"\n",
    "            capability_score = \"NOT PRODUCTION READY\"\n",
    "\n",
    "        print(f\"\\nüèÜ OVERALL ASSESSMENT:\")\n",
    "        print(f\"   Result: {overall_success}\")\n",
    "        print(f\"   Capability: {capability_score}\")\n",
    "        \n",
    "        self.results[\"success\"] = posts_extracted >= 2500  # 50% of 5K target\n",
    "        self.results[\"total_extracted\"][\"posts\"] = posts_extracted\n",
    "        self.results[\"total_extracted\"].update(multi_data_results)\n",
    "\n",
    "        # Production Recommendations\n",
    "        print(f\"\\nüí° PRODUCTION RECOMMENDATIONS:\")\n",
    "        if posts_extracted >= 2500:\n",
    "            print(f\"   ‚úÖ System ready for ultra-scale production use\")\n",
    "            print(f\"   ‚úÖ Use intelligent batching for requests > 150 posts\")\n",
    "            print(f\"   ‚úÖ Expected 5K extraction time: ~{(5000/150) * 180:.0f} seconds\")\n",
    "            print(f\"   ‚úÖ Implement progress tracking for user visibility\")\n",
    "        elif posts_extracted >= 1000:\n",
    "            print(f\"   ‚ö†Ô∏è System capable of large-scale production with limits\")\n",
    "            print(f\"   ‚ö†Ô∏è Recommended maximum: 1000-2000 posts per request\")\n",
    "            print(f\"   ‚ö†Ô∏è Monitor system performance and adjust limits\")\n",
    "        else:\n",
    "            print(f\"   ‚ùå System requires infrastructure improvements\")\n",
    "            print(f\"   ‚ùå Fix baseline extraction issues before production\")\n",
    "\n",
    "        print(f\"\\nüéØ 5K+ VALIDATION RESULT: {'‚úÖ PASSED' if self.results['success'] else '‚ùå FAILED'}\")\n",
    "        print(f\"üìä System is {'READY' if self.results['success'] else 'NOT READY'} for ultra-scale production\")\n",
    "\n",
    "        return self.results\n",
    "\n",
    "    def run_complete_5k_test(self):\n",
    "        \"\"\"\n",
    "        üö® Run Complete Ultra-Scale 5K+ Testing Suite\n",
    "        \n",
    "        This is the main method that orchestrates all testing phases.\n",
    "        \"\"\"\n",
    "        print(\"üö® ULTRA-SCALE 5K+ DATA EXTRACTION TESTING\")\n",
    "        print(\"=\"*80)\n",
    "        print(f\"üéØ TARGET: @{self.target_username}\")\n",
    "        print(f\"üìä GOAL: Extract 5000+ posts, followers, following, media, likes, mentions\")\n",
    "        print(f\"‚è±Ô∏è APPROACH: Patient, intelligent batching with real-time progress\")\n",
    "        print(f\"üî¨ VALIDATION: Comprehensive data quality and architecture assessment\")\n",
    "        print(f\"üö® ESTIMATED TIME: 30-60 minutes for complete validation\")\n",
    "        print(\"=\"*80)\n",
    "\n",
    "        self.results[\"start_time\"] = datetime.now()\n",
    "        \n",
    "        # Check API connectivity first\n",
    "        try:\n",
    "            test_response = requests.get(f\"{self.endpoint}/healthz\", timeout=5)\n",
    "            if test_response.status_code == 200:\n",
    "                print(f\"‚úÖ API connectivity: Connected to {self.endpoint}\")\n",
    "            else:\n",
    "                print(f\"‚ö†Ô∏è API connectivity: Unexpected response {test_response.status_code}\")\n",
    "                return self.results\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå API connectivity: Failed - {e}\")\n",
    "            self.results[\"issues\"].append(f\"API connectivity failed: {e}\")\n",
    "            return self.results\n",
    "\n",
    "        # Phase 1: Progressive Scale Testing\n",
    "        baseline_success = self.run_progressive_scale_test()\n",
    "        if not baseline_success:\n",
    "            print(f\"\\nüö® STOPPING: Baseline testing failed - cannot proceed to 5K\")\n",
    "            return self.generate_comprehensive_report(0, {})\n",
    "\n",
    "        # Phase 2: Intelligent 5K+ Batching\n",
    "        proven_capacity = self.results[\"performance_metrics\"].get(\"proven_capacity\", 150)\n",
    "        posts_extracted = self.run_intelligent_5k_batching(proven_capacity)\n",
    "\n",
    "        # Phase 3: Multi-Data Type Testing  \n",
    "        multi_data_results = self.run_multi_data_type_test(posts_extracted)\n",
    "\n",
    "        # Generate Final Report\n",
    "        return self.generate_comprehensive_report(posts_extracted, multi_data_results)\n",
    "\n",
    "# Initialize and run ultra-scale 5K testing\n",
    "print(\"üöÄ INITIALIZING ULTRA-SCALE 5K+ TESTING\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# You can change the target username here\n",
    "TARGET_USERNAME = \"naval\" \n",
    " # Change this to test different accounts\n",
    "\n",
    "tester = UltraScale5KTester(target_username=TARGET_USERNAME)\n",
    "final_results = tester.run_complete_5k_test()\n",
    "\n",
    "print(f\"\\nüéâ ULTRA-SCALE 5K+ TESTING COMPLETED!\")\n",
    "print(f\"üìä Final Results: {final_results['success']}\")\n",
    "print(f\"üìÅ Full results stored in tester.results for detailed analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
