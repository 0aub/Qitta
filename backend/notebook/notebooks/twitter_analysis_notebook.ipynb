{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "setup",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üê¶ Twitter Analysis Notebook Initialized!\n",
      "üìç API Endpoint: http://browser:8004\n",
      "üìÅ Storage: /storage/scraped_data\n",
      "‚ö†Ô∏è Twitter data directory not found at /storage/scraped_data/twitter\n",
      "‚úÖ API connectivity: Connected to browser service\n"
     ]
    }
   ],
   "source": [
    "import os, time, pathlib, pprint, requests, json\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "EP = \"http://browser:8004\"  # Fixed port to 8004\n",
    "SCRAPED = pathlib.Path(\"/storage/scraped_data\")\n",
    "\n",
    "def wait_for(job_id, every=3):\n",
    "    print(f\"‚è≥ Waiting for job {job_id}...\")\n",
    "    while True:\n",
    "        rec = requests.get(f\"{EP}/jobs/{job_id}\").json()\n",
    "        status = rec[\"status\"]\n",
    "        if status not in {\"finished\", \"error\"}:\n",
    "            print(f\"\\r‚è±Ô∏è  {rec['status_with_elapsed']}\", end=\"\")\n",
    "        else:\n",
    "            print(f\"\\n‚úÖ {status.upper()}\")\n",
    "            return rec\n",
    "        time.sleep(every)\n",
    "\n",
    "def submit(task, payload):\n",
    "    print(f\"üöÄ Submitting {task} task...\")\n",
    "    print(f\"üìù Payload: {json.dumps(payload, indent=2)}\")\n",
    "    r = requests.post(f\"{EP}/jobs/{task}\", json=payload)\n",
    "    r.raise_for_status()\n",
    "    jid = r.json()[\"job_id\"]\n",
    "    print(f\"üÜî Job ID: {jid}\")\n",
    "    return wait_for(jid)\n",
    "\n",
    "def analyze_twitter_results(result, test_name):\n",
    "    \"\"\"Comprehensive analysis function for Twitter scraping results.\"\"\"\n",
    "    print(f\"\\n\" + \"=\"*70)\n",
    "    print(f\"üê¶ ANALYSIS: {test_name}\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    if result[\"status\"] == \"error\":\n",
    "        print(f\"‚ùå FAILED: {result.get('error', 'Unknown error')}\")\n",
    "        return\n",
    "    \n",
    "    if \"result\" not in result:\n",
    "        print(f\"‚ùå No result data found\")\n",
    "        pprint.pp(result)\n",
    "        return\n",
    "        \n",
    "    res = result[\"result\"]\n",
    "    metadata = res.get(\"search_metadata\", {})\n",
    "    data = res.get(\"data\", [])\n",
    "    \n",
    "    # === BASIC INFO ===\n",
    "    print(f\"‚úÖ STATUS: Task completed successfully\")\n",
    "    print(f\"üéØ TARGET: @{metadata.get('target_username', 'N/A')}\")\n",
    "    print(f\"üìä TOTAL EXTRACTED: {len(data)} items (found: {metadata.get('total_found', 'N/A')})\")\n",
    "    print(f\"üîß METHOD: {metadata.get('extraction_method', 'N/A')}\")\n",
    "    print(f\"üìà SCRAPE LEVEL: {metadata.get('scrape_level', 'N/A')}\")\n",
    "    print(f\"üìà SUCCESS RATE: {metadata.get('success_rate', 0):.1%}\")\n",
    "    \n",
    "    if not data:\n",
    "        print(f\"‚ùå NO DATA EXTRACTED\")\n",
    "        return\n",
    "    \n",
    "    # Determine data structure based on first item\n",
    "    first_item = data[0] if data else {}\n",
    "    \n",
    "    # Check if it's comprehensive user data (has profile + posts)\n",
    "    if isinstance(first_item, dict) and 'profile' in first_item:\n",
    "        profile_data = first_item\n",
    "        print(f\"\\nüìã PROFILE DATA:\")\n",
    "        profile = profile_data.get('profile', {})\n",
    "        print(f\"   üè∑Ô∏è Name: {profile.get('display_name', 'N/A')}\")\n",
    "        print(f\"   üìù Bio: {profile.get('bio', 'N/A')[:100]}{'...' if len(profile.get('bio', '')) > 100 else ''}\")\n",
    "        print(f\"   üìä Followers: {profile.get('followers_count', 'N/A')}\")\n",
    "        print(f\"   üë• Following: {profile.get('following_count', 'N/A')}\")\n",
    "        print(f\"   üìù Posts Count: {profile.get('posts_count', 'N/A')}\")\n",
    "        \n",
    "        # Analyze different data types\n",
    "        data_types = ['posts', 'likes', 'mentions', 'media', 'followers', 'following']\n",
    "        total_items = 0\n",
    "        \n",
    "        print(f\"\\nüìä EXTRACTED DATA BREAKDOWN:\")\n",
    "        for data_type in data_types:\n",
    "            items = profile_data.get(data_type, [])\n",
    "            if items:\n",
    "                total_items += len(items)\n",
    "                print(f\"   {get_emoji(data_type)} {data_type.title()}: {len(items)} items\")\n",
    "                \n",
    "                # Specific analysis for posts/tweets\n",
    "                if data_type == 'posts' and len(items) > 0:\n",
    "                    analyze_tweets(items)\n",
    "                    \n",
    "                # Sample first item\n",
    "                if len(items) > 0:\n",
    "                    sample = items[0]\n",
    "                    if isinstance(sample, dict):\n",
    "                        sample_text = sample.get('text', sample.get('content', str(sample)))[:80]\n",
    "                        print(f\"      Sample: {sample_text}{'...' if len(str(sample)) > 80 else ''}\")\n",
    "        \n",
    "        print(f\"\\nüìà TOTAL ITEMS: {total_items} across all data types\")\n",
    "        \n",
    "    else:\n",
    "        # Direct tweet/post list\n",
    "        print(f\"\\nüìù POSTS/TWEETS ANALYSIS:\")\n",
    "        analyze_tweets(data)\n",
    "    \n",
    "    # === EXECUTION TIME ===\n",
    "    completed_at = metadata.get('search_completed_at')\n",
    "    if completed_at:\n",
    "        print(f\"\\n‚è±Ô∏è  COMPLETED: {completed_at}\")\n",
    "    \n",
    "    # === SAMPLE DATA ===\n",
    "    if data:\n",
    "        sample = data[0]\n",
    "        print(f\"\\nüìã SAMPLE DATA STRUCTURE:\")\n",
    "        print(f\"   üìÑ Type: {type(sample).__name__}\")\n",
    "        if isinstance(sample, dict):\n",
    "            keys = list(sample.keys())[:10]\n",
    "            print(f\"   üîë Keys: {keys}{'...' if len(sample.keys()) > 10 else ''}\")\n",
    "\n",
    "def analyze_tweets(tweets):\n",
    "    \"\"\"Analyze a list of tweets/posts.\"\"\"\n",
    "    if not tweets:\n",
    "        return\n",
    "        \n",
    "    print(f\"   üìù Total Tweets: {len(tweets)}\")\n",
    "    \n",
    "    # Count tweets with different data types\n",
    "    tweets_with_metrics = [t for t in tweets if isinstance(t, dict) and any(k in t for k in ['likes', 'retweets', 'replies'])]\n",
    "    tweets_with_dates = [t for t in tweets if isinstance(t, dict) and t.get('date')]\n",
    "    tweets_with_media = [t for t in tweets if isinstance(t, dict) and t.get('media')]\n",
    "    \n",
    "    print(f\"   üìä With engagement metrics: {len(tweets_with_metrics)}/{len(tweets)}\")\n",
    "    print(f\"   üìÖ With dates: {len(tweets_with_dates)}/{len(tweets)}\")\n",
    "    print(f\"   üñºÔ∏è With media: {len(tweets_with_media)}/{len(tweets)}\")\n",
    "    \n",
    "    # Analyze engagement if available\n",
    "    if tweets_with_metrics:\n",
    "        total_likes = sum(t.get('likes', 0) for t in tweets_with_metrics if isinstance(t.get('likes'), int))\n",
    "        total_retweets = sum(t.get('retweets', 0) for t in tweets_with_metrics if isinstance(t.get('retweets'), int))\n",
    "        avg_likes = total_likes / len(tweets_with_metrics) if tweets_with_metrics else 0\n",
    "        avg_retweets = total_retweets / len(tweets_with_metrics) if tweets_with_metrics else 0\n",
    "        \n",
    "        print(f\"   ‚ù§Ô∏è Avg Likes: {avg_likes:.1f} (total: {total_likes})\")\n",
    "        print(f\"   üîÑ Avg Retweets: {avg_retweets:.1f} (total: {total_retweets})\")\n",
    "    \n",
    "    # Show sample tweets\n",
    "    sample_count = min(3, len(tweets))\n",
    "    for i, tweet in enumerate(tweets[:sample_count]):\n",
    "        if isinstance(tweet, dict):\n",
    "            text = tweet.get('text', tweet.get('content', str(tweet)))\n",
    "            date = tweet.get('date', 'No date')\n",
    "            likes = tweet.get('likes', 'N/A')\n",
    "            print(f\"   üê¶ Tweet {i+1}: {text[:100]}{'...' if len(text) > 100 else ''}\")\n",
    "            print(f\"      üìÖ {date} | ‚ù§Ô∏è {likes}\")\n",
    "        else:\n",
    "            print(f\"   üê¶ Tweet {i+1}: {str(tweet)[:100]}{'...' if len(str(tweet)) > 100 else ''}\")\n",
    "\n",
    "def get_emoji(data_type):\n",
    "    \"\"\"Get emoji for data type.\"\"\"\n",
    "    emojis = {\n",
    "        'posts': 'üìù',\n",
    "        'likes': '‚ù§Ô∏è', \n",
    "        'mentions': '@Ô∏è‚É£',\n",
    "        'media': 'üñºÔ∏è',\n",
    "        'followers': 'üë•',\n",
    "        'following': '‚û°Ô∏è'\n",
    "    }\n",
    "    return emojis.get(data_type, 'üìä')\n",
    "\n",
    "print(\"üê¶ Twitter Analysis Notebook Initialized!\")\n",
    "print(f\"üìç API Endpoint: {EP}\")\n",
    "print(f\"üìÅ Storage: {SCRAPED}\")\n",
    "\n",
    "# Check storage accessibility\n",
    "twitter_dir = SCRAPED / \"twitter\"\n",
    "if twitter_dir.exists():\n",
    "    recent_jobs = sorted([d.name for d in twitter_dir.iterdir() if d.is_dir()], reverse=True)[:3]\n",
    "    print(f\"‚úÖ Storage accessible - Recent jobs: {recent_jobs}\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è Twitter data directory not found at {twitter_dir}\")\n",
    "\n",
    "# Test API connectivity\n",
    "try:\n",
    "    test_response = requests.get(f\"{EP}/healthz\", timeout=5)\n",
    "    if test_response.status_code == 200:\n",
    "        print(f\"‚úÖ API connectivity: Connected to browser service\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è API connectivity: Unexpected response {test_response.status_code}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå API connectivity: Failed to connect - {e}\")\n",
    "    print(f\"üîç Check if browser service is running on port 8004\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "test-accounts",
   "metadata": {},
   "source": [
    "# üß™ Twitter Scraper Testing & Validation\n",
    "\n",
    "## Test popular accounts with different extraction modes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "basic-tests",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ Available test accounts:\n",
      "   @naval: High-quality tweets, philosophy\n",
      "   @elonmusk: High activity, mixed content\n",
      "   @paulg: Startup advice, essays\n",
      "   @sama: AI/tech commentary\n",
      "   @vitalikbuterin: Crypto/blockchain content\n",
      "\n",
      "üöÄ ENHANCED EXTRACTION TEST\n",
      "==================================================\n",
      "\n",
      "üß™ Testing: Naval - Enhanced with Engagement\n",
      "üöÄ Submitting twitter task...\n",
      "üìù Payload: {\n",
      "  \"username\": \"naval\",\n",
      "  \"scrape_posts\": true,\n",
      "  \"max_posts\": 10,\n",
      "  \"scrape_likes\": true,\n",
      "  \"max_likes\": 5,\n",
      "  \"scrape_mentions\": true,\n",
      "  \"max_mentions\": 3,\n",
      "  \"scrape_media\": true,\n",
      "  \"max_media\": 3,\n",
      "  \"scrape_level\": 4\n",
      "}\n",
      "üÜî Job ID: 3b71cf14a94f45abaf62acd5b2208d15\n",
      "‚è≥ Waiting for job 3b71cf14a94f45abaf62acd5b2208d15...\n",
      "‚è±Ô∏è  running 12s\n",
      "‚úÖ ERROR\n",
      "\n",
      "======================================================================\n",
      "üê¶ ANALYSIS: Naval - Enhanced with Engagement\n",
      "======================================================================\n",
      "‚ùå FAILED: object of type 'bool' has no len()\n"
     ]
    }
   ],
   "source": [
    "# Test accounts with different characteristics\n",
    "test_accounts = {\n",
    "    \"naval\": \"High-quality tweets, philosophy\",\n",
    "    \"elonmusk\": \"High activity, mixed content\", \n",
    "    \"paulg\": \"Startup advice, essays\",\n",
    "    \"sama\": \"AI/tech commentary\",\n",
    "    \"vitalikbuterin\": \"Crypto/blockchain content\"\n",
    "}\n",
    "\n",
    "print(\"üéØ Available test accounts:\")\n",
    "for account, desc in test_accounts.items():\n",
    "    print(f\"   @{account}: {desc}\")\n",
    "\n",
    "def test_twitter_extraction(username, max_posts=20, test_name=None):\n",
    "    \"\"\"Test comprehensive Twitter extraction with engagement metrics.\"\"\"\n",
    "    if not test_name:\n",
    "        test_name = f\"@{username} Enhanced Extraction\"\n",
    "        \n",
    "    payload = {\n",
    "        \"username\": username,\n",
    "        \"scrape_posts\": True,\n",
    "        \"max_posts\": max_posts,\n",
    "        \"scrape_likes\": True,\n",
    "        \"max_likes\": 5,\n",
    "        \"scrape_mentions\": True,\n",
    "        \"max_mentions\": 3,\n",
    "        \"scrape_media\": True,\n",
    "        \"max_media\": 3,\n",
    "        \"scrape_level\": 4  # Full extraction\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nüß™ Testing: {test_name}\")\n",
    "    result = submit(\"twitter\", payload)\n",
    "    analyze_twitter_results(result, test_name)\n",
    "    return result\n",
    "\n",
    "# Test enhanced extraction with engagement metrics\n",
    "print(\"\\nüöÄ ENHANCED EXTRACTION TEST\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "enhanced_result = test_twitter_extraction(\"naval\", max_posts=10, test_name=\"Naval - Enhanced with Engagement\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comprehensive-test",
   "metadata": {},
   "source": [
    "## üéØ Comprehensive Data Extraction Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "comprehensive",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üî¨ COMPREHENSIVE EXTRACTION TEST\n",
      "==================================================\n",
      "üìä Testing multiple data types extraction\n",
      "\n",
      "üéØ Testing: @paulg - Comprehensive Extraction\n",
      "üöÄ Submitting twitter task...\n",
      "üìù Payload: {\n",
      "  \"username\": \"paulg\",\n",
      "  \"scrape_posts\": true,\n",
      "  \"max_posts\": 15,\n",
      "  \"scrape_likes\": true,\n",
      "  \"max_likes\": 10,\n",
      "  \"scrape_mentions\": true,\n",
      "  \"max_mentions\": 5,\n",
      "  \"scrape_media\": true,\n",
      "  \"max_media\": 5,\n",
      "  \"scrape_followers\": false,\n",
      "  \"scrape_following\": false,\n",
      "  \"level\": 4\n",
      "}\n",
      "üÜî Job ID: fd9783df33ed423db447a4fd38bdb17b\n",
      "‚è≥ Waiting for job fd9783df33ed423db447a4fd38bdb17b...\n",
      "‚è±Ô∏è  running 9s\n",
      "‚úÖ ERROR\n",
      "\n",
      "======================================================================\n",
      "üê¶ ANALYSIS: @paulg - Comprehensive Extraction\n",
      "======================================================================\n",
      "‚ùå FAILED: object of type 'bool' has no len()\n"
     ]
    }
   ],
   "source": [
    "def test_comprehensive_extraction(username):\n",
    "    \"\"\"Test comprehensive extraction with all data types.\"\"\"\n",
    "    payload = {\n",
    "        \"username\": username,\n",
    "        \"scrape_posts\": True,\n",
    "        \"max_posts\": 15,\n",
    "        \"scrape_likes\": True,\n",
    "        \"max_likes\": 10,\n",
    "        \"scrape_mentions\": True,\n",
    "        \"max_mentions\": 5,\n",
    "        \"scrape_media\": True,\n",
    "        \"max_media\": 5,\n",
    "        \"scrape_followers\": False,  # Skip for speed\n",
    "        \"scrape_following\": False,  # Skip for speed\n",
    "        \"level\": 4\n",
    "    }\n",
    "    \n",
    "    test_name = f\"@{username} - Comprehensive Extraction\"\n",
    "    print(f\"\\nüéØ Testing: {test_name}\")\n",
    "    result = submit(\"twitter\", payload)\n",
    "    analyze_twitter_results(result, test_name)\n",
    "    return result\n",
    "\n",
    "print(\"\\nüî¨ COMPREHENSIVE EXTRACTION TEST\")\n",
    "print(\"=\"*50)\n",
    "print(\"üìä Testing multiple data types extraction\")\n",
    "\n",
    "comprehensive_result = test_comprehensive_extraction(\"paulg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "date-filtering",
   "metadata": {},
   "source": [
    "## üìÖ Date Filtering Performance Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "date-filtering-test",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìÖ DATE FILTERING PERFORMANCE TESTS\n",
      "==================================================\n",
      "üéØ Testing performance improvements with date filtering\n",
      "\n",
      "üìÖ Testing: @sama - Date Filter (last_week)\n",
      "üöÄ Submitting twitter task...\n",
      "üìù Payload: {\n",
      "  \"username\": \"sama\",\n",
      "  \"scrape_posts\": true,\n",
      "  \"max_posts\": 25,\n",
      "  \"enable_date_filtering\": true,\n",
      "  \"date_range\": \"last_week\",\n",
      "  \"stop_at_date_threshold\": true,\n",
      "  \"level\": 4\n",
      "}\n",
      "üÜî Job ID: 30554dd6046941a3ba9e0f4d4b30478b\n",
      "‚è≥ Waiting for job 30554dd6046941a3ba9e0f4d4b30478b...\n",
      "‚è±Ô∏è  running 12s\n",
      "‚úÖ ERROR\n",
      "\n",
      "======================================================================\n",
      "üê¶ ANALYSIS: @sama - Date Filter (last_week)\n",
      "======================================================================\n",
      "‚ùå FAILED: object of type 'bool' has no len()\n",
      "\n",
      "üìÖ Testing: @sama - Date Filter (last_day)\n",
      "üöÄ Submitting twitter task...\n",
      "üìù Payload: {\n",
      "  \"username\": \"sama\",\n",
      "  \"scrape_posts\": true,\n",
      "  \"max_posts\": 25,\n",
      "  \"enable_date_filtering\": true,\n",
      "  \"date_range\": \"last_day\",\n",
      "  \"stop_at_date_threshold\": true,\n",
      "  \"level\": 4\n",
      "}\n",
      "üÜî Job ID: 921238cf8cf74c778ed09e313e9875d8\n",
      "‚è≥ Waiting for job 921238cf8cf74c778ed09e313e9875d8...\n",
      "‚è±Ô∏è  running 15s\n",
      "‚úÖ ERROR\n",
      "\n",
      "======================================================================\n",
      "üê¶ ANALYSIS: @sama - Date Filter (last_day)\n",
      "======================================================================\n",
      "‚ùå FAILED: object of type 'bool' has no len()\n",
      "\n",
      "üìÖ Testing: @sama - Date Filter (today)\n",
      "üöÄ Submitting twitter task...\n",
      "üìù Payload: {\n",
      "  \"username\": \"sama\",\n",
      "  \"scrape_posts\": true,\n",
      "  \"max_posts\": 25,\n",
      "  \"enable_date_filtering\": true,\n",
      "  \"date_range\": \"today\",\n",
      "  \"stop_at_date_threshold\": true,\n",
      "  \"level\": 4\n",
      "}\n",
      "üÜî Job ID: 6c4d5ce3a77b45d1a56384916445fc50\n",
      "‚è≥ Waiting for job 6c4d5ce3a77b45d1a56384916445fc50...\n",
      "‚è±Ô∏è  running 12s\n",
      "‚úÖ ERROR\n",
      "\n",
      "======================================================================\n",
      "üê¶ ANALYSIS: @sama - Date Filter (today)\n",
      "======================================================================\n",
      "‚ùå FAILED: object of type 'bool' has no len()\n"
     ]
    }
   ],
   "source": [
    "def test_date_filtering(username, date_range, max_posts=20):\n",
    "    \"\"\"Test date filtering functionality and performance.\"\"\"\n",
    "    payload = {\n",
    "        \"username\": username,\n",
    "        \"scrape_posts\": True,\n",
    "        \"max_posts\": max_posts,\n",
    "        \"enable_date_filtering\": True,\n",
    "        \"date_range\": date_range,\n",
    "        \"stop_at_date_threshold\": True,\n",
    "        \"level\": 4\n",
    "    }\n",
    "    \n",
    "    test_name = f\"@{username} - Date Filter ({date_range})\"\n",
    "    print(f\"\\nüìÖ Testing: {test_name}\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    result = submit(\"twitter\", payload)\n",
    "    end_time = time.time()\n",
    "    \n",
    "    execution_time = end_time - start_time\n",
    "    \n",
    "    analyze_twitter_results(result, test_name)\n",
    "    \n",
    "    # Performance analysis\n",
    "    if result[\"status\"] == \"finished\":\n",
    "        data_count = len(result[\"result\"].get(\"data\", []))\n",
    "        print(f\"\\n‚ö° PERFORMANCE ANALYSIS:\")\n",
    "        print(f\"   ‚è±Ô∏è Execution time: {execution_time:.1f} seconds\")\n",
    "        print(f\"   üìä Items extracted: {data_count}\")\n",
    "        print(f\"   üöÄ Speed: {data_count/execution_time:.1f} items/second\")\n",
    "        \n",
    "        # Expected performance improvement\n",
    "        expected_improvement = {\n",
    "            \"today\": \"90-95%\",\n",
    "            \"last_day\": \"90-95%\",\n",
    "            \"last_week\": \"70-85%\",\n",
    "            \"last_month\": \"50-70%\"\n",
    "        }\n",
    "        improvement = expected_improvement.get(date_range, \"varies\")\n",
    "        print(f\"   üìà Expected improvement vs full scrape: {improvement} faster\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "print(\"\\nüìÖ DATE FILTERING PERFORMANCE TESTS\")\n",
    "print(\"=\"*50)\n",
    "print(\"üéØ Testing performance improvements with date filtering\")\n",
    "\n",
    "# Test different date ranges\n",
    "date_ranges = [\"last_week\", \"last_day\", \"today\"]\n",
    "date_results = {}\n",
    "\n",
    "for date_range in date_ranges:\n",
    "    date_results[date_range] = test_date_filtering(\"sama\", date_range, max_posts=25)\n",
    "    time.sleep(2)  # Brief pause between tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "level-comparison",
   "metadata": {},
   "source": [
    "## üî¨ Scrape Level Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "level-tests",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üî¨ SCRAPE LEVEL COMPARISON\n",
      "==================================================\n",
      "üìä Testing different extraction levels\n",
      "\n",
      "üìà Level 1: Basic extraction\n",
      "\n",
      "üî¨ Testing: @vitalikbuterin - Level 1\n",
      "üöÄ Submitting twitter task...\n",
      "üìù Payload: {\n",
      "  \"username\": \"vitalikbuterin\",\n",
      "  \"scrape_posts\": true,\n",
      "  \"max_posts\": 8,\n",
      "  \"level\": 1,\n",
      "  \"scrape_level\": 1\n",
      "}\n",
      "üÜî Job ID: f1d33ff152ff4623b7b52f1efb8d31e6\n",
      "‚è≥ Waiting for job f1d33ff152ff4623b7b52f1efb8d31e6...\n",
      "‚è±Ô∏è  running 12s\n",
      "‚úÖ ERROR\n",
      "\n",
      "======================================================================\n",
      "üê¶ ANALYSIS: @vitalikbuterin - Level 1\n",
      "======================================================================\n",
      "‚ùå FAILED: object of type 'bool' has no len()\n",
      "\n",
      "üìà Level 2: Enhanced data\n",
      "\n",
      "üî¨ Testing: @vitalikbuterin - Level 2\n",
      "üöÄ Submitting twitter task...\n",
      "üìù Payload: {\n",
      "  \"username\": \"vitalikbuterin\",\n",
      "  \"scrape_posts\": true,\n",
      "  \"max_posts\": 8,\n",
      "  \"level\": 2,\n",
      "  \"scrape_level\": 2\n",
      "}\n",
      "üÜî Job ID: 68278faa5e1f48f1ba8cb1803282b168\n",
      "‚è≥ Waiting for job 68278faa5e1f48f1ba8cb1803282b168...\n",
      "‚è±Ô∏è  running 12s\n",
      "‚úÖ ERROR\n",
      "\n",
      "======================================================================\n",
      "üê¶ ANALYSIS: @vitalikbuterin - Level 2\n",
      "======================================================================\n",
      "‚ùå FAILED: object of type 'bool' has no len()\n",
      "\n",
      "üìà Level 3: Full profile data\n",
      "\n",
      "üî¨ Testing: @vitalikbuterin - Level 3\n",
      "üöÄ Submitting twitter task...\n",
      "üìù Payload: {\n",
      "  \"username\": \"vitalikbuterin\",\n",
      "  \"scrape_posts\": true,\n",
      "  \"max_posts\": 8,\n",
      "  \"level\": 3,\n",
      "  \"scrape_level\": 3\n",
      "}\n",
      "üÜî Job ID: badb9e10d9214bfe818ae12072249a41\n",
      "‚è≥ Waiting for job badb9e10d9214bfe818ae12072249a41...\n",
      "‚è±Ô∏è  running 12s\n",
      "‚úÖ ERROR\n",
      "\n",
      "======================================================================\n",
      "üê¶ ANALYSIS: @vitalikbuterin - Level 3\n",
      "======================================================================\n",
      "‚ùå FAILED: object of type 'bool' has no len()\n",
      "\n",
      "üìà Level 4: Comprehensive extraction\n",
      "\n",
      "üî¨ Testing: @vitalikbuterin - Level 4\n",
      "üöÄ Submitting twitter task...\n",
      "üìù Payload: {\n",
      "  \"username\": \"vitalikbuterin\",\n",
      "  \"scrape_posts\": true,\n",
      "  \"max_posts\": 8,\n",
      "  \"level\": 4,\n",
      "  \"scrape_level\": 4\n",
      "}\n",
      "üÜî Job ID: 7a9ddef0d2d2406c895150084ce4c6e3\n",
      "‚è≥ Waiting for job 7a9ddef0d2d2406c895150084ce4c6e3...\n",
      "‚è±Ô∏è  running 48s\n",
      "‚úÖ FINISHED\n",
      "\n",
      "======================================================================\n",
      "üê¶ ANALYSIS: @vitalikbuterin - Level 4\n",
      "======================================================================\n",
      "‚úÖ STATUS: Task completed successfully\n",
      "üéØ TARGET: @N/A\n",
      "üìä TOTAL EXTRACTED: 0 items (found: N/A)\n",
      "üîß METHOD: N/A\n",
      "üìà SCRAPE LEVEL: N/A\n",
      "üìà SUCCESS RATE: 0.0%\n",
      "‚ùå NO DATA EXTRACTED\n",
      "\n",
      "======================================================================\n",
      "üìä LEVEL COMPARISON SUMMARY\n",
      "======================================================================\n",
      "Level 1: ‚ùå Failed or incomplete\n",
      "Level 2: ‚ùå Failed or incomplete\n",
      "Level 3: ‚ùå Failed or incomplete\n",
      "Level 4: 0 items | Unknown\n"
     ]
    }
   ],
   "source": [
    "def test_scrape_level(username, level, max_posts=10):\n",
    "    \"\"\"Test specific scrape level.\"\"\"\n",
    "    payload = {\n",
    "        \"username\": username,\n",
    "        \"scrape_posts\": True,\n",
    "        \"max_posts\": max_posts,\n",
    "        \"level\": level,\n",
    "        \"scrape_level\": level  # Both for compatibility\n",
    "    }\n",
    "    \n",
    "    test_name = f\"@{username} - Level {level}\"\n",
    "    print(f\"\\nüî¨ Testing: {test_name}\")\n",
    "    result = submit(\"twitter\", payload)\n",
    "    analyze_twitter_results(result, test_name)\n",
    "    return result\n",
    "\n",
    "print(\"\\nüî¨ SCRAPE LEVEL COMPARISON\")\n",
    "print(\"=\"*50)\n",
    "print(\"üìä Testing different extraction levels\")\n",
    "\n",
    "level_descriptions = {\n",
    "    1: \"Basic extraction\",\n",
    "    2: \"Enhanced data\", \n",
    "    3: \"Full profile data\",\n",
    "    4: \"Comprehensive extraction\"\n",
    "}\n",
    "\n",
    "level_results = {}\n",
    "test_username = \"vitalikbuterin\"\n",
    "\n",
    "for level in [1, 2, 3, 4]:\n",
    "    print(f\"\\nüìà Level {level}: {level_descriptions[level]}\")\n",
    "    level_results[level] = test_scrape_level(test_username, level, max_posts=8)\n",
    "    time.sleep(1)\n",
    "\n",
    "# Compare results across levels\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üìä LEVEL COMPARISON SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for level in [1, 2, 3, 4]:\n",
    "    result = level_results.get(level, {})\n",
    "    if result.get(\"status\") == \"finished\":\n",
    "        data = result[\"result\"].get(\"data\", [])\n",
    "        method = result[\"result\"].get(\"search_metadata\", {}).get(\"extraction_method\", \"Unknown\")\n",
    "        print(f\"Level {level}: {len(data)} items | {method}\")\n",
    "    else:\n",
    "        print(f\"Level {level}: ‚ùå Failed or incomplete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "batch-test",
   "metadata": {},
   "source": [
    "## üéØ Batch Testing Multiple Accounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "batch-testing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ BATCH TESTING MULTIPLE ACCOUNTS\n",
      "==================================================\n",
      "\n",
      "üß™ Testing @naval...\n",
      "üöÄ Submitting twitter task...\n",
      "üìù Payload: {\n",
      "  \"username\": \"naval\",\n",
      "  \"scrape_posts\": true,\n",
      "  \"max_posts\": 5,\n",
      "  \"enable_date_filtering\": true,\n",
      "  \"date_range\": \"last_week\",\n",
      "  \"level\": 4\n",
      "}\n",
      "üÜî Job ID: 32e1fe29c29e44c4976d9273fe10af96\n",
      "‚è≥ Waiting for job 32e1fe29c29e44c4976d9273fe10af96...\n",
      "‚è±Ô∏è  running 42s\n",
      "‚úÖ FINISHED\n",
      "‚úÖ @naval: 0 items in 45.1s\n",
      "\n",
      "üß™ Testing @paulg...\n",
      "üöÄ Submitting twitter task...\n",
      "üìù Payload: {\n",
      "  \"username\": \"paulg\",\n",
      "  \"scrape_posts\": true,\n",
      "  \"max_posts\": 5,\n",
      "  \"enable_date_filtering\": true,\n",
      "  \"date_range\": \"last_week\",\n",
      "  \"level\": 4\n",
      "}\n",
      "üÜî Job ID: 79f91a4c000e475a9e9eed20d80a8fab\n",
      "‚è≥ Waiting for job 79f91a4c000e475a9e9eed20d80a8fab...\n",
      "‚è±Ô∏è  running 39s\n",
      "‚úÖ FINISHED\n",
      "‚úÖ @paulg: 0 items in 42.1s\n",
      "\n",
      "üß™ Testing @sama...\n",
      "üöÄ Submitting twitter task...\n",
      "üìù Payload: {\n",
      "  \"username\": \"sama\",\n",
      "  \"scrape_posts\": true,\n",
      "  \"max_posts\": 5,\n",
      "  \"enable_date_filtering\": true,\n",
      "  \"date_range\": \"last_week\",\n",
      "  \"level\": 4\n",
      "}\n",
      "üÜî Job ID: 9b529274502847b988b3db4167a4f0b1\n",
      "‚è≥ Waiting for job 9b529274502847b988b3db4167a4f0b1...\n",
      "‚è±Ô∏è  running 45s\n",
      "‚úÖ FINISHED\n",
      "‚úÖ @sama: 0 items in 48.1s\n",
      "\n",
      "==================================================\n",
      "üìä BATCH TESTING SUMMARY\n",
      "==================================================\n",
      "‚úÖ @naval: 0 items | 45.1s | 0.0 items/s\n",
      "‚úÖ @paulg: 0 items | 42.1s | 0.0 items/s\n",
      "‚úÖ @sama: 0 items | 48.1s | 0.0 items/s\n",
      "\n",
      "üìà AVERAGES:\n",
      "   ‚è±Ô∏è Time: 45.1s per account\n",
      "   üìä Items: 0.0 per account\n",
      "   üöÄ Speed: 0.0 items/second overall\n",
      "   ‚úÖ Success rate: 3/3 (100%)\n"
     ]
    }
   ],
   "source": [
    "def test_multiple_accounts():\n",
    "    \"\"\"Test extraction across multiple accounts to validate consistency.\"\"\"\n",
    "    test_accounts_subset = [\"naval\", \"paulg\", \"sama\"]\n",
    "    results = {}\n",
    "    \n",
    "    print(\"üéØ BATCH TESTING MULTIPLE ACCOUNTS\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    for account in test_accounts_subset:\n",
    "        payload = {\n",
    "            \"username\": account,\n",
    "            \"scrape_posts\": True,\n",
    "            \"max_posts\": 5,  # Small for batch testing\n",
    "            \"enable_date_filtering\": True,\n",
    "            \"date_range\": \"last_week\",\n",
    "            \"level\": 4\n",
    "        }\n",
    "        \n",
    "        print(f\"\\nüß™ Testing @{account}...\")\n",
    "        start_time = time.time()\n",
    "        result = submit(\"twitter\", payload)\n",
    "        execution_time = time.time() - start_time\n",
    "        \n",
    "        results[account] = {\n",
    "            \"result\": result,\n",
    "            \"execution_time\": execution_time\n",
    "        }\n",
    "        \n",
    "        if result[\"status\"] == \"finished\":\n",
    "            data_count = len(result[\"result\"].get(\"data\", []))\n",
    "            print(f\"‚úÖ @{account}: {data_count} items in {execution_time:.1f}s\")\n",
    "        else:\n",
    "            print(f\"‚ùå @{account}: Failed - {result.get('error', 'Unknown error')}\")\n",
    "        \n",
    "        time.sleep(1)  # Brief pause between accounts\n",
    "    \n",
    "    # Summary analysis\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"üìä BATCH TESTING SUMMARY\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    successful_tests = 0\n",
    "    total_items = 0\n",
    "    total_time = 0\n",
    "    \n",
    "    for account, data in results.items():\n",
    "        result = data[\"result\"]\n",
    "        exec_time = data[\"execution_time\"]\n",
    "        \n",
    "        if result[\"status\"] == \"finished\":\n",
    "            successful_tests += 1\n",
    "            items = len(result[\"result\"].get(\"data\", []))\n",
    "            total_items += items\n",
    "            total_time += exec_time\n",
    "            \n",
    "            print(f\"‚úÖ @{account}: {items} items | {exec_time:.1f}s | {items/exec_time:.1f} items/s\")\n",
    "        else:\n",
    "            print(f\"‚ùå @{account}: Failed\")\n",
    "    \n",
    "    if successful_tests > 0:\n",
    "        avg_time = total_time / successful_tests\n",
    "        avg_items = total_items / successful_tests\n",
    "        print(f\"\\nüìà AVERAGES:\")\n",
    "        print(f\"   ‚è±Ô∏è Time: {avg_time:.1f}s per account\")\n",
    "        print(f\"   üìä Items: {avg_items:.1f} per account\")\n",
    "        print(f\"   üöÄ Speed: {total_items/total_time:.1f} items/second overall\")\n",
    "        print(f\"   ‚úÖ Success rate: {successful_tests}/{len(test_accounts_subset)} ({successful_tests/len(test_accounts_subset)*100:.0f}%)\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "batch_results = test_multiple_accounts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data-validation",
   "metadata": {},
   "source": [
    "## üîç Data Quality Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "data-validation-test",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç DATA QUALITY VALIDATION\n",
      "==================================================\n",
      "\n",
      "üîç DATA VALIDATION: Comprehensive Extraction\n",
      "==================================================\n",
      "‚ùå Cannot validate - extraction failed\n"
     ]
    }
   ],
   "source": [
    "def validate_extracted_data(result, test_name):\n",
    "    \"\"\"Validate quality and completeness of extracted data.\"\"\"\n",
    "    print(f\"\\nüîç DATA VALIDATION: {test_name}\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    if result[\"status\"] != \"finished\":\n",
    "        print(f\"‚ùå Cannot validate - extraction failed\")\n",
    "        return False\n",
    "    \n",
    "    data = result[\"result\"].get(\"data\", [])\n",
    "    if not data:\n",
    "        print(f\"‚ùå No data to validate\")\n",
    "        return False\n",
    "    \n",
    "    validation_score = 0\n",
    "    max_score = 0\n",
    "    \n",
    "    # Check if comprehensive extraction\n",
    "    if isinstance(data[0], dict) and 'profile' in data[0]:\n",
    "        profile_data = data[0]\n",
    "        \n",
    "        # Profile validation\n",
    "        profile = profile_data.get('profile', {})\n",
    "        max_score += 4\n",
    "        \n",
    "        if profile.get('display_name'):\n",
    "            validation_score += 1\n",
    "            print(f\"‚úÖ Profile name: {profile['display_name']}\")\n",
    "        else:\n",
    "            print(f\"‚ùå Missing profile name\")\n",
    "            \n",
    "        if profile.get('username'):\n",
    "            validation_score += 1\n",
    "            print(f\"‚úÖ Username: @{profile['username']}\")\n",
    "        else:\n",
    "            print(f\"‚ùå Missing username\")\n",
    "            \n",
    "        if profile.get('bio'):\n",
    "            validation_score += 1\n",
    "            print(f\"‚úÖ Bio: {len(profile['bio'])} chars\")\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è No bio found\")\n",
    "            \n",
    "        if profile.get('followers_count') is not None:\n",
    "            validation_score += 1\n",
    "            print(f\"‚úÖ Followers count: {profile['followers_count']}\")\n",
    "        else:\n",
    "            print(f\"‚ùå Missing followers count\")\n",
    "        \n",
    "        # Posts validation\n",
    "        posts = profile_data.get('posts', [])\n",
    "        if posts:\n",
    "            max_score += 3\n",
    "            print(f\"\\nüìù POSTS VALIDATION ({len(posts)} posts):\")\n",
    "            \n",
    "            posts_with_text = [p for p in posts if isinstance(p, dict) and p.get('text')]\n",
    "            if posts_with_text:\n",
    "                validation_score += 1\n",
    "                avg_length = sum(len(p['text']) for p in posts_with_text) / len(posts_with_text)\n",
    "                print(f\"‚úÖ Text content: {len(posts_with_text)}/{len(posts)} posts, avg {avg_length:.0f} chars\")\n",
    "            else:\n",
    "                print(f\"‚ùå No text content found in posts\")\n",
    "            \n",
    "            posts_with_dates = [p for p in posts if isinstance(p, dict) and p.get('date')]\n",
    "            if posts_with_dates:\n",
    "                validation_score += 1\n",
    "                print(f\"‚úÖ Dates: {len(posts_with_dates)}/{len(posts)} posts have dates\")\n",
    "            else:\n",
    "                print(f\"‚ö†Ô∏è No dates found in posts\")\n",
    "            \n",
    "            posts_with_metrics = [p for p in posts if isinstance(p, dict) and any(k in p for k in ['likes', 'retweets'])]\n",
    "            if posts_with_metrics:\n",
    "                validation_score += 1\n",
    "                print(f\"‚úÖ Engagement metrics: {len(posts_with_metrics)}/{len(posts)} posts\")\n",
    "            else:\n",
    "                print(f\"‚ö†Ô∏è No engagement metrics found\")\n",
    "                \n",
    "    else:\n",
    "        # Direct posts validation\n",
    "        max_score = 3\n",
    "        posts = data\n",
    "        print(f\"üìù DIRECT POSTS VALIDATION ({len(posts)} posts):\")\n",
    "        \n",
    "        posts_with_text = [p for p in posts if isinstance(p, dict) and p.get('text')]\n",
    "        if posts_with_text:\n",
    "            validation_score += 1\n",
    "            print(f\"‚úÖ Text content: {len(posts_with_text)}/{len(posts)} posts\")\n",
    "        \n",
    "        posts_with_dates = [p for p in posts if isinstance(p, dict) and p.get('date')]\n",
    "        if posts_with_dates:\n",
    "            validation_score += 1\n",
    "            print(f\"‚úÖ Dates: {len(posts_with_dates)}/{len(posts)} posts\")\n",
    "        \n",
    "        if len(posts) >= 5:\n",
    "            validation_score += 1\n",
    "            print(f\"‚úÖ Sufficient data: {len(posts)} posts extracted\")\n",
    "    \n",
    "    # Calculate validation score\n",
    "    score_percentage = (validation_score / max_score) * 100 if max_score > 0 else 0\n",
    "    \n",
    "    print(f\"\\nüìä VALIDATION SCORE: {validation_score}/{max_score} ({score_percentage:.0f}%)\")\n",
    "    \n",
    "    if score_percentage >= 80:\n",
    "        print(f\"‚úÖ EXCELLENT data quality\")\n",
    "    elif score_percentage >= 60:\n",
    "        print(f\"‚úÖ GOOD data quality\")\n",
    "    elif score_percentage >= 40:\n",
    "        print(f\"‚ö†Ô∏è FAIR data quality - some issues detected\")\n",
    "    else:\n",
    "        print(f\"‚ùå POOR data quality - significant issues\")\n",
    "    \n",
    "    return score_percentage >= 60\n",
    "\n",
    "# Validate recent results\n",
    "print(\"üîç DATA QUALITY VALIDATION\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "if 'comprehensive_result' in locals():\n",
    "    validate_extracted_data(comprehensive_result, \"Comprehensive Extraction\")\n",
    "\n",
    "if 'basic_result' in locals():\n",
    "    validate_extracted_data(basic_result, \"Basic Extraction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "final-summary",
   "metadata": {},
   "source": [
    "## üìä Final Testing Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "final-summary-code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä TWITTER SCRAPER TESTING SUMMARY\n",
      "======================================================================\n",
      "üìà OVERALL RESULTS:\n",
      "   Total Tests: 11\n",
      "   Successful: 4\n",
      "   Failed: 7\n",
      "   Success Rate: 36.4%\n",
      "\n",
      "üìã DETAILED RESULTS:\n",
      "   ‚ùå Comprehensive Extraction: FAILED\n",
      "   ‚ùå Date Filter (last_week): FAILED\n",
      "   ‚ùå Date Filter (last_day): FAILED\n",
      "   ‚ùå Date Filter (today): FAILED\n",
      "   ‚ùå Scrape Level 1: FAILED\n",
      "   ‚ùå Scrape Level 2: FAILED\n",
      "   ‚ùå Scrape Level 3: FAILED\n",
      "   ‚úÖ Scrape Level 4: PASSED\n",
      "   ‚úÖ Batch Test (@naval): PASSED\n",
      "   ‚úÖ Batch Test (@paulg): PASSED\n",
      "   ‚úÖ Batch Test (@sama): PASSED\n",
      "\n",
      "üéØ OVERALL ASSESSMENT:\n",
      "‚ùå POOR - Twitter scraper needs significant fixes\n",
      "\n",
      "üîó Data files can be found in: /storage/scraped_data/twitter/\n",
      "üéâ Testing completed!\n"
     ]
    }
   ],
   "source": [
    "print(\"üìä TWITTER SCRAPER TESTING SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Count successful tests\n",
    "total_tests = 0\n",
    "successful_tests = 0\n",
    "failed_tests = 0\n",
    "\n",
    "test_results = []\n",
    "\n",
    "# Check all test results\n",
    "if 'basic_result' in locals():\n",
    "    total_tests += 1\n",
    "    if basic_result.get('status') == 'finished':\n",
    "        successful_tests += 1\n",
    "        test_results.append((\"‚úÖ\", \"Basic Extraction\", \"PASSED\"))\n",
    "    else:\n",
    "        failed_tests += 1\n",
    "        test_results.append((\"‚ùå\", \"Basic Extraction\", \"FAILED\"))\n",
    "\n",
    "if 'comprehensive_result' in locals():\n",
    "    total_tests += 1\n",
    "    if comprehensive_result.get('status') == 'finished':\n",
    "        successful_tests += 1\n",
    "        test_results.append((\"‚úÖ\", \"Comprehensive Extraction\", \"PASSED\"))\n",
    "    else:\n",
    "        failed_tests += 1\n",
    "        test_results.append((\"‚ùå\", \"Comprehensive Extraction\", \"FAILED\"))\n",
    "\n",
    "if 'date_results' in locals():\n",
    "    for date_range, result in date_results.items():\n",
    "        total_tests += 1\n",
    "        if result.get('status') == 'finished':\n",
    "            successful_tests += 1\n",
    "            test_results.append((\"‚úÖ\", f\"Date Filter ({date_range})\", \"PASSED\"))\n",
    "        else:\n",
    "            failed_tests += 1\n",
    "            test_results.append((\"‚ùå\", f\"Date Filter ({date_range})\", \"FAILED\"))\n",
    "\n",
    "if 'level_results' in locals():\n",
    "    for level, result in level_results.items():\n",
    "        total_tests += 1\n",
    "        if result.get('status') == 'finished':\n",
    "            successful_tests += 1\n",
    "            test_results.append((\"‚úÖ\", f\"Scrape Level {level}\", \"PASSED\"))\n",
    "        else:\n",
    "            failed_tests += 1\n",
    "            test_results.append((\"‚ùå\", f\"Scrape Level {level}\", \"FAILED\"))\n",
    "\n",
    "if 'batch_results' in locals():\n",
    "    for account, data in batch_results.items():\n",
    "        total_tests += 1\n",
    "        if data['result'].get('status') == 'finished':\n",
    "            successful_tests += 1\n",
    "            test_results.append((\"‚úÖ\", f\"Batch Test (@{account})\", \"PASSED\"))\n",
    "        else:\n",
    "            failed_tests += 1\n",
    "            test_results.append((\"‚ùå\", f\"Batch Test (@{account})\", \"FAILED\"))\n",
    "\n",
    "# Display results\n",
    "print(f\"üìà OVERALL RESULTS:\")\n",
    "print(f\"   Total Tests: {total_tests}\")\n",
    "print(f\"   Successful: {successful_tests}\")\n",
    "print(f\"   Failed: {failed_tests}\")\n",
    "\n",
    "if total_tests > 0:\n",
    "    success_rate = (successful_tests / total_tests) * 100\n",
    "    print(f\"   Success Rate: {success_rate:.1f}%\")\n",
    "    \n",
    "    print(f\"\\nüìã DETAILED RESULTS:\")\n",
    "    for emoji, test_name, status in test_results:\n",
    "        print(f\"   {emoji} {test_name}: {status}\")\n",
    "    \n",
    "    print(f\"\\nüéØ OVERALL ASSESSMENT:\")\n",
    "    if success_rate >= 90:\n",
    "        print(f\"üéâ EXCELLENT - Twitter scraper is working perfectly!\")\n",
    "    elif success_rate >= 75:\n",
    "        print(f\"‚úÖ GOOD - Twitter scraper is working well with minor issues\")\n",
    "    elif success_rate >= 50:\n",
    "        print(f\"‚ö†Ô∏è FAIR - Twitter scraper has some issues that need attention\")\n",
    "    else:\n",
    "        print(f\"‚ùå POOR - Twitter scraper needs significant fixes\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No tests were run\")\n",
    "\n",
    "print(f\"\\nüîó Data files can be found in: {SCRAPED}/twitter/\")\n",
    "print(f\"üéâ Testing completed!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
