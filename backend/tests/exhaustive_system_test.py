#!/usr/bin/env python3
"""
Exhaustive System Testing - Test ALL Features and Identify ALL Issues
Tests every component systematically before implementing new features
"""

import json
import urllib.request
import time
import sys
from typing import Dict, List, Any, Tuple
from datetime import datetime

class ExhaustiveTwitterScraperTester:
    def __init__(self):
        self.endpoint = "http://localhost:8004"
        self.results = {}
        self.issues = []
        self.successes = []

    def submit_job_and_wait(self, payload: Dict, timeout_minutes: int = 3) -> Tuple[str, Dict]:
        """Submit job and wait for completion, return job_id and result"""
        try:
            # Submit job
            data = json.dumps(payload).encode('utf-8')
            req = urllib.request.Request(
                f'{self.endpoint}/jobs/twitter',
                data=data,
                headers={'Content-Type': 'application/json'}
            )

            with urllib.request.urlopen(req, timeout=15) as response:
                result = json.loads(response.read().decode('utf-8'))
                job_id = result['job_id']

            # Wait for completion
            start_time = time.time()
            timeout_seconds = timeout_minutes * 60

            while time.time() - start_time < timeout_seconds:
                try:
                    with urllib.request.urlopen(f'{self.endpoint}/jobs/{job_id}', timeout=10) as response:
                        result = json.loads(response.read().decode('utf-8'))

                        if result['status'] in ['finished', 'error']:
                            return job_id, result

                    time.sleep(2)
                except Exception as e:
                    continue

            return job_id, {"status": "timeout", "error": f"Job timed out after {timeout_minutes} minutes"}

        except Exception as e:
            return None, {"status": "submission_failed", "error": str(e)}

    def test_user_profiles_comprehensive(self) -> List[Dict]:
        """Test user profile extraction comprehensively"""
        print("\nüß™ COMPREHENSIVE USER PROFILE TESTING")
        print("=" * 60)

        test_cases = [
            # Different account types
            {"username": "naval", "posts": 3, "description": "High-profile tech account"},
            {"username": "sama", "posts": 5, "description": "OpenAI CEO account"},
            {"username": "elonmusk", "posts": 3, "description": "Very high-profile account"},
            {"username": "sundarpichai", "posts": 3, "description": "Google CEO account"},
            {"username": "satyanadella", "posts": 3, "description": "Microsoft CEO account"},

            # Different post counts (testing scaling)
            {"username": "naval", "posts": 1, "description": "Single post extraction"},
            {"username": "naval", "posts": 10, "description": "Medium-scale extraction"},
            {"username": "naval", "posts": 25, "description": "Large-scale extraction"},

            # Edge cases
            {"username": "nonexistentuser12345", "posts": 3, "description": "Non-existent user"},
            {"username": "test", "posts": 3, "description": "Potentially private/suspended user"},
        ]

        results = []

        for i, test_case in enumerate(test_cases, 1):
            username = test_case["username"]
            posts = test_case["posts"]
            description = test_case["description"]

            print(f"\n{i:2}. üì± Testing @{username} ({posts} posts) - {description}")

            payload = {
                "username": username,
                "scrape_posts": True,
                "max_posts": posts,
                "scrape_level": 4
            }

            job_id, result = self.submit_job_and_wait(payload, timeout_minutes=4)

            analysis = {
                "test_type": "user_profile",
                "username": username,
                "requested_posts": posts,
                "description": description,
                "job_id": job_id,
                "status": result.get("status"),
                "execution_time": 0,
                "actual_posts": 0,
                "has_profile_data": False,
                "has_media": 0,
                "has_engagement": 0,
                "has_classification": 0,
                "extraction_method": "unknown",
                "issues": [],
                "sample_post": None
            }

            if job_id and result.get("status") == "finished":
                try:
                    # Calculate execution time
                    if result.get("started_at") and result.get("finished_at"):
                        start = datetime.fromisoformat(result["started_at"].replace('Z', '+00:00'))
                        end = datetime.fromisoformat(result["finished_at"].replace('Z', '+00:00'))
                        analysis["execution_time"] = (end - start).total_seconds()

                    res_data = result["result"]
                    search_metadata = res_data.get("search_metadata", {})
                    analysis["extraction_method"] = search_metadata.get("extraction_method", "unknown")

                    data_results = res_data.get("data", [])
                    if data_results and len(data_results) > 0:
                        first_result = data_results[0]

                        # Check profile data
                        if "profile" in first_result:
                            analysis["has_profile_data"] = True

                        # Check posts data
                        if "posts" in first_result:
                            posts_data = first_result["posts"]
                            analysis["actual_posts"] = len(posts_data)

                            # Analyze post quality
                            for post in posts_data:
                                if post.get("media") and len(post["media"]) > 0:
                                    analysis["has_media"] += 1
                                if any([post.get("likes"), post.get("retweets"), post.get("replies")]):
                                    analysis["has_engagement"] += 1
                                if post.get("classification"):
                                    analysis["has_classification"] += 1

                            # Get sample post
                            if posts_data:
                                analysis["sample_post"] = {
                                    "text": posts_data[0].get("text", "")[:100],
                                    "author": posts_data[0].get("author", ""),
                                    "url": posts_data[0].get("url", "")
                                }

                        else:
                            analysis["issues"].append("No 'posts' key in result data")
                    else:
                        analysis["issues"].append("No data results found")

                    # Success assessment
                    if analysis["actual_posts"] > 0:
                        print(f"     ‚úÖ SUCCESS: {analysis['actual_posts']} posts extracted")
                        print(f"        Method: {analysis['extraction_method']}")
                        print(f"        Time: {analysis['execution_time']:.1f}s")
                        print(f"        Quality: Media({analysis['has_media']}), Engagement({analysis['has_engagement']}), Classification({analysis['has_classification']})")
                        if analysis["sample_post"]:
                            print(f"        Sample: \"{analysis['sample_post']['text']}...\"")
                    else:
                        analysis["issues"].append("Zero posts extracted despite success status")
                        print(f"     ‚ùå ISSUE: 0 posts extracted (method: {analysis['extraction_method']})")

                except Exception as e:
                    analysis["issues"].append(f"Result parsing error: {str(e)}")
                    print(f"     ‚ùå PARSING ERROR: {e}")

            elif result.get("status") == "error":
                analysis["issues"].append(f"Job failed: {result.get('error', 'Unknown error')}")
                print(f"     ‚ùå JOB ERROR: {result.get('error', 'Unknown error')}")

            elif result.get("status") == "timeout":
                analysis["issues"].append("Job timed out")
                print(f"     ‚è∞ TIMEOUT: Job did not complete within timeout")

            elif result.get("status") == "submission_failed":
                analysis["issues"].append(f"Job submission failed: {result.get('error', 'Unknown')}")
                print(f"     üí• SUBMISSION FAILED: {result.get('error', 'Unknown')}")

            else:
                analysis["issues"].append(f"Unexpected status: {result.get('status')}")
                print(f"     ‚ùì UNEXPECTED: {result.get('status')}")

            results.append(analysis)

        return results

    def test_search_functionality_comprehensive(self) -> List[Dict]:
        """Test all search functionality variations"""
        print("\nüîç COMPREHENSIVE SEARCH FUNCTIONALITY TESTING")
        print("=" * 60)

        test_cases = [
            # Hashtag searches
            {"type": "hashtag", "hashtag": "AI", "max_tweets": 5, "description": "Popular tech hashtag"},
            {"type": "hashtag", "hashtag": "bitcoin", "max_tweets": 5, "description": "Popular crypto hashtag"},
            {"type": "hashtag", "hashtag": "python", "max_tweets": 3, "description": "Programming hashtag"},
            {"type": "hashtag", "hashtag": "startup", "max_tweets": 3, "description": "Business hashtag"},

            # Query searches
            {"type": "query", "search_query": "machine learning", "max_tweets": 5, "description": "Tech query"},
            {"type": "query", "search_query": "artificial intelligence", "max_tweets": 3, "description": "AI query"},
            {"type": "query", "search_query": "crypto", "max_tweets": 3, "description": "Short crypto query"},
            {"type": "query", "search_query": "OpenAI", "max_tweets": 3, "description": "Company query"},

            # Different scales
            {"type": "hashtag", "hashtag": "AI", "max_tweets": 1, "description": "Single result search"},
            {"type": "hashtag", "hashtag": "AI", "max_tweets": 15, "description": "Large result search"},

            # Edge cases
            {"type": "hashtag", "hashtag": "veryrarehashtag12345", "max_tweets": 3, "description": "Rare/non-existent hashtag"},
            {"type": "query", "search_query": "very specific rare query 12345", "max_tweets": 3, "description": "Rare query"},
        ]

        results = []

        for i, test_case in enumerate(test_cases, 1):
            search_type = test_case["type"]
            search_term = test_case.get("hashtag", test_case.get("search_query"))
            max_tweets = test_case["max_tweets"]
            description = test_case["description"]

            print(f"\n{i:2}. üè∑Ô∏è Testing {search_type}: '{search_term}' ({max_tweets} tweets) - {description}")

            payload = {
                "scrape_level": 4,
                "max_tweets": max_tweets
            }

            if search_type == "hashtag":
                payload["hashtag"] = test_case["hashtag"]
            else:
                payload["search_query"] = test_case["search_query"]

            job_id, result = self.submit_job_and_wait(payload, timeout_minutes=3)

            analysis = {
                "test_type": "search",
                "search_type": search_type,
                "search_term": search_term,
                "requested_tweets": max_tweets,
                "description": description,
                "job_id": job_id,
                "status": result.get("status"),
                "execution_time": 0,
                "actual_posts": 0,
                "extraction_method": "unknown",
                "routing_correct": False,
                "has_search_metadata": False,
                "issues": [],
                "sample_post": None
            }

            if job_id and result.get("status") == "finished":
                try:
                    # Calculate execution time
                    if result.get("started_at") and result.get("finished_at"):
                        start = datetime.fromisoformat(result["started_at"].replace('Z', '+00:00'))
                        end = datetime.fromisoformat(result["finished_at"].replace('Z', '+00:00'))
                        analysis["execution_time"] = (end - start).total_seconds()

                    res_data = result["result"]
                    search_metadata = res_data.get("search_metadata", {})

                    if search_metadata:
                        analysis["has_search_metadata"] = True
                        analysis["extraction_method"] = search_metadata.get("extraction_method", "unknown")

                        # Check if routing is correct
                        if search_type == "hashtag" and "hashtag" in analysis["extraction_method"]:
                            analysis["routing_correct"] = True
                        elif search_type == "query" and "query" in analysis["extraction_method"]:
                            analysis["routing_correct"] = True

                    data_results = res_data.get("data", [])
                    if data_results and len(data_results) > 0:
                        first_result = data_results[0]

                        if "posts" in first_result:
                            posts_data = first_result["posts"]
                            analysis["actual_posts"] = len(posts_data)

                            # Get sample post
                            if posts_data:
                                analysis["sample_post"] = {
                                    "text": posts_data[0].get("text", "")[:100],
                                    "author": posts_data[0].get("author", ""),
                                    "url": posts_data[0].get("url", "")
                                }

                        else:
                            analysis["issues"].append("No 'posts' key in search result data")
                    else:
                        analysis["issues"].append("No data results in search response")

                    # Success assessment
                    if analysis["routing_correct"] and analysis["actual_posts"] > 0:
                        print(f"     ‚úÖ SUCCESS: {analysis['actual_posts']} posts via {analysis['extraction_method']}")
                        print(f"        Time: {analysis['execution_time']:.1f}s")
                        if analysis["sample_post"]:
                            print(f"        Sample: \"{analysis['sample_post']['text']}...\"")
                    elif analysis["routing_correct"] and analysis["actual_posts"] == 0:
                        analysis["issues"].append("Correct routing but zero posts extracted")
                        print(f"     ‚ö†Ô∏è ROUTING OK, NO CONTENT: {analysis['extraction_method']} found 0 posts")
                    elif not analysis["routing_correct"]:
                        analysis["issues"].append(f"Incorrect routing: {analysis['extraction_method']} for {search_type}")
                        print(f"     ‚ùå ROUTING ERROR: {analysis['extraction_method']} for {search_type} search")
                    else:
                        analysis["issues"].append("Unknown search issue")
                        print(f"     ‚ùì UNKNOWN ISSUE")

                except Exception as e:
                    analysis["issues"].append(f"Search result parsing error: {str(e)}")
                    print(f"     ‚ùå PARSING ERROR: {e}")

            elif result.get("status") == "error":
                analysis["issues"].append(f"Search job failed: {result.get('error', 'Unknown error')}")
                print(f"     ‚ùå JOB ERROR: {result.get('error', 'Unknown error')}")

            else:
                analysis["issues"].append(f"Search job status: {result.get('status')}")
                print(f"     ‚ùå JOB ISSUE: {result.get('status')}")

            results.append(analysis)

        return results

    def test_performance_and_scaling(self) -> List[Dict]:
        """Test performance across different scales"""
        print("\n‚ö° COMPREHENSIVE PERFORMANCE AND SCALING TESTING")
        print("=" * 60)

        test_cases = [
            {"username": "naval", "posts": 1, "description": "Minimal request"},
            {"username": "naval", "posts": 5, "description": "Small request"},
            {"username": "naval", "posts": 15, "description": "Medium request"},
            {"username": "naval", "posts": 30, "description": "Large request"},
            {"username": "naval", "posts": 50, "description": "Very large request"},
            {"username": "sama", "posts": 10, "description": "Different account medium"},
            {"username": "sama", "posts": 25, "description": "Different account large"},
        ]

        results = []

        for i, test_case in enumerate(test_cases, 1):
            username = test_case["username"]
            posts = test_case["posts"]
            description = test_case["description"]

            print(f"\n{i}. ‚ö° Testing @{username} ({posts} posts) - {description}")

            payload = {
                "username": username,
                "scrape_posts": True,
                "max_posts": posts,
                "scrape_level": 4
            }

            start_test_time = time.time()
            job_id, result = self.submit_job_and_wait(payload, timeout_minutes=6)
            total_test_time = time.time() - start_test_time

            analysis = {
                "test_type": "performance",
                "username": username,
                "requested_posts": posts,
                "description": description,
                "job_id": job_id,
                "status": result.get("status"),
                "total_test_time": total_test_time,
                "execution_time": 0,
                "actual_posts": 0,
                "extraction_rate": 0,
                "posts_per_minute": 0,
                "extraction_method": "unknown",
                "issues": []
            }

            if job_id and result.get("status") == "finished":
                try:
                    # Calculate execution time
                    if result.get("started_at") and result.get("finished_at"):
                        start = datetime.fromisoformat(result["started_at"].replace('Z', '+00:00'))
                        end = datetime.fromisoformat(result["finished_at"].replace('Z', '+00:00'))
                        analysis["execution_time"] = (end - start).total_seconds()

                    res_data = result["result"]
                    search_metadata = res_data.get("search_metadata", {})
                    analysis["extraction_method"] = search_metadata.get("extraction_method", "unknown")

                    data_results = res_data.get("data", [])
                    if data_results and "posts" in data_results[0]:
                        posts_data = data_results[0]["posts"]
                        analysis["actual_posts"] = len(posts_data)
                        analysis["extraction_rate"] = (analysis["actual_posts"] / posts) * 100
                        analysis["posts_per_minute"] = (analysis["actual_posts"] / max(analysis["execution_time"], 1)) * 60

                        print(f"     ‚úÖ SUCCESS: {analysis['actual_posts']}/{posts} posts ({analysis['extraction_rate']:.1f}%)")
                        print(f"        Time: {analysis['execution_time']:.1f}s ({analysis['posts_per_minute']:.1f} posts/min)")
                        print(f"        Method: {analysis['extraction_method']}")

                        # Performance assessment
                        if analysis["extraction_rate"] >= 80:
                            print(f"        üéâ EXCELLENT extraction rate")
                        elif analysis["extraction_rate"] >= 50:
                            print(f"        ‚ö†Ô∏è GOOD extraction rate")
                        else:
                            print(f"        ‚ùå POOR extraction rate")
                            analysis["issues"].append(f"Low extraction rate: {analysis['extraction_rate']:.1f}%")

                        if analysis["posts_per_minute"] >= 10:
                            print(f"        üöÄ FAST performance")
                        elif analysis["posts_per_minute"] >= 5:
                            print(f"        ‚úÖ GOOD performance")
                        else:
                            print(f"        üêå SLOW performance")
                            analysis["issues"].append(f"Slow performance: {analysis['posts_per_minute']:.1f} posts/min")

                    else:
                        analysis["issues"].append("No posts data in performance test")
                        print(f"     ‚ùå NO POSTS: Performance test failed")

                except Exception as e:
                    analysis["issues"].append(f"Performance analysis error: {str(e)}")
                    print(f"     ‚ùå ANALYSIS ERROR: {e}")

            else:
                analysis["issues"].append(f"Performance test failed: {result.get('status')}")
                print(f"     ‚ùå FAILED: {result.get('status')}")

            results.append(analysis)

        return results

    def test_error_handling_and_edge_cases(self) -> List[Dict]:
        """Test error handling and edge cases"""
        print("\nüö® COMPREHENSIVE ERROR HANDLING AND EDGE CASE TESTING")
        print("=" * 60)

        test_cases = [
            # Invalid users
            {"username": "nonexistentuser12345", "posts": 3, "description": "Non-existent user", "expect": "error_or_zero"},
            {"username": "verylongusernamethatdoesnotexist123456789", "posts": 3, "description": "Very long invalid username", "expect": "error_or_zero"},

            # Edge case parameters
            {"username": "naval", "posts": 0, "description": "Zero posts requested", "expect": "error_or_zero"},
            {"username": "naval", "posts": 1000, "description": "Extremely large request", "expect": "partial_or_timeout"},

            # Invalid parameters
            {"username": "", "posts": 3, "description": "Empty username", "expect": "error"},
            {"username": "naval", "posts": -1, "description": "Negative posts", "expect": "error"},

            # Potentially problematic accounts
            {"username": "test", "posts": 3, "description": "Generic test account", "expect": "error_or_zero"},
            {"username": "admin", "posts": 3, "description": "Admin account name", "expect": "error_or_zero"},
        ]

        results = []

        for i, test_case in enumerate(test_cases, 1):
            username = test_case["username"]
            posts = test_case["posts"]
            description = test_case["description"]
            expected = test_case["expect"]

            print(f"\n{i}. üîç Testing '{username}' ({posts} posts) - {description}")

            payload = {
                "username": username,
                "scrape_posts": True,
                "max_posts": posts,
                "scrape_level": 4
            }

            job_id, result = self.submit_job_and_wait(payload, timeout_minutes=2)

            analysis = {
                "test_type": "edge_case",
                "username": username,
                "requested_posts": posts,
                "description": description,
                "expected": expected,
                "job_id": job_id,
                "status": result.get("status"),
                "actual_posts": 0,
                "error_message": "",
                "handled_correctly": False,
                "issues": []
            }

            if job_id:
                if result.get("status") == "finished":
                    try:
                        res_data = result["result"]
                        data_results = res_data.get("data", [])

                        if data_results and "posts" in data_results[0]:
                            analysis["actual_posts"] = len(data_results[0]["posts"])

                        # Check if handled correctly based on expectation
                        if expected == "error_or_zero":
                            if analysis["actual_posts"] == 0:
                                analysis["handled_correctly"] = True
                                print(f"     ‚úÖ CORRECTLY HANDLED: 0 posts for problematic case")
                            else:
                                print(f"     ‚ùì UNEXPECTED: {analysis['actual_posts']} posts extracted")

                        elif expected == "partial_or_timeout":
                            if analysis["actual_posts"] > 0:
                                analysis["handled_correctly"] = True
                                print(f"     ‚úÖ PARTIAL SUCCESS: {analysis['actual_posts']} posts")
                            else:
                                print(f"     ‚ö†Ô∏è NO RESULTS: Large request returned 0 posts")

                        elif expected == "error":
                            analysis["issues"].append("Expected error but job finished")
                            print(f"     ‚ö†Ô∏è UNEXPECTED SUCCESS: Expected error but got {analysis['actual_posts']} posts")

                    except Exception as e:
                        analysis["issues"].append(f"Edge case analysis error: {str(e)}")
                        print(f"     ‚ùå ANALYSIS ERROR: {e}")

                elif result.get("status") == "error":
                    analysis["error_message"] = result.get("error", "Unknown error")

                    if expected in ["error", "error_or_zero"]:
                        analysis["handled_correctly"] = True
                        print(f"     ‚úÖ CORRECTLY ERRORED: {analysis['error_message']}")
                    else:
                        analysis["issues"].append(f"Unexpected error: {analysis['error_message']}")
                        print(f"     ‚ùå UNEXPECTED ERROR: {analysis['error_message']}")

                else:
                    analysis["issues"].append(f"Edge case status: {result.get('status')}")
                    print(f"     ‚ùì STATUS: {result.get('status')}")

            else:
                analysis["issues"].append("Job submission failed")
                print(f"     ‚ùå SUBMISSION FAILED")

            results.append(analysis)

        return results

    def generate_comprehensive_report(self, all_results: Dict[str, List[Dict]]):
        """Generate comprehensive system status report"""
        print("\n" + "=" * 80)
        print("üìã EXHAUSTIVE SYSTEM TESTING REPORT")
        print("=" * 80)

        # Calculate overall statistics
        total_tests = sum(len(results) for results in all_results.values())
        total_successful = 0
        total_issues = 0
        total_posts_extracted = 0

        category_stats = {}

        for category, results in all_results.items():
            successful = 0
            posts_extracted = 0
            issues_found = []

            for result in results:
                if result.get("actual_posts", 0) > 0 or result.get("handled_correctly", False):
                    successful += 1
                    total_successful += 1

                posts_extracted += result.get("actual_posts", 0)
                total_posts_extracted += result.get("actual_posts", 0)

                if result.get("issues"):
                    issues_found.extend(result["issues"])
                    total_issues += len(result["issues"])

            category_stats[category] = {
                "total": len(results),
                "successful": successful,
                "success_rate": (successful / len(results)) * 100,
                "posts_extracted": posts_extracted,
                "issues_found": len(issues_found),
                "unique_issues": list(set(issues_found))
            }

        # Print overall statistics
        print(f"\nüìä OVERALL STATISTICS:")
        print(f"   Total Tests: {total_tests}")
        print(f"   Successful: {total_successful} ({(total_successful/total_tests)*100:.1f}%)")
        print(f"   Total Posts Extracted: {total_posts_extracted}")
        print(f"   Total Issues Found: {total_issues}")

        # Print category breakdown
        print(f"\nüìà CATEGORY BREAKDOWN:")
        for category, stats in category_stats.items():
            print(f"   {category.upper()}:")
            print(f"      Tests: {stats['successful']}/{stats['total']} successful ({stats['success_rate']:.1f}%)")
            print(f"      Posts: {stats['posts_extracted']} extracted")
            print(f"      Issues: {stats['issues_found']} found")

        # Critical issues summary
        print(f"\nüö® CRITICAL ISSUES IDENTIFIED:")
        all_unique_issues = set()
        for category, stats in category_stats.items():
            all_unique_issues.update(stats['unique_issues'])

        if all_unique_issues:
            for i, issue in enumerate(sorted(all_unique_issues), 1):
                print(f"   {i}. {issue}")
        else:
            print("   ‚úÖ No critical issues identified")

        # Recommendations
        print(f"\nüí° RECOMMENDATIONS:")
        overall_success_rate = (total_successful / total_tests) * 100

        if overall_success_rate >= 90:
            print("   ‚úÖ SYSTEM READY FOR PRODUCTION")
            print("   ‚úÖ Excellent success rate across all features")
        elif overall_success_rate >= 75:
            print("   ‚ö†Ô∏è SYSTEM MOSTLY READY - Address remaining issues")
            print("   ‚ö†Ô∏è Good success rate but some features need work")
        elif overall_success_rate >= 50:
            print("   üö® SYSTEM NEEDS WORK - Multiple issues identified")
            print("   üö® Significant issues across multiple categories")
        else:
            print("   üö® SYSTEM NOT READY - Major overhaul needed")
            print("   üö® Critical failures across most test categories")

        # Detailed failure analysis
        print(f"\nüìã DETAILED FAILURE ANALYSIS:")
        for category, results in all_results.items():
            failures = [r for r in results if not (r.get("actual_posts", 0) > 0 or r.get("handled_correctly", False))]
            if failures:
                print(f"\n   {category.upper()} FAILURES:")
                for failure in failures[:5]:  # Show top 5 failures per category
                    print(f"      - {failure.get('description', 'Unknown')}: {failure.get('issues', ['No details'])}")

    def run_exhaustive_tests(self):
        """Run all comprehensive tests"""
        print("üöÄ EXHAUSTIVE TWITTER SCRAPER SYSTEM TESTING")
        print("Testing ALL features to identify ALL issues before new development")
        print("=" * 80)

        all_results = {}

        # Run all test suites
        print("Starting comprehensive testing...")

        all_results["user_profiles"] = self.test_user_profiles_comprehensive()
        all_results["search_functionality"] = self.test_search_functionality_comprehensive()
        all_results["performance_scaling"] = self.test_performance_and_scaling()
        all_results["error_handling"] = self.test_error_handling_and_edge_cases()

        # Generate comprehensive report
        self.generate_comprehensive_report(all_results)

        return all_results

if __name__ == "__main__":
    print("Initializing exhaustive system testing...")
    tester = ExhaustiveTwitterScraperTester()
    results = tester.run_exhaustive_tests()

    print(f"\nüéØ EXHAUSTIVE TESTING COMPLETE")
    print("All system components tested. Review report above for complete issue inventory.")